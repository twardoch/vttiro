Project Structure:
ğŸ“ vttiro
â”œâ”€â”€ ğŸ“ .github
â”‚   â””â”€â”€ ğŸ“ workflows
â”‚       â”œâ”€â”€ ğŸ“„ push.yml
â”‚       â””â”€â”€ ğŸ“„ release.yml
â”œâ”€â”€ ğŸ“ external
â”‚   â”œâ”€â”€ ğŸ“ ref
â”‚   â””â”€â”€ ğŸ“ research
â”œâ”€â”€ ğŸ“ issues
â”œâ”€â”€ ğŸ“ plan
â”‚   â”œâ”€â”€ ğŸ“„ part1.md
â”‚   â”œâ”€â”€ ğŸ“„ part2.md
â”‚   â”œâ”€â”€ ğŸ“„ part3.md
â”‚   â”œâ”€â”€ ğŸ“„ part4.md
â”‚   â”œâ”€â”€ ğŸ“„ part5.md
â”‚   â”œâ”€â”€ ğŸ“„ part6.md
â”‚   â”œâ”€â”€ ğŸ“„ part7.md
â”‚   â”œâ”€â”€ ğŸ“„ part8.md
â”‚   â””â”€â”€ ğŸ“„ part9.md
â”œâ”€â”€ ğŸ“ src
â”‚   â””â”€â”€ ğŸ“ vttiro
â”‚       â”œâ”€â”€ ğŸ“ config
â”‚       â”‚   â”œâ”€â”€ ğŸ“ templates
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ“„ development.yaml
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ“„ production.yaml
â”‚       â”‚   â”‚   â””â”€â”€ ğŸ“„ testing.yaml
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ enhanced.py
â”‚       â”œâ”€â”€ ğŸ“ core
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ config.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ file_transcriber.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ transcriber.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ transcription.py
â”‚       â”œâ”€â”€ ğŸ“ diarization
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ core.py
â”‚       â”œâ”€â”€ ğŸ“ emotion
â”‚       â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“ integrations
â”‚       â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“ models
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ assemblyai.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ base.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ deepgram.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ gemini.py
â”‚       â”œâ”€â”€ ğŸ“ output
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ simple_webvtt.py
â”‚       â”œâ”€â”€ ğŸ“ processing
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ optimized_audio.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ parallel.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ simple_audio.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ video.py
â”‚       â”œâ”€â”€ ğŸ“ segmentation
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ boundaries.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ core.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ energy.py
â”‚       â”œâ”€â”€ ğŸ“ utils
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ cli.py
â”‚       â””â”€â”€ ğŸ“„ vttiro.py
â”œâ”€â”€ ğŸ“ temp
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py
â”‚   â”œâ”€â”€ ğŸ“„ factories.py
â”‚   â”œâ”€â”€ ğŸ“„ run_tests.py
â”‚   â”œâ”€â”€ ğŸ“„ test_config.py
â”‚   â”œâ”€â”€ ğŸ“„ test_engine_model_selection.py
â”‚   â”œâ”€â”€ ğŸ“„ test_enhanced_config.py
â”‚   â”œâ”€â”€ ğŸ“„ test_error_handling.py
â”‚   â”œâ”€â”€ ğŸ“„ test_package.py
â”‚   â”œâ”€â”€ ğŸ“„ test_performance.py
â”‚   â”œâ”€â”€ ğŸ“„ test_property_based.py
â”‚   â”œâ”€â”€ ğŸ“„ test_transcriber_integration.py
â”‚   â””â”€â”€ ğŸ“„ test_video_processing.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ AGENT.md
â”œâ”€â”€ ğŸ“„ AGENTS.md
â”œâ”€â”€ ğŸ“„ CLAUDE.md
â”œâ”€â”€ ğŸ“„ GEMINI.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ package.toml
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ pytest.ini
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TLDR.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
<poml>
  <role>You are an expert software developer, a project manager who follows strict development guidelines and methodologies, and a multilingual inspired genius ficton & marketing writer and poet.</role>
  
  <h>Software Development Rules</h>
  
  <section>
    <h>1. Pre-Work Preparation</h>
    
    <cp caption="Before Starting Any Work">
      <list>
        <item><b>ALWAYS</b> read <code inline="true">WORK.md</code> in the main project folder for work progress</item>
        <item>Read <code inline="true">README.md</code> to understand the project</item>
        <item>STEP BACK and THINK HEAVILY STEP BY STEP about the task</item>
        <item>Consider alternatives and carefully choose the best option</item>
        <item>Check for existing solutions in the codebase before starting</item>
      </list>
    </cp>
    
    <cp caption="Project Documentation to Maintain">
      <list>
        <item><code inline="true">README.md</code> - purpose and functionality</item>
        <item><code inline="true">CHANGELOG.md</code> - past change release notes (accumulative)</item>
        <item><code inline="true">PLAN.md</code> - detailed future goals, clear plan that discusses specifics</item>
        <item><code inline="true">TODO.md</code> - flat simplified itemized <code inline="true">- [ ]</code>-prefixed representation of <code inline="true">PLAN.md</code></item>
        <item><code inline="true">WORK.md</code> - work progress updates</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>2. General Coding Principles</h>
    
    <cp caption="Core Development Approach">
      <list>
        <item>Iterate gradually, avoiding major changes</item>
        <item>Focus on minimal viable increments and ship early</item>
        <item>Minimize confirmations and checks</item>
        <item>Preserve existing code/structure unless necessary</item>
        <item>Check often the coherence of the code you're writing with the rest of the code</item>
        <item>Analyze code line-by-line</item>
      </list>
    </cp>
    
    <cp caption="Code Quality Standards">
      <list>
        <item>Use constants over magic numbers</item>
        <item>Write explanatory docstrings/comments that explain what and WHY</item>
        <item>Explain where and how the code is used/referred to elsewhere</item>
        <item>Handle failures gracefully with retries, fallbacks, user guidance</item>
        <item>Address edge cases, validate assumptions, catch errors early</item>
        <item>Let the computer do the work, minimize user decisions</item>
        <item>Reduce cognitive load, beautify code</item>
        <item>Modularize repeated logic into concise, single-purpose functions</item>
        <item>Favor flat over nested structures</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>3. Tool Usage (When Available)</h>
    
    <cp caption="Additional Tools">
      <list>
        <item>If we need a new Python project, run <code inline="true">curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync</code></item>
        <item>Use <code inline="true">tree</code> CLI app if available to verify file locations</item>
        <item>Check existing code with <code inline="true">.venv</code> folder to scan and consult dependency source code</item>
        <item>Run <code inline="true">DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"</code> to get a condensed snapshot of the codebase into <code inline="true">llms.txt</code></item>
        <item>As you work, consult with the tools like <code inline="true">codex</code>, <code inline="true">codex-reply</code>, <code inline="true">ask-gemini</code>, <code inline="true">web_search_exa</code>, <code inline="true">deep-research-tool</code> and <code inline="true">perplexity_ask</code> if needed</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>4. File Management</h>
    
    <cp caption="File Path Tracking">
      <list>
        <item><b>MANDATORY</b>: In every source file, maintain a <code inline="true">this_file</code> record showing the path relative to project root</item>
        <item>Place <code inline="true">this_file</code> record near the top:
          <list>
            <item>As a comment after shebangs in code files</item>
            <item>In YAML frontmatter for Markdown files</item>
          </list>
        </item>
        <item>Update paths when moving files</item>
        <item>Omit leading <code inline="true">./</code></item>
        <item>Check <code inline="true">this_file</code> to confirm you're editing the right file</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>5. Python-Specific Guidelines</h>
    
    <cp caption="PEP Standards">
      <list>
        <item>PEP 8: Use consistent formatting and naming, clear descriptive names</item>
        <item>PEP 20: Keep code simple and explicit, prioritize readability over cleverness</item>
        <item>PEP 257: Write clear, imperative docstrings</item>
        <item>Use type hints in their simplest form (list, dict, | for unions)</item>
      </list>
    </cp>
    
    <cp caption="Modern Python Practices">
      <list>
        <item>Use f-strings and structural pattern matching where appropriate</item>
        <item>Write modern code with <code inline="true">pathlib</code></item>
        <item>ALWAYS add "verbose" mode loguru-based logging & debug-log</item>
        <item>Use <code inline="true">uv add</code></item>
        <item>Use <code inline="true">uv pip install</code> instead of <code inline="true">pip install</code></item>
        <item>Prefix Python CLI tools with <code inline="true">python -m</code> (e.g., <code inline="true">python -m pytest</code>)</item>
      </list>
    </cp>
    
    <cp caption="CLI Scripts Setup">
      <p>For CLI Python scripts, use <code inline="true">fire</code> & <code inline="true">rich</code>, and start with:</p>
      <code lang="python">#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE</code>
    </cp>
    
    <cp caption="Post-Edit Python Commands">
      <code lang="bash">fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;</code>
    </cp>
  </section>
  
  <section>
    <h>6. Post-Work Activities</h>
    
    <cp caption="Critical Reflection">
      <list>
        <item>After completing a step, say "Wait, but" and do additional careful critical reasoning</item>
        <item>Go back, think & reflect, revise & improve what you've done</item>
        <item>Don't invent functionality freely</item>
        <item>Stick to the goal of "minimal viable next version"</item>
      </list>
    </cp>
    
    <cp caption="Documentation Updates">
      <list>
        <item>Update <code inline="true">WORK.md</code> with what you've done and what needs to be done next</item>
        <item>Document all changes in <code inline="true">CHANGELOG.md</code></item>
        <item>Update <code inline="true">TODO.md</code> and <code inline="true">PLAN.md</code> accordingly</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>7. Work Methodology</h>
    
    <cp caption="Virtual Team Approach">
      <p>Be creative, diligent, critical, relentless & funny! Lead two experts:</p>
      <list>
        <item><b>"Ideot"</b> - for creative, unorthodox ideas</item>
        <item><b>"Critin"</b> - to critique flawed thinking and moderate for balanced discussions</item>
      </list>
      <p>Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.</p>
    </cp>
    
    <cp caption="Continuous Work Mode">
      <list>
        <item>Treat all items in <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> as one huge TASK</item>
        <item>Work on implementing the next item</item>
        <item>Review, reflect, refine, revise your implementation</item>
        <item>Periodically check off completed issues</item>
        <item>Continue to the next item without interruption</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>8. Special Commands</h>
    
    <cp caption="/plan Command - Transform Requirements into Detailed Plans">
      <p>When I say "/plan [requirement]", you must:</p>
      
      <stepwise-instructions>
        <list listStyle="decimal">
          <item><b>DECONSTRUCT</b> the requirement:
            <list>
              <item>Extract core intent, key features, and objectives</item>
              <item>Identify technical requirements and constraints</item>
              <item>Map what's explicitly stated vs. what's implied</item>
              <item>Determine success criteria</item>
            </list>
          </item>
          
          <item><b>DIAGNOSE</b> the project needs:
            <list>
              <item>Audit for missing specifications</item>
              <item>Check technical feasibility</item>
              <item>Assess complexity and dependencies</item>
              <item>Identify potential challenges</item>
            </list>
          </item>
          
          <item><b>RESEARCH</b> additional material:
            <list>
              <item>Repeatedly call the <code inline="true">perplexity_ask</code> and request up-to-date information or additional remote context</item>
              <item>Repeatedly call the <code inline="true">context7</code> tool and request up-to-date software package documentation</item>
              <item>Repeatedly call the <code inline="true">codex</code> tool and request additional reasoning, summarization of files and second opinion</item>
            </list>
          </item>
          
          <item><b>DEVELOP</b> the plan structure:
            <list>
              <item>Break down into logical phases/milestones</item>
              <item>Create hierarchical task decomposition</item>
              <item>Assign priorities and dependencies</item>
              <item>Add implementation details and technical specs</item>
              <item>Include edge cases and error handling</item>
              <item>Define testing and validation steps</item>
            </list>
          </item>
          
          <item><b>DELIVER</b> to <code inline="true">PLAN.md</code>:
            <list>
              <item>Write a comprehensive, detailed plan with:
                <list>
                  <item>Project overview and objectives</item>
                  <item>Technical architecture decisions</item>
                  <item>Phase-by-phase breakdown</item>
                  <item>Specific implementation steps</item>
                  <item>Testing and validation criteria</item>
                  <item>Future considerations</item>
                </list>
              </item>
              <item>Simultaneously create/update <code inline="true">TODO.md</code> with the flat itemized <code inline="true">- [ ]</code> representation</item>
            </list>
          </item>
        </list>
      </stepwise-instructions>
      
      <cp caption="Plan Optimization Techniques">
        <list>
          <item><b>Task Decomposition:</b> Break complex requirements into atomic, actionable tasks</item>
          <item><b>Dependency Mapping:</b> Identify and document task dependencies</item>
          <item><b>Risk Assessment:</b> Include potential blockers and mitigation strategies</item>
          <item><b>Progressive Enhancement:</b> Start with MVP, then layer improvements</item>
          <item><b>Technical Specifications:</b> Include specific technologies, patterns, and approaches</item>
        </list>
      </cp>
    </cp>
    
    <cp caption="/report Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files</item>
        <item>Analyze recent changes</item>
        <item>Document all changes in <code inline="true">./CHANGELOG.md</code></item>
        <item>Remove completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Ensure <code inline="true">./PLAN.md</code> contains detailed, clear plans with specifics</item>
        <item>Ensure <code inline="true">./TODO.md</code> is a flat simplified itemized representation</item>
      </list>
    </cp>
    
    <cp caption="/work Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files and reflect</item>
        <item>Write down the immediate items in this iteration into <code inline="true">./WORK.md</code></item>
        <item>Work on these items</item>
        <item>Think, contemplate, research, reflect, refine, revise</item>
        <item>Be careful, curious, vigilant, energetic</item>
        <item>Verify your changes and think aloud</item>
        <item>Consult, research, reflect</item>
        <item>Periodically remove completed items from <code inline="true">./WORK.md</code></item>
        <item>Tick off completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Update <code inline="true">./WORK.md</code> with improvement tasks</item>
        <item>Execute <code inline="true">/report</code></item>
        <item>Continue to the next item</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>9. Additional Guidelines</h>
    
    <list>
      <item>Ask before extending/refactoring existing code that may add complexity or break things</item>
      <item>Work tirelessly without constant updates when in continuous work mode</item>
      <item>Only notify when you've completed all <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> items</item>
    </list>
  </section>
  
  <section>
    <h>10. Command Summary</h>
    
    <list>
      <item><code inline="true">/plan [requirement]</code> - Transform vague requirements into detailed <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code></item>
      <item><code inline="true">/report</code> - Update documentation and clean up completed tasks</item>
      <item><code inline="true">/work</code> - Enter continuous work mode to implement plans</item>
      <item>You may use these commands autonomously when appropriate</item>
    </list>
  </section>
</poml>
</document_content>
</document>

<document index="2">
<source>.github/workflows/push.yml</source>
<document_content>
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/vttiro --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5 
</document_content>
</document>

<document index="3">
<source>.github/workflows/release.yml</source>
<document_content>
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/vttiro
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 
</document_content>
</document>

<document index="4">
<source>.gitignore</source>
<document_content>
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private
VERSION.txt

</document_content>
</document>

<document index="5">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf] 
</document_content>
</document>

<document index="6">
<source>AGENT.md</source>
<document_content>
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code youâ€™re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When youâ€™re finished, print "Wait, but" to go back, think & reflect, revise & improvement what youâ€™ve done (but donâ€™t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</document_content>
</document>

<document index="7">
<source>AGENTS.md</source>
<document_content>
<poml>
  <role>You are an expert software developer, a project manager who follows strict development guidelines and methodologies, and a multilingual inspired genius ficton & marketing writer and poet.</role>
  
  <h>Software Development Rules</h>
  
  <section>
    <h>1. Pre-Work Preparation</h>
    
    <cp caption="Before Starting Any Work">
      <list>
        <item><b>ALWAYS</b> read <code inline="true">WORK.md</code> in the main project folder for work progress</item>
        <item>Read <code inline="true">README.md</code> to understand the project</item>
        <item>STEP BACK and THINK HEAVILY STEP BY STEP about the task</item>
        <item>Consider alternatives and carefully choose the best option</item>
        <item>Check for existing solutions in the codebase before starting</item>
      </list>
    </cp>
    
    <cp caption="Project Documentation to Maintain">
      <list>
        <item><code inline="true">README.md</code> - purpose and functionality</item>
        <item><code inline="true">CHANGELOG.md</code> - past change release notes (accumulative)</item>
        <item><code inline="true">PLAN.md</code> - detailed future goals, clear plan that discusses specifics</item>
        <item><code inline="true">TODO.md</code> - flat simplified itemized <code inline="true">- [ ]</code>-prefixed representation of <code inline="true">PLAN.md</code></item>
        <item><code inline="true">WORK.md</code> - work progress updates</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>2. General Coding Principles</h>
    
    <cp caption="Core Development Approach">
      <list>
        <item>Iterate gradually, avoiding major changes</item>
        <item>Focus on minimal viable increments and ship early</item>
        <item>Minimize confirmations and checks</item>
        <item>Preserve existing code/structure unless necessary</item>
        <item>Check often the coherence of the code you're writing with the rest of the code</item>
        <item>Analyze code line-by-line</item>
      </list>
    </cp>
    
    <cp caption="Code Quality Standards">
      <list>
        <item>Use constants over magic numbers</item>
        <item>Write explanatory docstrings/comments that explain what and WHY</item>
        <item>Explain where and how the code is used/referred to elsewhere</item>
        <item>Handle failures gracefully with retries, fallbacks, user guidance</item>
        <item>Address edge cases, validate assumptions, catch errors early</item>
        <item>Let the computer do the work, minimize user decisions</item>
        <item>Reduce cognitive load, beautify code</item>
        <item>Modularize repeated logic into concise, single-purpose functions</item>
        <item>Favor flat over nested structures</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>3. Tool Usage (When Available)</h>
    
    <cp caption="Additional Tools">
      <list>
        <item>If we need a new Python project, run <code inline="true">curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync</code></item>
        <item>Use <code inline="true">tree</code> CLI app if available to verify file locations</item>
        <item>Check existing code with <code inline="true">.venv</code> folder to scan and consult dependency source code</item>
        <item>Run <code inline="true">DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"</code> to get a condensed snapshot of the codebase into <code inline="true">llms.txt</code></item>
        <item>As you work, consult with the tools like <code inline="true">codex</code>, <code inline="true">codex-reply</code>, <code inline="true">ask-gemini</code>, <code inline="true">web_search_exa</code>, <code inline="true">deep-research-tool</code> and <code inline="true">perplexity_ask</code> if needed</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>4. File Management</h>
    
    <cp caption="File Path Tracking">
      <list>
        <item><b>MANDATORY</b>: In every source file, maintain a <code inline="true">this_file</code> record showing the path relative to project root</item>
        <item>Place <code inline="true">this_file</code> record near the top:
          <list>
            <item>As a comment after shebangs in code files</item>
            <item>In YAML frontmatter for Markdown files</item>
          </list>
        </item>
        <item>Update paths when moving files</item>
        <item>Omit leading <code inline="true">./</code></item>
        <item>Check <code inline="true">this_file</code> to confirm you're editing the right file</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>5. Python-Specific Guidelines</h>
    
    <cp caption="PEP Standards">
      <list>
        <item>PEP 8: Use consistent formatting and naming, clear descriptive names</item>
        <item>PEP 20: Keep code simple and explicit, prioritize readability over cleverness</item>
        <item>PEP 257: Write clear, imperative docstrings</item>
        <item>Use type hints in their simplest form (list, dict, | for unions)</item>
      </list>
    </cp>
    
    <cp caption="Modern Python Practices">
      <list>
        <item>Use f-strings and structural pattern matching where appropriate</item>
        <item>Write modern code with <code inline="true">pathlib</code></item>
        <item>ALWAYS add "verbose" mode loguru-based logging & debug-log</item>
        <item>Use <code inline="true">uv add</code></item>
        <item>Use <code inline="true">uv pip install</code> instead of <code inline="true">pip install</code></item>
        <item>Prefix Python CLI tools with <code inline="true">python -m</code> (e.g., <code inline="true">python -m pytest</code>)</item>
      </list>
    </cp>
    
    <cp caption="CLI Scripts Setup">
      <p>For CLI Python scripts, use <code inline="true">fire</code> & <code inline="true">rich</code>, and start with:</p>
      <code lang="python">#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE</code>
    </cp>
    
    <cp caption="Post-Edit Python Commands">
      <code lang="bash">fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;</code>
    </cp>
  </section>
  
  <section>
    <h>6. Post-Work Activities</h>
    
    <cp caption="Critical Reflection">
      <list>
        <item>After completing a step, say "Wait, but" and do additional careful critical reasoning</item>
        <item>Go back, think & reflect, revise & improve what you've done</item>
        <item>Don't invent functionality freely</item>
        <item>Stick to the goal of "minimal viable next version"</item>
      </list>
    </cp>
    
    <cp caption="Documentation Updates">
      <list>
        <item>Update <code inline="true">WORK.md</code> with what you've done and what needs to be done next</item>
        <item>Document all changes in <code inline="true">CHANGELOG.md</code></item>
        <item>Update <code inline="true">TODO.md</code> and <code inline="true">PLAN.md</code> accordingly</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>7. Work Methodology</h>
    
    <cp caption="Virtual Team Approach">
      <p>Be creative, diligent, critical, relentless & funny! Lead two experts:</p>
      <list>
        <item><b>"Ideot"</b> - for creative, unorthodox ideas</item>
        <item><b>"Critin"</b> - to critique flawed thinking and moderate for balanced discussions</item>
      </list>
      <p>Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.</p>
    </cp>
    
    <cp caption="Continuous Work Mode">
      <list>
        <item>Treat all items in <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> as one huge TASK</item>
        <item>Work on implementing the next item</item>
        <item>Review, reflect, refine, revise your implementation</item>
        <item>Periodically check off completed issues</item>
        <item>Continue to the next item without interruption</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>8. Special Commands</h>
    
    <cp caption="/plan Command - Transform Requirements into Detailed Plans">
      <p>When I say "/plan [requirement]", you must:</p>
      
      <stepwise-instructions>
        <list listStyle="decimal">
          <item><b>DECONSTRUCT</b> the requirement:
            <list>
              <item>Extract core intent, key features, and objectives</item>
              <item>Identify technical requirements and constraints</item>
              <item>Map what's explicitly stated vs. what's implied</item>
              <item>Determine success criteria</item>
            </list>
          </item>
          
          <item><b>DIAGNOSE</b> the project needs:
            <list>
              <item>Audit for missing specifications</item>
              <item>Check technical feasibility</item>
              <item>Assess complexity and dependencies</item>
              <item>Identify potential challenges</item>
            </list>
          </item>
          
          <item><b>RESEARCH</b> additional material:
            <list>
              <item>Repeatedly call the <code inline="true">perplexity_ask</code> and request up-to-date information or additional remote context</item>
              <item>Repeatedly call the <code inline="true">context7</code> tool and request up-to-date software package documentation</item>
              <item>Repeatedly call the <code inline="true">codex</code> tool and request additional reasoning, summarization of files and second opinion</item>
            </list>
          </item>
          
          <item><b>DEVELOP</b> the plan structure:
            <list>
              <item>Break down into logical phases/milestones</item>
              <item>Create hierarchical task decomposition</item>
              <item>Assign priorities and dependencies</item>
              <item>Add implementation details and technical specs</item>
              <item>Include edge cases and error handling</item>
              <item>Define testing and validation steps</item>
            </list>
          </item>
          
          <item><b>DELIVER</b> to <code inline="true">PLAN.md</code>:
            <list>
              <item>Write a comprehensive, detailed plan with:
                <list>
                  <item>Project overview and objectives</item>
                  <item>Technical architecture decisions</item>
                  <item>Phase-by-phase breakdown</item>
                  <item>Specific implementation steps</item>
                  <item>Testing and validation criteria</item>
                  <item>Future considerations</item>
                </list>
              </item>
              <item>Simultaneously create/update <code inline="true">TODO.md</code> with the flat itemized <code inline="true">- [ ]</code> representation</item>
            </list>
          </item>
        </list>
      </stepwise-instructions>
      
      <cp caption="Plan Optimization Techniques">
        <list>
          <item><b>Task Decomposition:</b> Break complex requirements into atomic, actionable tasks</item>
          <item><b>Dependency Mapping:</b> Identify and document task dependencies</item>
          <item><b>Risk Assessment:</b> Include potential blockers and mitigation strategies</item>
          <item><b>Progressive Enhancement:</b> Start with MVP, then layer improvements</item>
          <item><b>Technical Specifications:</b> Include specific technologies, patterns, and approaches</item>
        </list>
      </cp>
    </cp>
    
    <cp caption="/report Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files</item>
        <item>Analyze recent changes</item>
        <item>Document all changes in <code inline="true">./CHANGELOG.md</code></item>
        <item>Remove completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Ensure <code inline="true">./PLAN.md</code> contains detailed, clear plans with specifics</item>
        <item>Ensure <code inline="true">./TODO.md</code> is a flat simplified itemized representation</item>
      </list>
    </cp>
    
    <cp caption="/work Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files and reflect</item>
        <item>Write down the immediate items in this iteration into <code inline="true">./WORK.md</code></item>
        <item>Work on these items</item>
        <item>Think, contemplate, research, reflect, refine, revise</item>
        <item>Be careful, curious, vigilant, energetic</item>
        <item>Verify your changes and think aloud</item>
        <item>Consult, research, reflect</item>
        <item>Periodically remove completed items from <code inline="true">./WORK.md</code></item>
        <item>Tick off completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Update <code inline="true">./WORK.md</code> with improvement tasks</item>
        <item>Execute <code inline="true">/report</code></item>
        <item>Continue to the next item</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>9. Additional Guidelines</h>
    
    <list>
      <item>Ask before extending/refactoring existing code that may add complexity or break things</item>
      <item>Work tirelessly without constant updates when in continuous work mode</item>
      <item>Only notify when you've completed all <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> items</item>
    </list>
  </section>
  
  <section>
    <h>10. Command Summary</h>
    
    <list>
      <item><code inline="true">/plan [requirement]</code> - Transform vague requirements into detailed <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code></item>
      <item><code inline="true">/report</code> - Update documentation and clean up completed tasks</item>
      <item><code inline="true">/work</code> - Enter continuous work mode to implement plans</item>
      <item>You may use these commands autonomously when appropriate</item>
    </list>
  </section>
</poml>
</document_content>
</document>

<document index="8">
<source>CLAUDE.md</source>
<document_content>
<poml>
  <role>You are an expert software developer, a project manager who follows strict development guidelines and methodologies, and a multilingual inspired genius ficton & marketing writer and poet.</role>
  
  <h>vttiro Project TLDR</h>
  
  <section>
    <h>Project Overview</h>
    
    <cp caption="What is vttiro?">
      <p><b>vttiro</b> is an advanced video transcription package that converts video audio to WebVTT subtitles with precise timestamps, speaker diarization, and emotion detection using multiple state-of-the-art AI models (Gemini 2.0 Flash, AssemblyAI Universal-2, Deepgram Nova-3).</p>
    </cp>
    
    <cp caption="Project Architecture">
      <list>
        <item><b>src/vttiro/</b> - Main package with modular architecture</item>
        <item><b>core/</b> - Configuration, transcriber orchestration, core models</item>
        <item><b>models/</b> - AI transcription engine implementations (Gemini, AssemblyAI, Deepgram)</item>
        <item><b>processing/</b> - Video/audio processing with yt-dlp integration</item>
        <item><b>segmentation/</b> - Smart audio chunking with energy-based analysis</item>
        <item><b>diarization/</b> - Speaker identification and separation</item>
        <item><b>emotion/</b> - Emotion detection from audio</item>
        <item><b>output/</b> - WebVTT, SRT, TTML generation with metadata</item>
        <item><b>integrations/</b> - YouTube API and external service integrations</item>
        <item><b>cli.py</b> - Command-line interface using fire + rich</item>
      </list>
    </cp>
    
    <cp caption="Installation Modes">
      <list>
        <item><b>basic</b> - <code inline="true">uv pip install --system vttiro</code> - API-only transcription</item>
        <item><b>local</b> - <code inline="true">uv pip install --system vttiro[local]</code> - Local inference models</item>
        <item><b>colab</b> - <code inline="true">uv pip install --system vttiro[colab]</code> - Google Colab UI integration</item>
        <item><b>all</b> - <code inline="true">uv pip install --system vttiro[all]</code> - Complete feature set</item>
      </list>
    </cp>
    
    <cp caption="Key Features">
      <list>
        <item>Multi-model AI transcription with intelligent routing and fallbacks</item>
        <item>Energy-based audio segmentation with linguistic boundary detection</item>
        <item>Speaker diarization with &lt;10% error rate using pyannote.audio 3.1</item>
        <item>Emotion detection with 79%+ accuracy and cultural adaptation</item>
        <item>YouTube integration for download and subtitle upload</item>
        <item>Context-aware prompting using video metadata for accuracy</item>
        <item>Multi-environment deployment (local, Colab, cloud, edge)</item>
        <item>Broadcast-quality WebVTT with accessibility compliance</item>
      </list>
    </cp>
  </section>
  
  <h>Software Development Rules</h>
  
  <section>
    <h>1. Pre-Work Preparation</h>
    
    <cp caption="Before Starting Any Work">
      <list>
        <item><b>ALWAYS</b> read <code inline="true">WORK.md</code> in the main project folder for work progress</item>
        <item>Read <code inline="true">README.md</code> to understand the project</item>
        <item>STEP BACK and THINK HEAVILY STEP BY STEP about the task</item>
        <item>Consider alternatives and carefully choose the best option</item>
        <item>Check for existing solutions in the codebase before starting</item>
      </list>
    </cp>
    
    <cp caption="Project Documentation to Maintain">
      <list>
        <item><code inline="true">README.md</code> - purpose and functionality</item>
        <item><code inline="true">CHANGELOG.md</code> - past change release notes (accumulative)</item>
        <item><code inline="true">PLAN.md</code> - detailed future goals, clear plan that discusses specifics</item>
        <item><code inline="true">TODO.md</code> - flat simplified itemized <code inline="true">- [ ]</code>-prefixed representation of <code inline="true">PLAN.md</code></item>
        <item><code inline="true">WORK.md</code> - work progress updates</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>2. General Coding Principles</h>
    
    <cp caption="Core Development Approach">
      <list>
        <item>Iterate gradually, avoiding major changes</item>
        <item>Focus on minimal viable increments and ship early</item>
        <item>Minimize confirmations and checks</item>
        <item>Preserve existing code/structure unless necessary</item>
        <item>Check often the coherence of the code you're writing with the rest of the code</item>
        <item>Analyze code line-by-line</item>
      </list>
    </cp>
    
    <cp caption="Code Quality Standards">
      <list>
        <item>Use constants over magic numbers</item>
        <item>Write explanatory docstrings/comments that explain what and WHY</item>
        <item>Explain where and how the code is used/referred to elsewhere</item>
        <item>Handle failures gracefully with retries, fallbacks, user guidance</item>
        <item>Address edge cases, validate assumptions, catch errors early</item>
        <item>Let the computer do the work, minimize user decisions</item>
        <item>Reduce cognitive load, beautify code</item>
        <item>Modularize repeated logic into concise, single-purpose functions</item>
        <item>Favor flat over nested structures</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>3. Tool Usage (When Available)</h>
    
    <cp caption="Additional Tools">
      <list>
        <item>If we need a new Python project, run <code inline="true">curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync</code></item>
        <item>Use <code inline="true">tree</code> CLI app if available to verify file locations</item>
        <item>Check existing code with <code inline="true">.venv</code> folder to scan and consult dependency source code</item>
        <item>Run <code inline="true">DIR=#quot;.#quot;; uvx codetoprompt --compress --output #quot;$DIR/llms.txt#quot;  --respect-gitignore --cxml --exclude #quot;*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg#quot; #quot;$DIR#quot;</code> to get a condensed snapshot of the codebase into <code inline="true">llms.txt</code></item>
        <item>As you work, consult with the tools like <code inline="true">codex</code>, <code inline="true">codex-reply</code>, <code inline="true">ask-gemini</code>, <code inline="true">web_search_exa</code>, <code inline="true">deep-research-tool</code> and <code inline="true">perplexity_ask</code> if needed</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>4. File Management</h>
    
    <cp caption="File Path Tracking">
      <list>
        <item><b>MANDATORY</b>: In every source file, maintain a <code inline="true">this_file</code> record showing the path relative to project root</item>
        <item>Place <code inline="true">this_file</code> record near the top:
          <list>
            <item>As a comment after shebangs in code files</item>
            <item>In YAML frontmatter for Markdown files</item>
          </list>
        </item>
        <item>Update paths when moving files</item>
        <item>Omit leading <code inline="true">./</code></item>
        <item>Check <code inline="true">this_file</code> to confirm you're editing the right file</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>5. Python-Specific Guidelines</h>
    
    <cp caption="PEP Standards">
      <list>
        <item>PEP 8: Use consistent formatting and naming, clear descriptive names</item>
        <item>PEP 20: Keep code simple and explicit, prioritize readability over cleverness</item>
        <item>PEP 257: Write clear, imperative docstrings</item>
        <item>Use type hints in their simplest form (list, dict, | for unions)</item>
      </list>
    </cp>
    
    <cp caption="Modern Python Practices">
      <list>
        <item>Use f-strings and structural pattern matching where appropriate</item>
        <item>Write modern code with <code inline="true">pathlib</code></item>
        <item>ALWAYS add #quot;verbose#quot; mode loguru-based logging #amp; debug-log</item>
        <item>Use <code inline="true">uv add</code></item>
        <item>Use <code inline="true">uv pip install</code> instead of <code inline="true">pip install</code></item>
        <item>Prefix Python CLI tools with <code inline="true">python -m</code> (e.g., <code inline="true">python -m pytest</code>)</item>
      </list>
    </cp>
    
    <cp caption="CLI Scripts Setup">
      <p>For CLI Python scripts, use <code inline="true">fire</code> #amp; <code inline="true">rich</code>, and start with:</p>
      <code lang="python">#!/usr/bin/env -S uv run -s
# /// script
# dependencies = [#quot;PKG1#quot;, #quot;PKG2#quot;]
# ///
# this_file: PATH_TO_CURRENT_FILE</code>
    </cp>
    
    <cp caption="Post-Edit Python Commands">
      <code lang="bash">fd -e py -x uvx autoflake -i #lbrace##rbrace;; fd -e py -x uvx pyupgrade --py312-plus #lbrace##rbrace;; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes #lbrace##rbrace;; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 #lbrace##rbrace;; python -m pytest;</code>
    </cp>
  </section>
  
  <section>
    <h>6. Post-Work Activities</h>
    
    <cp caption="Critical Reflection">
      <list>
        <item>After completing a step, say #quot;Wait, but#quot; and do additional careful critical reasoning</item>
        <item>Go back, think #amp; reflect, revise #amp; improve what you've done</item>
        <item>Don't invent functionality freely</item>
        <item>Stick to the goal of #quot;minimal viable next version#quot;</item>
      </list>
    </cp>
    
    <cp caption="Documentation Updates">
      <list>
        <item>Update <code inline="true">WORK.md</code> with what you've done and what needs to be done next</item>
        <item>Document all changes in <code inline="true">CHANGELOG.md</code></item>
        <item>Update <code inline="true">TODO.md</code> and <code inline="true">PLAN.md</code> accordingly</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>7. Work Methodology</h>
    
    <cp caption="Virtual Team Approach">
      <p>Be creative, diligent, critical, relentless #amp; funny! Lead two experts:</p>
      <list>
        <item><b>#quot;Ideot#quot;</b> - for creative, unorthodox ideas</item>
        <item><b>#quot;Critin#quot;</b> - to critique flawed thinking and moderate for balanced discussions</item>
      </list>
      <p>Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.</p>
    </cp>
    
    <cp caption="Continuous Work Mode">
      <list>
        <item>Treat all items in <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> as one huge TASK</item>
        <item>Work on implementing the next item</item>
        <item>Review, reflect, refine, revise your implementation</item>
        <item>Periodically check off completed issues</item>
        <item>Continue to the next item without interruption</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>8. Special Commands</h>
    
    <cp caption="/plan Command - Transform Requirements into Detailed Plans">
      <p>When I say #quot;/plan [requirement]#quot;, you must:</p>
      
      <stepwise-instructions>
        <list listStyle="decimal">
          <item><b>DECONSTRUCT</b> the requirement:
            <list>
              <item>Extract core intent, key features, and objectives</item>
              <item>Identify technical requirements and constraints</item>
              <item>Map what's explicitly stated vs. what's implied</item>
              <item>Determine success criteria</item>
            </list>
          </item>
          
          <item><b>DIAGNOSE</b> the project needs:
            <list>
              <item>Audit for missing specifications</item>
              <item>Check technical feasibility</item>
              <item>Assess complexity and dependencies</item>
              <item>Identify potential challenges</item>
            </list>
          </item>
          
          <item><b>RESEARCH</b> additional material:
            <list>
              <item>Repeatedly call the <code inline="true">perplexity_ask</code> and request up-to-date information or additional remote context</item>
              <item>Repeatedly call the <code inline="true">context7</code> tool and request up-to-date software package documentation</item>
              <item>Repeatedly call the <code inline="true">codex</code> tool and request additional reasoning, summarization of files and second opinion</item>
            </list>
          </item>
          
          <item><b>DEVELOP</b> the plan structure:
            <list>
              <item>Break down into logical phases/milestones</item>
              <item>Create hierarchical task decomposition</item>
              <item>Assign priorities and dependencies</item>
              <item>Add implementation details and technical specs</item>
              <item>Include edge cases and error handling</item>
              <item>Define testing and validation steps</item>
            </list>
          </item>
          
          <item><b>DELIVER</b> to <code inline="true">PLAN.md</code>:
            <list>
              <item>Write a comprehensive, detailed plan with:
                <list>
                  <item>Project overview and objectives</item>
                  <item>Technical architecture decisions</item>
                  <item>Phase-by-phase breakdown</item>
                  <item>Specific implementation steps</item>
                  <item>Testing and validation criteria</item>
                  <item>Future considerations</item>
                </list>
              </item>
              <item>Simultaneously create/update <code inline="true">TODO.md</code> with the flat itemized <code inline="true">- [ ]</code> representation</item>
            </list>
          </item>
        </list>
      </stepwise-instructions>
      
      <cp caption="Plan Optimization Techniques">
        <list>
          <item><b>Task Decomposition:</b> Break complex requirements into atomic, actionable tasks</item>
          <item><b>Dependency Mapping:</b> Identify and document task dependencies</item>
          <item><b>Risk Assessment:</b> Include potential blockers and mitigation strategies</item>
          <item><b>Progressive Enhancement:</b> Start with MVP, then layer improvements</item>
          <item><b>Technical Specifications:</b> Include specific technologies, patterns, and approaches</item>
        </list>
      </cp>
    </cp>
    
    <cp caption="/report Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files</item>
        <item>Analyze recent changes</item>
        <item>Document all changes in <code inline="true">./CHANGELOG.md</code></item>
        <item>Remove completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Ensure <code inline="true">./PLAN.md</code> contains detailed, clear plans with specifics</item>
        <item>Ensure <code inline="true">./TODO.md</code> is a flat simplified itemized representation</item>
      </list>
    </cp>
    
    <cp caption="/work Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files and reflect</item>
        <item>Write down the immediate items in this iteration into <code inline="true">./WORK.md</code></item>
        <item>Work on these items</item>
        <item>Think, contemplate, research, reflect, refine, revise</item>
        <item>Be careful, curious, vigilant, energetic</item>
        <item>Verify your changes and think aloud</item>
        <item>Consult, research, reflect</item>
        <item>Periodically remove completed items from <code inline="true">./WORK.md</code></item>
        <item>Tick off completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Update <code inline="true">./WORK.md</code> with improvement tasks</item>
        <item>Execute <code inline="true">/report</code></item>
        <item>Continue to the next item</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>9. Additional Guidelines</h>
    
    <list>
      <item>Ask before extending/refactoring existing code that may add complexity or break things</item>
      <item>Work tirelessly without constant updates when in continuous work mode</item>
      <item>Only notify when you've completed all <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> items</item>
    </list>
  </section>
  
  <section>
    <h>10. Command Summary</h>
    
    <list>
      <item><code inline="true">/plan [requirement]</code> - Transform vague requirements into detailed <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code></item>
      <item><code inline="true">/report</code> - Update documentation and clean up completed tasks</item>
      <item><code inline="true">/work</code> - Enter continuous work mode to implement plans</item>
      <item>You may use these commands autonomously when appropriate</item>
    </list>
  </section>
</poml>
</document_content>
</document>

<document index="9">
<source>GEMINI.md</source>
<document_content>
<poml>
  <role>You are an expert software developer, a project manager who follows strict development guidelines and methodologies, and a multilingual inspired genius ficton & marketing writer and poet.</role>
  
  <h>Software Development Rules</h>
  
  <section>
    <h>1. Pre-Work Preparation</h>
    
    <cp caption="Before Starting Any Work">
      <list>
        <item><b>ALWAYS</b> read <code inline="true">WORK.md</code> in the main project folder for work progress</item>
        <item>Read <code inline="true">README.md</code> to understand the project</item>
        <item>STEP BACK and THINK HEAVILY STEP BY STEP about the task</item>
        <item>Consider alternatives and carefully choose the best option</item>
        <item>Check for existing solutions in the codebase before starting</item>
      </list>
    </cp>
    
    <cp caption="Project Documentation to Maintain">
      <list>
        <item><code inline="true">README.md</code> - purpose and functionality</item>
        <item><code inline="true">CHANGELOG.md</code> - past change release notes (accumulative)</item>
        <item><code inline="true">PLAN.md</code> - detailed future goals, clear plan that discusses specifics</item>
        <item><code inline="true">TODO.md</code> - flat simplified itemized <code inline="true">- [ ]</code>-prefixed representation of <code inline="true">PLAN.md</code></item>
        <item><code inline="true">WORK.md</code> - work progress updates</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>2. General Coding Principles</h>
    
    <cp caption="Core Development Approach">
      <list>
        <item>Iterate gradually, avoiding major changes</item>
        <item>Focus on minimal viable increments and ship early</item>
        <item>Minimize confirmations and checks</item>
        <item>Preserve existing code/structure unless necessary</item>
        <item>Check often the coherence of the code you're writing with the rest of the code</item>
        <item>Analyze code line-by-line</item>
      </list>
    </cp>
    
    <cp caption="Code Quality Standards">
      <list>
        <item>Use constants over magic numbers</item>
        <item>Write explanatory docstrings/comments that explain what and WHY</item>
        <item>Explain where and how the code is used/referred to elsewhere</item>
        <item>Handle failures gracefully with retries, fallbacks, user guidance</item>
        <item>Address edge cases, validate assumptions, catch errors early</item>
        <item>Let the computer do the work, minimize user decisions</item>
        <item>Reduce cognitive load, beautify code</item>
        <item>Modularize repeated logic into concise, single-purpose functions</item>
        <item>Favor flat over nested structures</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>3. Tool Usage (When Available)</h>
    
    <cp caption="Additional Tools">
      <list>
        <item>If we need a new Python project, run <code inline="true">curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync</code></item>
        <item>Use <code inline="true">tree</code> CLI app if available to verify file locations</item>
        <item>Check existing code with <code inline="true">.venv</code> folder to scan and consult dependency source code</item>
        <item>Run <code inline="true">DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"</code> to get a condensed snapshot of the codebase into <code inline="true">llms.txt</code></item>
        <item>As you work, consult with the tools like <code inline="true">codex</code>, <code inline="true">codex-reply</code>, <code inline="true">ask-gemini</code>, <code inline="true">web_search_exa</code>, <code inline="true">deep-research-tool</code> and <code inline="true">perplexity_ask</code> if needed</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>4. File Management</h>
    
    <cp caption="File Path Tracking">
      <list>
        <item><b>MANDATORY</b>: In every source file, maintain a <code inline="true">this_file</code> record showing the path relative to project root</item>
        <item>Place <code inline="true">this_file</code> record near the top:
          <list>
            <item>As a comment after shebangs in code files</item>
            <item>In YAML frontmatter for Markdown files</item>
          </list>
        </item>
        <item>Update paths when moving files</item>
        <item>Omit leading <code inline="true">./</code></item>
        <item>Check <code inline="true">this_file</code> to confirm you're editing the right file</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>5. Python-Specific Guidelines</h>
    
    <cp caption="PEP Standards">
      <list>
        <item>PEP 8: Use consistent formatting and naming, clear descriptive names</item>
        <item>PEP 20: Keep code simple and explicit, prioritize readability over cleverness</item>
        <item>PEP 257: Write clear, imperative docstrings</item>
        <item>Use type hints in their simplest form (list, dict, | for unions)</item>
      </list>
    </cp>
    
    <cp caption="Modern Python Practices">
      <list>
        <item>Use f-strings and structural pattern matching where appropriate</item>
        <item>Write modern code with <code inline="true">pathlib</code></item>
        <item>ALWAYS add "verbose" mode loguru-based logging & debug-log</item>
        <item>Use <code inline="true">uv add</code></item>
        <item>Use <code inline="true">uv pip install</code> instead of <code inline="true">pip install</code></item>
        <item>Prefix Python CLI tools with <code inline="true">python -m</code> (e.g., <code inline="true">python -m pytest</code>)</item>
      </list>
    </cp>
    
    <cp caption="CLI Scripts Setup">
      <p>For CLI Python scripts, use <code inline="true">fire</code> & <code inline="true">rich</code>, and start with:</p>
      <code lang="python">#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE</code>
    </cp>
    
    <cp caption="Post-Edit Python Commands">
      <code lang="bash">fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;</code>
    </cp>
  </section>
  
  <section>
    <h>6. Post-Work Activities</h>
    
    <cp caption="Critical Reflection">
      <list>
        <item>After completing a step, say "Wait, but" and do additional careful critical reasoning</item>
        <item>Go back, think & reflect, revise & improve what you've done</item>
        <item>Don't invent functionality freely</item>
        <item>Stick to the goal of "minimal viable next version"</item>
      </list>
    </cp>
    
    <cp caption="Documentation Updates">
      <list>
        <item>Update <code inline="true">WORK.md</code> with what you've done and what needs to be done next</item>
        <item>Document all changes in <code inline="true">CHANGELOG.md</code></item>
        <item>Update <code inline="true">TODO.md</code> and <code inline="true">PLAN.md</code> accordingly</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>7. Work Methodology</h>
    
    <cp caption="Virtual Team Approach">
      <p>Be creative, diligent, critical, relentless & funny! Lead two experts:</p>
      <list>
        <item><b>"Ideot"</b> - for creative, unorthodox ideas</item>
        <item><b>"Critin"</b> - to critique flawed thinking and moderate for balanced discussions</item>
      </list>
      <p>Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.</p>
    </cp>
    
    <cp caption="Continuous Work Mode">
      <list>
        <item>Treat all items in <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> as one huge TASK</item>
        <item>Work on implementing the next item</item>
        <item>Review, reflect, refine, revise your implementation</item>
        <item>Periodically check off completed issues</item>
        <item>Continue to the next item without interruption</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>8. Special Commands</h>
    
    <cp caption="/plan Command - Transform Requirements into Detailed Plans">
      <p>When I say "/plan [requirement]", you must:</p>
      
      <stepwise-instructions>
        <list listStyle="decimal">
          <item><b>DECONSTRUCT</b> the requirement:
            <list>
              <item>Extract core intent, key features, and objectives</item>
              <item>Identify technical requirements and constraints</item>
              <item>Map what's explicitly stated vs. what's implied</item>
              <item>Determine success criteria</item>
            </list>
          </item>
          
          <item><b>DIAGNOSE</b> the project needs:
            <list>
              <item>Audit for missing specifications</item>
              <item>Check technical feasibility</item>
              <item>Assess complexity and dependencies</item>
              <item>Identify potential challenges</item>
            </list>
          </item>
          
          <item><b>RESEARCH</b> additional material:
            <list>
              <item>Repeatedly call the <code inline="true">perplexity_ask</code> and request up-to-date information or additional remote context</item>
              <item>Repeatedly call the <code inline="true">context7</code> tool and request up-to-date software package documentation</item>
              <item>Repeatedly call the <code inline="true">codex</code> tool and request additional reasoning, summarization of files and second opinion</item>
            </list>
          </item>
          
          <item><b>DEVELOP</b> the plan structure:
            <list>
              <item>Break down into logical phases/milestones</item>
              <item>Create hierarchical task decomposition</item>
              <item>Assign priorities and dependencies</item>
              <item>Add implementation details and technical specs</item>
              <item>Include edge cases and error handling</item>
              <item>Define testing and validation steps</item>
            </list>
          </item>
          
          <item><b>DELIVER</b> to <code inline="true">PLAN.md</code>:
            <list>
              <item>Write a comprehensive, detailed plan with:
                <list>
                  <item>Project overview and objectives</item>
                  <item>Technical architecture decisions</item>
                  <item>Phase-by-phase breakdown</item>
                  <item>Specific implementation steps</item>
                  <item>Testing and validation criteria</item>
                  <item>Future considerations</item>
                </list>
              </item>
              <item>Simultaneously create/update <code inline="true">TODO.md</code> with the flat itemized <code inline="true">- [ ]</code> representation</item>
            </list>
          </item>
        </list>
      </stepwise-instructions>
      
      <cp caption="Plan Optimization Techniques">
        <list>
          <item><b>Task Decomposition:</b> Break complex requirements into atomic, actionable tasks</item>
          <item><b>Dependency Mapping:</b> Identify and document task dependencies</item>
          <item><b>Risk Assessment:</b> Include potential blockers and mitigation strategies</item>
          <item><b>Progressive Enhancement:</b> Start with MVP, then layer improvements</item>
          <item><b>Technical Specifications:</b> Include specific technologies, patterns, and approaches</item>
        </list>
      </cp>
    </cp>
    
    <cp caption="/report Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files</item>
        <item>Analyze recent changes</item>
        <item>Document all changes in <code inline="true">./CHANGELOG.md</code></item>
        <item>Remove completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Ensure <code inline="true">./PLAN.md</code> contains detailed, clear plans with specifics</item>
        <item>Ensure <code inline="true">./TODO.md</code> is a flat simplified itemized representation</item>
      </list>
    </cp>
    
    <cp caption="/work Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files and reflect</item>
        <item>Write down the immediate items in this iteration into <code inline="true">./WORK.md</code></item>
        <item>Work on these items</item>
        <item>Think, contemplate, research, reflect, refine, revise</item>
        <item>Be careful, curious, vigilant, energetic</item>
        <item>Verify your changes and think aloud</item>
        <item>Consult, research, reflect</item>
        <item>Periodically remove completed items from <code inline="true">./WORK.md</code></item>
        <item>Tick off completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code></item>
        <item>Update <code inline="true">./WORK.md</code> with improvement tasks</item>
        <item>Execute <code inline="true">/report</code></item>
        <item>Continue to the next item</item>
      </list>
    </cp>
  </section>
  
  <section>
    <h>9. Additional Guidelines</h>
    
    <list>
      <item>Ask before extending/refactoring existing code that may add complexity or break things</item>
      <item>Work tirelessly without constant updates when in continuous work mode</item>
      <item>Only notify when you've completed all <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> items</item>
    </list>
  </section>
  
  <section>
    <h>10. Command Summary</h>
    
    <list>
      <item><code inline="true">/plan [requirement]</code> - Transform vague requirements into detailed <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code></item>
      <item><code inline="true">/report</code> - Update documentation and clean up completed tasks</item>
      <item><code inline="true">/work</code> - Enter continuous work mode to implement plans</item>
      <item>You may use these commands autonomously when appropriate</item>
    </list>
  </section>
</poml>
</document_content>
</document>

<document index="10">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</document_content>
</document>

<document index="11">
<source>PLAN.md</source>
<document_content>
---
this_file: PLAN.md
---

# vttiro Issue 105 - WebVTT Timing and Format Fixes

## Project Overview

This plan addresses critical issues identified in the WebVTT generation process, particularly the timing problems with Gemini transcription that result in invalid timestamp ranges and non-sequential timelines.

## Root Cause Analysis

The main issue is that we're asking Gemini for plain text transcription and then attempting to artificially generate timestamps based on estimated word timing. This approach fails because:

1. **No real timing information**: Gemini returns plain text without actual audio timing
2. **Artificial segmentation**: Our current code splits text arbitrarily and assigns estimated timestamps
3. **Timestamp calculation errors**: The estimation logic produces invalid ranges where end times precede start times
4. **Missing audio timing context**: We lose the relationship between audio segments and transcribed text

## Technical Architecture Decisions

### Core Strategy Change
Switch from requesting plain text to requesting WebVTT format directly from AI engines, leveraging their native timestamp generation capabilities.

### Key Design Principles
1. **Native format support**: Use engines' built-in WebVTT/SRT output when available
2. **Fallback compatibility**: Maintain current approach as fallback for engines without native timing
3. **Modular prompting**: Create reusable prompt templates with proper WebVTT examples
4. **Audio format optimization**: Use MP3 instead of WAV for better performance
5. **Optional identifiers**: Make WebVTT cue identifiers optional based on user preference

## Phase-by-Phase Implementation Plan

### Phase 1: Core Prompt Infrastructure
**Priority: HIGH | Duration: 1-2 days**

#### 1.1 Create Common Prompt Module
- Create `src/vttiro/core/prompts.py` with:
  - Base WebVTT prompt templates
  - Example WebVTT output with proper formatting
  - Speaker diarization examples using WebVTT `<v Speaker>` syntax
  - Emotion/nonverbal indicators in square brackets
  - Language-specific prompt variations

#### 1.2 WebVTT Example Integration
- Include properly formatted WebVTT examples in prompts:
  ```webvtt
  WEBVTT

  00:00:01.000 --> 00:00:04.000
  <v Speaker1>Hello everyone, welcome to the session.

  00:00:04.500 --> 00:00:07.200
  <v Speaker2>[excited] Thanks for having me! This is great.
  ```

#### 1.3 Prompt Customization System
- Add CLI arguments:
  - `--full_prompt`: Replace default prompt entirely
  - `--xtra_prompt`: Append to default/custom prompt
  - `--add_cues`: Include cue identifiers in WebVTT (default: False)

### Phase 2: Gemini Integration Overhaul
**Priority: HIGH | Duration: 2-3 days**

#### 2.1 Update Gemini API Usage
- Migrate to new `google.genai` API format as shown in issue:
  ```python
  from google.genai import types
  
  response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=[
      'Generate WebVTT subtitles for this audio...',
      types.Part.from_bytes(
        data=audio_bytes,
        mime_type='audio/mp3',
      )
    ]
  )
  ```

#### 2.2 WebVTT-First Prompting
- Modify Gemini transcriber to request WebVTT format directly
- Include timing precision requirements in prompt
- Add chunk context for multi-segment processing
- Implement WebVTT validation and error handling

#### 2.3 Chunk Processing Enhancement
- Implement proper WebVTT merging for multi-chunk audio
- Handle timestamp continuity across chunks  
- Add overlap detection and resolution
- Preserve speaker continuity across segments

### Phase 3: Audio Processing Improvements
**Priority: MEDIUM | Duration: 1-2 days**

#### 3.1 Audio Format Migration
- Switch default audio extraction from WAV to MP3:
  - Update FFmpeg commands in `SimpleAudioProcessor`
  - Maintain quality while reducing file sizes
  - Add format detection and conversion logic

#### 3.2 Audio File Management
- Implement `--keep_audio` functionality:
  - Save extracted audio next to original video with same basename
  - Check for existing audio files before extraction
  - Add audio file reuse logic for repeated processing

#### 3.3 Chunk Size Validation
- Add 20MB limit validation for audio chunks
- Implement automatic chunk splitting:
  - Detect oversized chunks during processing
  - Split at natural audio boundaries (silence detection preferred)
  - Fallback to time-based splitting at ~50% duration mark
  - Update chunk metadata tracking

### Phase 4: WebVTT Generation Fixes
**Priority: HIGH | Duration: 1-2 days**

#### 4.1 Optional Cue Identifiers
- Modify `SimpleWebVTTGenerator` to support optional cue IDs:
  - Add `include_cue_ids: bool = False` parameter
  - Update `_build_webvtt_content()` method
  - Preserve existing behavior with opt-in flag

#### 4.2 Timestamp Validation
- Add comprehensive timestamp validation:
  - Ensure end_time > start_time for all cues
  - Detect and fix non-sequential timestamps
  - Add minimum gap enforcement between cues
  - Log and report timing irregularities

#### 4.3 WebVTT Parser Integration
- Create WebVTT parser for engine-generated content:
  - Parse native WebVTT from AI engines
  - Extract and validate cue structure
  - Handle malformed content gracefully
  - Convert to internal segment format

### Phase 5: Multi-Engine Native Format Support
**Priority: MEDIUM | Duration: 2-3 days**

#### 5.1 AssemblyAI Native Timing
- Research AssemblyAI's native WebVTT/SRT export capabilities
- Implement direct format retrieval if available
- Update transcription workflow to prefer native formats

#### 5.2 Deepgram Native Timing
- Research Deepgram's WebVTT support in API responses
- Implement native timestamp extraction
- Add Deepgram-specific WebVTT processing

#### 5.3 Unified Format Handling
- Create abstraction layer for native format processing:
  - `NativeFormatProcessor` base class
  - Engine-specific implementations
  - Fallback to current estimation approach
  - Format validation and conversion utilities

### Phase 6: Enhanced CLI Features
**Priority: LOW | Duration: 1 day**

#### 6.1 Prompt Control Arguments
- Implement `--full_prompt` flag:
  - Accept file path or direct text
  - Replace entire default prompt
  - Validate prompt structure for WebVTT requests

#### 6.2 Prompt Enhancement Arguments  
- Implement `--xtra_prompt` flag:
  - Append additional instructions to base prompt
  - Support both file and direct text input
  - Merge with context-aware prompting

#### 6.3 WebVTT Display Options
- Add `--add_cues` flag for cue identifier inclusion
- Implement cue naming patterns (numbered, speaker-based, timestamp-based)
- Add preview/validation mode for generated WebVTT

## Specific Implementation Steps

### Step 1: Create Core Prompt Infrastructure

1. **Create `src/vttiro/core/prompts.py`:**
   ```python
   class WebVTTPromptGenerator:
       def __init__(self, include_examples: bool = True, include_diarization: bool = True):
           self.include_examples = include_examples
           self.include_diarization = include_diarization
       
       def generate_webvtt_prompt(self, language: str = None, context: dict = None) -> str:
           # Base WebVTT generation prompt with examples
           
       def get_webvtt_example(self) -> str:
           # Return properly formatted WebVTT example
   ```

2. **WebVTT Example Template:**
   ```webvtt
   WEBVTT
   Language: auto
   
   00:00:00.000 --> 00:00:03.240
   <v Speaker1>Welcome to this presentation on font design.
   
   00:00:03.240 --> 00:00:06.180
   <v Speaker1>[enthusiastic] I'm excited to share my workflow with you.
   
   00:00:06.180 --> 00:00:08.920
   <v Speaker2>Thanks for having me! Let's dive right in.
   ```

### Step 2: Update Gemini Integration

1. **Modify `GeminiTranscriber.transcribe()` method:**
   - Change prompt to request WebVTT format
   - Add WebVTT parsing logic
   - Handle chunk merging with timestamp adjustment

2. **Update `_generate_transcription_prompt()` method:**
   - Integrate `WebVTTPromptGenerator`
   - Add WebVTT-specific instructions
   - Include format validation requirements

### Step 3: Audio Processing Updates

1. **Modify `SimpleAudioProcessor.extract_audio()`:**
   - Change output format from WAV to MP3
   - Add `--keep_audio` support with file reuse logic
   - Implement chunk size validation with splitting

2. **Update FFmpeg command:**
   ```python
   cmd = [
       'ffmpeg', '-i', str(video_path),
       '-vn', '-acodec', 'mp3', '-ar', '16000', '-ac', '1',
       '-b:a', '128k', '-y', str(output_path)
   ]
   ```

### Step 4: WebVTT Generator Enhancement

1. **Update `SimpleWebVTTGenerator.__init__()`:**
   ```python
   def __init__(self, ..., include_cue_ids: bool = False):
       self.include_cue_ids = include_cue_ids
   ```

2. **Modify `_build_webvtt_content()` method:**
   - Make cue ID generation conditional
   - Add timestamp validation before output
   - Ensure proper WebVTT structure

## Testing and Validation Criteria

### Critical Success Metrics
1. **Timing Accuracy**: All generated WebVTT files have valid timestamp ranges (end > start)
2. **Sequential Order**: Timestamps progress monotonically forward
3. **Format Compliance**: Generated WebVTT files validate against WebVTT specification
4. **Content Preservation**: Transcription quality maintains or improves current levels

### Test Cases
1. **Short audio clips** (< 1 minute): Perfect timing expected
2. **Medium audio clips** (1-10 minutes): Accurate segmentation with proper boundaries
3. **Long audio clips** (> 10 minutes): Multi-chunk processing with seamless merging
4. **Multiple speakers**: Proper diarization in WebVTT format
5. **Technical content**: Specialized terminology preserved with timing
6. **Various languages**: Multilingual support with appropriate timing

### Validation Process
1. **Automated Testing**: Unit tests for timestamp validation, format parsing
2. **Integration Testing**: End-to-end transcription with various file types
3. **Manual Validation**: Visual inspection of generated WebVTT in video players
4. **Performance Testing**: Processing time and accuracy comparisons

## Edge Cases and Error Handling

### Timestamp Edge Cases
- **Zero-duration segments**: Assign minimum 0.5-second duration
- **Overlapping segments**: Detect and resolve with gap insertion
- **Out-of-order segments**: Sort and validate timestamp sequence
- **Malformed engine responses**: Parse and repair or fallback to estimation

### Audio Processing Edge Cases  
- **Corrupted audio files**: Graceful failure with user feedback
- **Unsupported formats**: Clear error messages with format suggestions
- **Large files**: Progressive processing with memory management
- **Network interruptions**: Retry logic with partial result preservation

### Multi-Engine Compatibility
- **Format differences**: Normalize WebVTT structure across engines
- **Capability variations**: Feature detection and graceful degradation
- **API changes**: Version-aware integration with fallback options
- **Rate limiting**: Respect engine limits with queue management

## Future Considerations

### Performance Optimization
- **Parallel processing**: Multi-threaded audio chunk processing
- **Caching system**: Store intermediate results for large files
- **Progressive output**: Stream WebVTT cues as they're generated
- **Batch processing**: Optimize multiple file handling

### Advanced Features  
- **Custom timestamp adjustment**: User-defined offset and scaling
- **Style preservation**: WebVTT styling and positioning support
- **Metadata integration**: Rich metadata embedding in WebVTT files
- **Quality metrics**: Confidence scoring and accuracy reporting

### Integration Enhancements
- **Plugin system**: Extensible engine integration framework
- **Configuration profiles**: Predefined settings for common use cases
- **External tool integration**: Direct output to video editing software
- **Cloud deployment**: Scalable processing infrastructure

This comprehensive plan addresses all identified issues while establishing a robust foundation for future enhancements. The phased approach ensures critical timing issues are resolved first, followed by systematic improvements to audio processing and multi-engine support.
</document_content>
</document>

<document index="12">
<source>README.md</source>
<document_content>
# vttiro

> Simple video transcription to WebVTT subtitles using AI models

**vttiro** is a straightforward Python tool that converts audio and video files to WebVTT subtitles using powerful AI transcription models. No complex setup, no over-engineering - just clean, simple transcription that works.

[![Python 3.12+](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Code Style: Ruff](https://img.shields.io/badge/Code%20Style-Ruff-000000.svg)](https://github.com/astral-sh/ruff)

## âœ¨ What It Does

- **Transcribes local files**: Convert MP4, MP3, WAV, MOV, AVI, MKV, WebM to WebVTT subtitles
- **Multiple AI engines**: Google Gemini, AssemblyAI, Deepgram with specific model selection
- **Simple CLI**: `vttiro transcribe video.mp4` - that's it!
- **Clean output**: Properly formatted WebVTT with readable timestamps
- **Auto audio extraction**: Handles video files by extracting audio with ffmpeg

## ğŸš€ Quick Start

### 1. Install

```bash
uv pip install --system vttiro
```

### 2. Set API Key

```bash
export VTTIRO_GEMINI_API_KEY="your-gemini-api-key"
# OR
export VTTIRO_ASSEMBLYAI_API_KEY="your-assemblyai-key"  
# OR
export VTTIRO_DEEPGRAM_API_KEY="your-deepgram-key"
```

### 3. Transcribe

```bash
vttiro transcribe video.mp4
```

That's it! Your WebVTT file will be saved as `video.vtt`.

## ğŸ“– Usage Examples

### Basic Transcription
```bash
# Transcribe video to WebVTT (uses default: gemini/gemini-2.0-flash)
vttiro transcribe meeting.mp4

# Custom output file
vttiro transcribe lecture.mp4 --output subtitles.vtt

# Use different AI engine
vttiro transcribe podcast.mp3 --engine assemblyai

# Use specific model within engine
vttiro transcribe interview.mp4 --engine gemini --model gemini-2.5-pro
```

### Discovery Commands
```bash
# List available AI engines
vttiro engines

# List all available models
vttiro models

# List models for specific engine
vttiro models --engine gemini
```

### Check Supported Formats
```bash
vttiro formats
```

### Get Help
```bash
vttiro help
```

## ğŸ¯ Supported Formats

**Input formats:**
- Video: `.mp4`, `.avi`, `.mov`, `.mkv`, `.webm`
- Audio: `.mp3`, `.wav`, `.m4a`, `.aac`, `.ogg`, `.flac`

**Output format:**
- WebVTT (`.vtt`) - properly formatted with timestamps

## âš™ï¸ Configuration

vttiro uses environment variables for configuration:

| Variable | Description | Default |
|----------|-------------|---------|
| `VTTIRO_GEMINI_API_KEY` | Google Gemini API key | - |
| `VTTIRO_ASSEMBLYAI_API_KEY` | AssemblyAI API key | - |
| `VTTIRO_DEEPGRAM_API_KEY` | Deepgram API key | - |
| `VTTIRO_ENGINE` | Default AI engine to use | `gemini` |
| `VTTIRO_CHUNK_DURATION` | Audio chunk size in seconds | `600` |

## ğŸ¤– AI Engines and Models

vttiro supports multiple AI engines, each with various model options:

### **Gemini** (Google) - Default Engine
- `gemini-2.0-flash` (default) - Latest model with excellent accuracy
- `gemini-2.0-flash-exp` - Experimental version with cutting-edge features  
- `gemini-2.5-pro` - Highest quality model for critical transcription
- `gemini-2.5-flash` - Balanced speed and accuracy
- And more models available

### **AssemblyAI** - Professional-Grade Transcription  
- `universal-2` (default) - Latest universal transcription model
- `universal-1` - Previous generation universal model
- `nano` - Fast lightweight model
- `best` - Automatically selects optimal model

### **Deepgram** - Fast and Multilingual
- `nova-3` (default) - Latest Nova model with 30+ languages
- `nova-2` - Previous generation Nova model
- `enhanced` - Enhanced accuracy model
- `base` - Fast basic transcription
- `whisper-cloud` - OpenAI Whisper via Deepgram

**Usage:**
```bash
vttiro transcribe video.mp4 --engine gemini --model gemini-2.5-pro
vttiro transcribe audio.mp3 --engine deepgram --model nova-3
vttiro transcribe meeting.mp4 --engine assemblyai --model universal-2
```

## ğŸ“‹ Requirements

- **Python 3.12+**
- **ffmpeg** (for video processing)
- **API key** for at least one supported service

### Install ffmpeg:

**macOS:**
```bash
brew install ffmpeg
```

**Ubuntu/Debian:**
```bash
sudo apt install ffmpeg
```

**Windows:**
Download from [ffmpeg.org](https://ffmpeg.org/download.html)

## ğŸ”§ Installation Options

### Basic Installation (Recommended)
```bash
uv pip install --system vttiro
```
Includes all core functionality for API-based transcription.

### With Local Models
```bash
uv pip install --system vttiro[local]
```
Adds PyTorch and local inference capabilities.

### All Features
```bash
uv pip install --system vttiro[all]
```
Complete installation with all optional dependencies.

## ğŸ“ Example Output

Input: `meeting.mp4`

Output: `meeting.vtt`
```webvtt
WEBVTT

cue-0001
00:00:00.000 --> 00:00:03.500
Hello everyone, welcome to today's meeting.

cue-0002
00:00:03.500 --> 00:00:07.200
We'll be discussing the quarterly results and future plans.
```

## ğŸš¨ Troubleshooting

### "File not found" error
- Check that the input file exists and path is correct
- Use quotes around filenames with spaces: `vttiro transcribe "my video.mp4"`

### "No API key" error  
- Set at least one API key environment variable
- Verify the key is valid and has sufficient credits

### "ffmpeg not found" error
- Install ffmpeg using the instructions above
- Ensure ffmpeg is in your system PATH

### Audio extraction fails
- Check that the video file isn't corrupted
- Try with a different video format
- Ensure sufficient disk space for temporary files

## ğŸ§‘â€ğŸ’» Development

### Local Development Setup
```bash
git clone https://github.com/twardoch/vttiro.git
cd vttiro
uv venv --python 3.12
uv sync --all-extras
```

### Run Tests
```bash
uv run pytest
```

### Code Quality
```bash
uv run ruff check src/
uv run ruff format src/
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ¤ Contributing

Contributions welcome! Please read our [contributing guidelines](CONTRIBUTING.md) first.

## ğŸ“ Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/twardoch/vttiro/issues)
- **Documentation**: [Read the full docs](https://github.com/twardoch/vttiro#readme)

---

**vttiro** - Simple transcription that just works! ğŸ¬â†’ğŸ“
</document_content>
</document>

<document index="13">
<source>TLDR.md</source>
<document_content>
---
this_file: TLDR.md
---

# vttiro Complete Task List

## Part 1: Core Architecture & Project Setup

- [ ] Set up modern Python package structure with `src/` layout
- [ ] Configure `pyproject.toml` with multiple installation options (basic, local, colab, all)
- [ ] Configure dependency groups for different use cases
- [ ] Set up proper entry points for CLI usage
- [ ] Create comprehensive `__init__.py` files with proper imports
- [ ] Design modular architecture with clear separation of concerns
- [ ] Implement abstract base classes for extensibility
- [ ] Design configuration system with YAML/JSON support
- [ ] Create plugin architecture for custom models
- [ ] Integrate loguru for advanced logging capabilities
- [ ] Set up structured logging with JSON output option
- [ ] Implement log levels: DEBUG, INFO, WARNING, ERROR
- [ ] Add performance monitoring hooks
- [ ] Create progress tracking for long-running operations
- [ ] Set up metrics collection framework (Prometheus-compatible)
- [ ] Design hierarchical configuration system
- [ ] Implement configuration validation with Pydantic models
- [ ] Add configuration templates for different use cases
- [ ] Support for encrypted secrets management
- [ ] Design comprehensive exception hierarchy
- [ ] Implement retry mechanisms with exponential backoff
- [ ] Add circuit breaker patterns for external API calls
- [ ] Create graceful degradation strategies
- [ ] Set up dead letter queues for failed operations
- [ ] Implement health check endpoints
- [ ] Implement main CLI interface using `fire` library
- [ ] Add `rich` for beautiful terminal output and progress bars
- [ ] Create subcommands for different operations
- [ ] Add comprehensive help and documentation
- [ ] Implement shell completion support
- [ ] Set up development dependencies
- [ ] Create development scripts and Makefile
- [ ] Set up continuous integration workflows
- [ ] Configure development containers for consistent environments
- [ ] Set up documentation structure with MkDocs/Sphinx
- [ ] Create API documentation generation from docstrings
- [ ] Add usage examples and tutorials
- [ ] Set up documentation deployment pipeline
- [ ] Create contribution guidelines
- [ ] Set up comprehensive testing framework
- [ ] Create test data management system
- [ ] Set up test environment isolation
- [ ] Implement test coverage reporting
- [ ] Create Docker containers for different deployment scenarios
- [ ] Set up Kubernetes manifests for cloud deployment
- [ ] Prepare Google Colab notebook templates
- [ ] Create deployment documentation and runbooks
- [ ] Set up monitoring and alerting infrastructure

## Part 2: Video Processing & Audio Extraction

- [ ] Create `VideoProcessor` class with comprehensive yt-dlp integration
- [ ] Support multiple video sources (YouTube, Vimeo, Twitch, etc.)
- [ ] Implement intelligent format selection
- [ ] Add comprehensive metadata extraction
- [ ] Implement retry mechanisms with exponential backoff
- [ ] Add progress tracking and cancellation support
- [ ] Handle network interruptions and resume capabilities
- [ ] Create download queue management for batch operations
- [ ] Implement concurrent downloads with rate limiting
- [ ] Add support for proxy and authentication requirements
- [ ] Handle geo-restricted content with appropriate fallbacks
- [ ] Extract audio with optimal settings
- [ ] Implement audio quality assessment
- [ ] Add audio enhancement preprocessing
- [ ] Implement content type detection
- [ ] Extract contextual information for better transcription
- [ ] Develop energy-based segmentation algorithm
- [ ] Implement configurable chunking strategies
- [ ] Add overlap handling for chunk boundaries
- [ ] Implement streaming audio processing
- [ ] Add memory usage monitoring and limits
- [ ] Support multiple input formats
- [ ] Implement FFmpeg integration for format conversion
- [ ] Extract comprehensive metadata for transcription context
- [ ] Create metadata-driven model selection
- [ ] Comprehensive error handling for all failure modes
- [ ] Implement graceful degradation
- [ ] Create detailed error reporting and diagnostics
- [ ] Multi-threaded download and processing
- [ ] Implement caching strategies
- [ ] Add performance monitoring

## Part 3: Multi-Model Transcription Engine

- [ ] Design abstract `TranscriptionEngine` base class
- [ ] Implement plugin architecture for adding new models
- [ ] Create `TranscriptionEnsemble` for multi-model coordination
- [ ] Design result merging and confidence scoring system
- [ ] Implement fallback chains for model failures
- [ ] Add performance monitoring and model comparison metrics
- [ ] Implement `GeminiTranscriber` class with advanced features
- [ ] Add Gemini-specific optimizations with factual prompting
- [ ] Implement rate limiting and quota management
- [ ] Add cost tracking and optimization features
- [ ] Implement `AssemblyAITranscriber` for maximum accuracy
- [ ] Configure for optimal performance
- [ ] Implement `DeepgramTranscriber` optimized for speed
- [ ] Add Deepgram-specific features
- [ ] Implement `VoxtralTranscriber` for open-source alternative
- [ ] Optimize for various deployment scenarios
- [ ] Implement `WhisperXTranscriber` for improved Whisper performance
- [ ] Add `SpeechmaticsTranscriber` for specialized use cases
- [ ] Develop content-aware routing algorithm
- [ ] Implement cost-performance optimization
- [ ] Implement multiple ensemble strategies
- [ ] Add result validation and quality assurance
- [ ] Implement metadata-driven improvements with factual prompts
- [ ] Add adaptive prompting system for better name/brand recognition
- [ ] Implement subtitle improvement pipeline with better models
- [ ] Implement comprehensive metrics tracking
- [ ] Add quality assessment tools

## Part 4: Smart Audio Segmentation

- [ ] Implement sophisticated energy analysis algorithms (balance lowest energy & longest low energy)
- [ ] Develop adaptive energy thresholding
- [ ] Implement speech pattern recognition
- [ ] Add syntactic awareness
- [ ] Develop content type-specific strategies
- [ ] Implement adaptive chunk sizing (default 10-minute max chunks)
- [ ] Ensure millisecond-precision timestamp accuracy with integer-second preference
- [ ] Implement overlap handling strategies
- [ ] Assess audio quality for optimal chunking
- [ ] Adapt segmentation based on quality metrics
- [ ] Develop speaker-aware segmentation
- [ ] Integrate with preliminary diarization
- [ ] Implement efficient processing algorithms
- [ ] Memory-efficient implementations
- [ ] Implement machine learning-based approaches
- [ ] Add signal processing techniques
- [ ] Develop robust reassembly algorithms with correct timestamping
- [ ] Implement consistency checking
- [ ] Create comprehensive configuration framework
- [ ] Add performance monitoring and analytics

## Part 5: Speaker Diarization Implementation

- [ ] Implement `DiarizationEngine` with multiple algorithm support
- [ ] Design speaker embedding management system
- [ ] Optimize pyannote pipeline configuration
- [ ] Implement production-ready pipeline features
- [ ] Integrate transcription feedback for improved accuracy
- [ ] Cross-modal validation and correction
- [ ] Develop sophisticated overlap detection
- [ ] Implement overlap resolution strategies
- [ ] Build speaker identity management system
- [ ] Implement consistency validation
- [ ] Develop streaming diarization pipeline
- [ ] Implement real-time optimization features
- [ ] Implement comprehensive diarization quality metrics
- [ ] Add quality assurance mechanisms
- [ ] Coordinate diarization with transcription timing
- [ ] Implement speaker-aware transcription enhancement
- [ ] Implement state-of-the-art clustering algorithms
- [ ] Develop embedding enhancement techniques
- [ ] Optimize for various deployment scenarios
- [ ] Implement performance monitoring and optimization

## Part 6: Emotion Detection Integration

- [ ] Design `EmotionAnalyzer` with multi-modal fusion capabilities
- [ ] Implement emotion representation systems
- [ ] Integrate state-of-the-art speech emotion models
- [ ] Implement advanced audio feature extraction
- [ ] Deploy advanced NLP models for text emotion recognition
- [ ] Implement linguistic emotion indicators
- [ ] Develop streaming emotion analysis pipeline
- [ ] Implement emotion transition detection
- [ ] Integrate with speaker diarization for per-speaker emotions
- [ ] Implement speaker emotion consistency validation
- [ ] Develop culture-aware emotion recognition
- [ ] Implement multilingual emotion analysis
- [ ] Leverage content metadata for emotion context
- [ ] Implement conversational emotion analysis
- [ ] Develop emotion detection quality metrics
- [ ] Implement quality assurance mechanisms
- [ ] Implement sophisticated emotion analysis capabilities
- [ ] Add specialized emotion detection features
- [ ] Coordinate with other pipeline components
- [ ] Implement comprehensive output formats

## Part 7: WebVTT Generation with Enhancements

- [ ] Implement `WebVTTGenerator` with broadcast-quality standards
- [ ] Design flexible cue creation system
- [ ] Implement intelligent text formatting
- [ ] Add typographic enhancements
- [ ] Integrate speaker diarization results for attribution
- [ ] Implement speaker label formatting
- [ ] Integrate emotion detection results for enhanced representation
- [ ] Add advanced emotion visualization
- [ ] Implement comprehensive language support
- [ ] Add translation and localization features
- [ ] Ensure full accessibility standard compliance
- [ ] Implement advanced accessibility features
- [ ] Generate platform-specific subtitle formats
- [ ] Optimize for major platforms
- [ ] Implement comprehensive quality checking
- [ ] Add automated testing and validation
- [ ] Implement rich styling capabilities
- [ ] Add interactive and enhanced features
- [ ] Optimize subtitle generation performance
- [ ] Implement scalable architecture

## Part 8: YouTube Integration & Upload

- [ ] Implement robust OAuth 2.0 authentication system
- [ ] Add comprehensive API client management
- [ ] Extract comprehensive video metadata using YouTube Data API
- [ ] Implement metadata-driven transcription enhancement
- [ ] Develop intelligent subtitle upload system
- [ ] Implement batch upload optimization
- [ ] Build sophisticated quota management system
- [ ] Add intelligent scheduling features
- [ ] Create end-to-end YouTube video processing pipeline
- [ ] Implement workflow automation
- [ ] Develop channel-wide management capabilities
- [ ] Add advanced analytics features
- [ ] Implement automatic content enhancement
- [ ] Add advanced content analysis
- [ ] Build comprehensive multi-language capabilities
- [ ] Implement localization features
- [ ] Create robust error handling system
- [ ] Add monitoring and alerting
- [ ] Coordinate with entire transcription pipeline
- [ ] Implement deployment and scaling features

## Part 9: Multi-Environment Deployment & Testing

- [ ] Create comprehensive local setup with hardware optimization
- [ ] Implement local model management
- [ ] Develop Colab-specific optimizations and UI
- [ ] Create Colab installation packages
- [ ] Implement Kubernetes-based production deployment
- [ ] Add cloud provider-specific optimizations
- [ ] Implement multi-layered testing approach
- [ ] Add specialized testing scenarios
- [ ] Deploy comprehensive observability stack
- [ ] Implement application performance monitoring
- [ ] Build robust continuous integration/deployment
- [ ] Add quality gates and automation
- [ ] Implement intelligent scaling strategies
- [ ] Add resource optimization features
- [ ] Implement comprehensive security measures
- [ ] Add security monitoring and audit
- [ ] Develop robust disaster recovery capabilities
- [ ] Implement business continuity planning
- [ ] Create comprehensive documentation ecosystem
- [ ] Establish maintenance and support procedures

## Success Criteria Summary

- [ ] 30-40% accuracy improvement over Whisper Large-v3
- [ ] Sub-10% Diarization Error Rate (DER)
- [ ] 79%+ emotion detection accuracy
- [ ] Process 10+ minutes audio per minute computation
- [ ] Handle videos up to 10 hours duration
- [ ] 99.9% uptime in production deployments
- [ ] Support 20+ languages with broadcast quality
- [ ] Memory efficient processing (<4GB per task)
- [ ] Full accessibility compliance (WCAG 2.1 AA)
- [ ] Comprehensive multi-environment deployment support
</document_content>
</document>

<document index="14">
<source>TODO.md</source>
<document_content>
---
this_file: TODO.md
---

# vttiro Issue 105 - WebVTT Timing and Format Fixes

## Phase 1: Core Prompt Infrastructure
- [ ] Create `src/vttiro/core/prompts.py` with WebVTTPromptGenerator class
- [ ] Implement base WebVTT prompt templates with proper formatting examples
- [ ] Add speaker diarization examples using WebVTT `<v Speaker>` syntax  
- [ ] Include emotion/nonverbal indicators in square brackets examples
- [ ] Add language-specific prompt variations support
- [ ] Create WebVTT example integration with properly formatted samples
- [ ] Add CLI argument `--full_prompt` to replace default prompt entirely
- [ ] Add CLI argument `--xtra_prompt` to append to default/custom prompt
- [ ] Add CLI argument `--add_cues` to include cue identifiers (default: False)

## Phase 2: Gemini Integration Overhaul
- [ ] Update Gemini API to new `google.genai` format with `types.Part.from_bytes()`
- [ ] Migrate from `google.generativeai` to `google.genai` imports
- [ ] Modify Gemini transcriber to request WebVTT format directly in prompts
- [ ] Include timing precision requirements in WebVTT generation prompt
- [ ] Add chunk context handling for multi-segment processing
- [ ] Implement WebVTT validation and error handling for engine responses
- [ ] Implement proper WebVTT merging for multi-chunk audio processing
- [ ] Handle timestamp continuity across audio chunks
- [ ] Add overlap detection and resolution between segments
- [ ] Preserve speaker continuity across segments in chunked processing

## Phase 3: Audio Processing Improvements  
- [ ] Switch audio extraction from WAV to MP3 format in `SimpleAudioProcessor`
- [ ] Update FFmpeg commands for MP3 output with quality settings
- [ ] Maintain audio quality while reducing file sizes
- [ ] Add audio format detection and conversion logic
- [ ] Implement `--keep_audio` CLI functionality
- [ ] Save extracted audio next to original video with same basename
- [ ] Check for existing audio files before extraction and reuse them
- [ ] Add audio file reuse logic for repeated processing
- [ ] Add 20MB limit validation for audio chunks
- [ ] Implement automatic chunk splitting for oversized files
- [ ] Add silence detection for natural audio boundary splitting
- [ ] Implement fallback time-based splitting at ~50% duration mark
- [ ] Update chunk metadata tracking system

## Phase 4: WebVTT Generation Fixes
- [ ] Modify `SimpleWebVTTGenerator` to support optional cue IDs
- [ ] Add `include_cue_ids: bool = False` parameter to WebVTT generator
- [ ] Update `_build_webvtt_content()` method for conditional cue ID generation
- [ ] Preserve existing WebVTT behavior with opt-in flag for cue identifiers
- [ ] Add comprehensive timestamp validation (ensure end_time > start_time)
- [ ] Detect and fix non-sequential timestamps in generated WebVTT
- [ ] Add minimum gap enforcement between consecutive cues
- [ ] Log and report timing irregularities for debugging
- [ ] Create WebVTT parser for engine-generated content
- [ ] Parse native WebVTT responses from AI engines
- [ ] Extract and validate cue structure from parsed content
- [ ] Handle malformed WebVTT content gracefully with fallbacks
- [ ] Convert parsed WebVTT to internal segment format

## Phase 5: Multi-Engine Native Format Support
- [ ] Research AssemblyAI's native WebVTT/SRT export capabilities
- [ ] Implement direct format retrieval from AssemblyAI if available
- [ ] Research Deepgram's WebVTT support in API responses
- [ ] Implement native timestamp extraction from Deepgram
- [ ] Add Deepgram-specific WebVTT processing logic
- [ ] Update transcription workflow to prefer native formats over estimation
- [ ] Create `NativeFormatProcessor` base class abstraction
- [ ] Implement engine-specific native format processors
- [ ] Maintain fallback to current estimation approach when native unavailable
- [ ] Add format validation and conversion utilities

## Phase 6: Enhanced CLI Features
- [ ] Implement `--full_prompt` flag accepting file path or direct text
- [ ] Replace entire default prompt with custom prompt input
- [ ] Validate prompt structure for WebVTT generation requests
- [ ] Implement `--xtra_prompt` flag for appending instructions
- [ ] Support both file and direct text input for extra prompts
- [ ] Merge extra prompts with context-aware prompting system
- [ ] Add `--add_cues` flag for cue identifier inclusion control
- [ ] Implement cue naming patterns (numbered, speaker-based, timestamp-based)
- [ ] Add preview/validation mode for generated WebVTT files

## Testing and Validation
- [ ] Create unit tests for timestamp validation logic
- [ ] Add integration tests for WebVTT format parsing
- [ ] Test short audio clips (< 1 minute) for perfect timing
- [ ] Test medium audio clips (1-10 minutes) for proper segmentation
- [ ] Test long audio clips (> 10 minutes) for multi-chunk processing
- [ ] Test multiple speakers for proper diarization in WebVTT
- [ ] Test technical content for terminology preservation with timing
- [ ] Test various languages for multilingual support
- [ ] Add automated testing for timestamp validation
- [ ] Add end-to-end transcription tests with various file types
- [ ] Perform manual validation of generated WebVTT in video players
- [ ] Conduct performance testing comparing processing time and accuracy

## Bug Fixes and Edge Cases
- [ ] Fix zero-duration segments by assigning minimum 0.5-second duration
- [ ] Detect and resolve overlapping segments with gap insertion
- [ ] Sort and validate timestamp sequence for out-of-order segments
- [ ] Parse and repair malformed engine responses or fallback to estimation
- [ ] Handle corrupted audio files with graceful failure and user feedback
- [ ] Provide clear error messages for unsupported formats with suggestions
- [ ] Implement progressive processing with memory management for large files
- [ ] Add retry logic with partial result preservation for network interruptions
- [ ] Normalize WebVTT structure across different engines
- [ ] Implement feature detection and graceful degradation
- [ ] Add version-aware integration with fallback options for API changes
- [ ] Implement rate limiting respect with queue management
</document_content>
</document>

<document index="15">
<source>WORK.md</source>
<document_content>
---
this_file: WORK.md
---

# vttiro Current Work Session - AI Engine/Model Architecture Improvement

## ğŸ¯ Current Iteration Focus: Implementing Engine/Model Separation

### âœ… COMPLETED: Engine/Model Architecture Implementation

Successfully implemented a clean separation between AI engines and specific models within each engine. This addresses the confusing terminology where "models" were actually referring to AI providers.

#### ğŸ“‹ Implementation Summary

**Problem Solved:**
- The CLI was incorrectly using `--model` to select AI engines (gemini, assemblyai, deepgram)
- Users couldn't select specific models within each engine (e.g., gemini-2.0-flash vs gemini-2.5-pro)
- Terminology was confusing and unprofessional

**Solution Implemented:**
- Separated "engines" (AI providers) from "models" (specific variants within each engine)
- Updated CLI to use `--engine` for provider selection and `--model` for specific model selection
- Added discovery commands to list available engines and models
- Implemented proper validation and default handling

#### âœ… COMPLETED TASKS

**Phase 1: Core Architecture Changes** âœ…
- [x] Created `src/vttiro/models/base.py` with engine and model enums
- [x] Added `TranscriptionEngine` enum (gemini, assemblyai, deepgram)
- [x] Added engine-specific model enums (`GeminiModel`, `AssemblyAIModel`, `DeepgramModel`)
- [x] Implemented utility functions for default models and validation
- [x] Updated `models/__init__.py` to export new enums and utilities

**Phase 2: Model Implementation Updates** âœ…
- [x] Updated `GeminiTranscriber` to accept specific model parameter
- [x] Modified constructor to use `GeminiModel` enum
- [x] Updated model name reporting to reflect actual model used
- [x] Fixed metadata in transcription results

**Phase 3: CLI Interface Overhaul** âœ…
- [x] Updated `transcribe` command to use `--engine` and `--model` parameters
- [x] Added validation for engine/model combinations
- [x] Implemented `engines` command to list available AI engines
- [x] Implemented `models` command to list all models or filter by engine
- [x] Updated help text and documentation
- [x] Added clear error messages for invalid combinations

**Phase 4: Core Integration** âœ…
- [x] Updated `FileTranscriber` to support engine/model selection
- [x] Added `_create_transcriber` method for dynamic transcriber creation
- [x] Implemented proper validation and error handling
- [x] Updated transcription workflow to use new architecture

#### ğŸ§ª TESTING RESULTS

**CLI Commands Tested:**
```bash
vttiro engines                           # Lists: gemini, assemblyai, deepgram
vttiro models                            # Lists all models by engine
vttiro models --engine gemini            # Lists: gemini-2.0-flash, gemini-2.5-pro, etc.
```

**New Usage Examples:**
```bash
vttiro transcribe video.mp4                                    # Uses gemini/gemini-2.0-flash (default)
vttiro transcribe video.mp4 --engine assemblyai                # Uses assemblyai/universal-2 (default)
vttiro transcribe video.mp4 --engine gemini --model gemini-2.5-pro  # Specific model selection
```

#### ğŸ‰ SUCCESS CRITERIA ACHIEVED

1. **Clear Separation**: âœ… Engine selection (gemini/assemblyai/deepgram) separate from model selection
2. **Flexible CLI**: âœ… Users can specify both engine and specific model
3. **Sensible Defaults**: âœ… Works without specifying model (uses engine defaults)
4. **Discoverability**: âœ… Easy to list available engines and models
5. **Professional Terminology**: âœ… Clear distinction between engines and models
6. **Validation**: âœ… Proper validation of engine/model combinations

#### ğŸ’¡ **Key Improvements Made**

**Before (Confusing):**
```bash
vttiro transcribe video.mp4 --model gemini      # Wrong - gemini is an engine, not a model
vttiro transcribe video.mp4 --model assemblyai  # Wrong - assemblyai is an engine, not a model
```

**After (Clear):**
```bash
vttiro transcribe video.mp4 --engine gemini --model gemini-2.0-flash
vttiro transcribe video.mp4 --engine assemblyai --model universal-2
vttiro transcribe video.mp4 --engine deepgram --model nova-3
```

**Discovery Commands:**
```bash
vttiro engines                     # See available AI engines
vttiro models                      # See all models across engines  
vttiro models --engine gemini      # See models for specific engine
```

#### ğŸ—ï¸ **Architecture Overview**

**Engine/Model Mapping:**
- **Gemini**: gemini-2.0-flash (default), gemini-2.0-flash-exp, gemini-2.5-pro, etc.
- **AssemblyAI**: universal-2 (default), universal-1, nano, best
- **Deepgram**: nova-3 (default), nova-2, enhanced, base, whisper-cloud

**Key Files Modified:**
- `src/vttiro/models/base.py` - New enums and utilities
- `src/vttiro/models/__init__.py` - Export new functionality
- `src/vttiro/models/gemini.py` - Accept model parameter
- `src/vttiro/cli.py` - Updated CLI interface
- `src/vttiro/core/file_transcriber.py` - Engine/model selection logic

### ğŸ› ï¸ **Next Phase: Expand Model Support**

**Remaining Tasks for Complete Implementation:**
1. Update `AssemblyAITranscriber` to accept model parameter
2. Update `DeepgramTranscriber` to accept model parameter  
3. Add backward compatibility warnings for deprecated `--model` usage
4. Create model registry system with capability information
5. Add comprehensive testing for all engine/model combinations

### ğŸ“ˆ **Impact Assessment**

**User Experience:**
- âœ… Much clearer and more professional CLI interface
- âœ… Users can now select specific models within each engine
- âœ… Easy discovery of available options
- âœ… Proper validation prevents user errors

**Code Quality:**
- âœ… Better separation of concerns
- âœ… More maintainable and extensible architecture
- âœ… Clear type safety with enums
- âœ… Proper validation and error handling

**Professional Presentation:**
- âœ… Terminology now matches industry standards
- âœ… CLI interface feels more polished and enterprise-ready
- âœ… Clear documentation and help text

---

## Quality & Reliability Improvements - âœ… **COMPLETED**

After completing the Engine/Model Architecture improvements, focused on 3 high-priority quality enhancements to increase project reliability and user experience.

### ğŸ› ï¸ **Priority 1: Fixed Pydantic Deprecation Warnings** âœ…
- **Updated**: `src/vttiro/core/config.py` - Replaced deprecated @validator with @field_validator
- **Result**: Eliminated Pydantic v1 deprecation warnings in test output
- **Validation**: All field validation continues to work correctly (confidence_threshold, chunk_duration)
- **Future-Proof**: Code now uses Pydantic v2 patterns for compatibility

### ğŸ¯ **Priority 2: Enhanced CLI Robustness & User Experience** âœ…
- **File Format Validation**: Added supported format checking with helpful error messages
- **Better Error Messages**: Specific, actionable error messages with tips and suggestions
- **Dependency Checking**: Graceful handling of missing AI SDK dependencies with install commands
- **Progress Indicators**: Added spinner progress indication for long-running operations
- **Input Validation**: Comprehensive validation of files, engines, and models
- **User Guidance**: Added helpful tips pointing users to discovery commands

**Enhanced Error Examples:**
```
âœ— Unsupported file format: .txt
ğŸ’¡ Tip: Use `vttiro formats` to see supported formats

âœ— Invalid model 'invalid_model' for engine 'gemini'
Available models for gemini: gemini-2.0-flash, gemini-2.5-flash, ...
ğŸ’¡ Tip: Use `vttiro models --engine gemini` to see all gemini models
```

### ğŸ”§ **Priority 3: Core System Reliability Enhancements** âœ…
- **Structured Logging**: Added correlation IDs for request tracking across operations
- **Timeout Handling**: 5-minute timeout per transcription attempt with clear error messages
- **Retry Logic**: 3-attempt retry with exponential backoff (2s, 4s, 6s) for transient failures
- **Error Classification**: Distinguish between retryable (network) and non-retryable errors
- **Performance Tracking**: Log elapsed time for all operations (success and failure)
- **Enhanced Error Context**: Detailed error messages with correlation IDs and timing

**Reliability Features Added:**
```python
[a1b2c3d4] Starting transcription: video.mp4 -> video.vtt
[a1b2c3d4] Transcription attempt 1/3
[a1b2c3d4] Transcription successful on attempt 1
[a1b2c3d4] Transcription completed successfully: video.vtt (took 45.23s)
```

### ğŸ“Š **Impact Assessment**

**User Experience Improvements:**
- âœ… Clear, actionable error messages reduce user confusion
- âœ… Progress indicators provide feedback during long operations
- âœ… Comprehensive validation prevents common user errors
- âœ… Dependency checking guides users to correct installation steps

**System Reliability Improvements:**  
- âœ… Correlation ID tracking enables debugging across distributed operations
- âœ… Timeout handling prevents hanging on problematic files
- âœ… Retry logic handles transient network failures automatically
- âœ… Structured logging provides operational visibility

**Code Quality Improvements:**
- âœ… Future-proofed Pydantic usage for v2+ compatibility
- âœ… Comprehensive error handling with appropriate exception types
- âœ… Professional error messages match enterprise software standards
- âœ… Robust validation prevents runtime errors

### ğŸ¯ **All High-Priority Tasks Completed Successfully**
- [x] Fix Pydantic deprecation warnings in core/config.py
- [x] Improve CLI robustness with better error handling and validation  
- [x] Enhance core system reliability with validation and error handling
- [x] Add unit tests for engine/model selection logic (25 comprehensive tests)
- [x] Update README.md with new engine/model architecture
- [x] Complete engine/model separation for all transcribers (Gemini, AssemblyAI, Deepgram)

---

**Session Start**: 2025-08-21  
**Major Accomplishments**: Engine/Model Architecture + Quality & Reliability Improvements - ALL COMPLETED âœ…  
**Status**: Project significantly enhanced with professional-grade error handling, user experience, and system reliability
</document_content>
</document>

<document index="16">
<source>package.toml</source>
<document_content>
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows 
</document_content>
</document>

<document index="17">
<source>plan/part1.md</source>
<document_content>
---
this_file: plan/part1.md
---

# Part 1: Core Architecture & Project Setup

## Overview

Establish the foundational architecture, project structure, and development environment for vttiro. This part focuses on creating a modern Python package with multiple installation options and a scalable architecture that can handle both local and cloud-based inference.

## Detailed Tasks

### 1.1 Project Structure & Package Configuration

- [ ] Set up modern Python package structure with `src/` layout
- [ ] Configure `pyproject.toml` with multiple installation options:
  - [ ] Basic: `uv pip install vttiro` (API-only, no local models)  
  - [ ] Local: `uv pip install vttiro[local]` (includes local inference dependencies)
  - [ ] Colab: `uv pip install vttiro[colab]` (Google Colab optimizations + UI)
  - [ ] All: `uv pip install vttiro[all]` (everything included)
- [ ] Configure dependency groups for different use cases
- [ ] Set up proper entry points for CLI usage
- [ ] Create comprehensive `__init__.py` files with proper imports

### 1.2 Core Architecture Design

- [ ] Design modular architecture with clear separation of concerns:
  - [ ] `vttiro.core` - Core transcription engine interfaces
  - [ ] `vttiro.models` - Model implementations and wrappers
  - [ ] `vttiro.processing` - Audio/video processing utilities
  - [ ] `vttiro.diarization` - Speaker separation functionality
  - [ ] `vttiro.emotion` - Emotion detection modules  
  - [ ] `vttiro.output` - WebVTT generation and formatting
  - [ ] `vttiro.integrations` - External service integrations
  - [ ] `vttiro.utils` - Common utilities and helpers
- [ ] Implement abstract base classes for extensibility
- [ ] Design configuration system with YAML/JSON support
- [ ] Create plugin architecture for custom models

### 1.3 Logging & Monitoring Infrastructure

- [ ] Integrate loguru for advanced logging capabilities
- [ ] Set up structured logging with JSON output option
- [ ] Implement log levels: DEBUG, INFO, WARNING, ERROR
- [ ] Add performance monitoring hooks
- [ ] Create progress tracking for long-running operations
- [ ] Set up metrics collection framework (Prometheus-compatible)

### 1.4 Configuration Management

- [ ] Design hierarchical configuration system:
  - [ ] Default settings in code
  - [ ] User config files (`~/.vttiro/config.yaml`)
  - [ ] Project-specific config files
  - [ ] Environment variable overrides
  - [ ] Command-line argument precedence
- [ ] Implement configuration validation with Pydantic models
- [ ] Add configuration templates for different use cases
- [ ] Support for encrypted secrets management

### 1.5 Error Handling & Resilience

- [ ] Design comprehensive exception hierarchy
- [ ] Implement retry mechanisms with exponential backoff
- [ ] Add circuit breaker patterns for external API calls
- [ ] Create graceful degradation strategies
- [ ] Set up dead letter queues for failed operations
- [ ] Implement health check endpoints

### 1.6 CLI Framework Setup

- [ ] Implement main CLI interface using `fire` library
- [ ] Add `rich` for beautiful terminal output and progress bars
- [ ] Create subcommands for different operations:
  - [ ] `vttiro transcribe <url/file>` - Main transcription command
  - [ ] `vttiro batch <config>` - Batch processing
  - [ ] `vttiro config` - Configuration management
  - [ ] `vttiro test` - System testing and diagnostics
- [ ] Add comprehensive help and documentation
- [ ] Implement shell completion support

### 1.7 Development Environment

- [ ] Set up development dependencies:
  - [ ] pytest for testing
  - [ ] black/ruff for code formatting
  - [ ] mypy for type checking
  - [ ] pre-commit hooks
- [ ] Create development scripts and Makefile
- [ ] Set up continuous integration workflows
- [ ] Configure development containers for consistent environments

### 1.8 Documentation Framework

- [ ] Set up documentation structure with MkDocs/Sphinx
- [ ] Create API documentation generation from docstrings
- [ ] Add usage examples and tutorials
- [ ] Set up documentation deployment pipeline
- [ ] Create contribution guidelines

### 1.9 Testing Infrastructure

- [ ] Set up comprehensive testing framework:
  - [ ] Unit tests for all core components
  - [ ] Integration tests for API interactions
  - [ ] End-to-end tests for complete workflows
  - [ ] Performance benchmarks and regression tests
- [ ] Create test data management system
- [ ] Set up test environment isolation
- [ ] Implement test coverage reporting

### 1.10 Deployment Preparation

- [ ] Create Docker containers for different deployment scenarios
- [ ] Set up Kubernetes manifests for cloud deployment
- [ ] Prepare Google Colab notebook templates
- [ ] Create deployment documentation and runbooks
- [ ] Set up monitoring and alerting infrastructure

## Technical Specifications

### Package Structure
```
vttiro/
â”œâ”€â”€ src/vttiro/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ diarization/
â”‚   â”œâ”€â”€ emotion/
â”‚   â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ integrations/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”œâ”€â”€ examples/
â””â”€â”€ pyproject.toml
```

### Configuration Schema
```yaml
api:
  gemini:
    api_key: ${GEMINI_API_KEY}
    model: "gemini-2.0-flash"
  assemblyai:
    api_key: ${ASSEMBLYAI_API_KEY}
  deepgram:
    api_key: ${DEEPGRAM_API_KEY}

processing:
  chunk_duration: 600  # seconds
  overlap_duration: 30  # seconds
  max_duration: 36000  # 10 hours max

output:
  format: "webvtt"
  include_emotions: true
  include_speakers: true
  max_chars_per_line: 42
```

## Dependencies

### Core Dependencies
- `python >= 3.12`
- `pydantic >= 2.0`
- `loguru >= 0.7`
- `fire >= 0.5`
- `rich >= 13.0`
- `pyyaml >= 6.0`

### Optional Dependencies (by installation type)
- **Local**: `torch`, `torchaudio`, `transformers`, `pyannote.audio`
- **Colab**: `IPython`, `ipywidgets`, additional Colab-specific packages
- **All**: Everything above plus development and testing tools

## Success Criteria

- [ ] Package installs correctly in all four configurations
- [ ] CLI interface provides intuitive user experience
- [ ] Configuration system handles all deployment scenarios
- [ ] Logging provides appropriate visibility into operations
- [ ] Testing framework achieves >90% code coverage
- [ ] Documentation covers all major use cases
- [ ] Development environment enables rapid iteration

## Timeline

**Week 1-2**: Core project setup and architecture design  
**Week 3**: Configuration and logging implementation  
**Week 4**: CLI framework and development environment  
**Ongoing**: Documentation and testing as features are added

This foundation will enable rapid development of subsequent components while ensuring maintainability, scalability, and reliability throughout the project lifecycle.
</document_content>
</document>

<document index="18">
<source>plan/part2.md</source>
<document_content>
---
this_file: plan/part2.md
---

# Part 2: Video Processing & Audio Extraction

## Overview

Implement robust video processing capabilities using yt-dlp for downloading and extracting audio from various sources, with intelligent handling of different formats, durations, and quality levels. Focus on efficiency, reliability, and preparing audio data optimally for transcription processing.

## Detailed Tasks

### 2.1 Enhanced yt-dlp Integration

- [ ] Create `VideoProcessor` class with comprehensive yt-dlp integration
- [ ] Support multiple video sources beyond YouTube:
  - [ ] YouTube (primary focus)
  - [ ] Vimeo, Twitch, Facebook, Twitter
  - [ ] Direct video file URLs
  - [ ] Local video files
- [ ] Implement intelligent format selection:
  - [ ] Prefer best audio quality available
  - [ ] Fallback strategies for restricted content
  - [ ] Adaptive bitrate selection based on content length
- [ ] Add comprehensive metadata extraction:
  - [ ] Title, description, duration, language hints
  - [ ] Existing captions/subtitles for context
  - [ ] Uploader information and timestamps
  - [ ] Thumbnail and chapter information

### 2.2 Robust Download Management

- [ ] Implement retry mechanisms with exponential backoff
- [ ] Add progress tracking and cancellation support
- [ ] Handle network interruptions and resume capabilities
- [ ] Create download queue management for batch operations
- [ ] Implement concurrent downloads with rate limiting
- [ ] Add support for proxy and authentication requirements
- [ ] Handle geo-restricted content with appropriate fallbacks

### 2.3 Audio Extraction & Preprocessing

- [ ] Extract audio with optimal settings:
  - [ ] Target format: WAV 16kHz mono for consistency
  - [ ] Support multiple input formats (MP4, WebM, etc.)
  - [ ] Preserve original quality when possible
  - [ ] Normalize audio levels to prevent clipping
- [ ] Implement audio quality assessment:
  - [ ] Signal-to-noise ratio analysis
  - [ ] Dynamic range detection
  - [ ] Background noise profiling
- [ ] Add audio enhancement preprocessing:
  - [ ] Noise reduction for poor quality audio
  - [ ] Automatic gain control
  - [ ] Echo and reverb reduction

### 2.4 Intelligent Content Analysis

- [ ] Implement content type detection:
  - [ ] Identify music vs speech content
  - [ ] Detect multiple speakers vs single speaker
  - [ ] Language detection from audio characteristics
  - [ ] Content domain classification (news, podcast, lecture, etc.)
- [ ] Extract contextual information for better transcription:
  - [ ] Speaker gender detection for diarization hints
  - [ ] Estimated speaker count
  - [ ] Audio quality metrics
  - [ ] Background music/noise levels

### 2.5 Smart Chunking Implementation

- [ ] Develop energy-based segmentation algorithm:
  - [ ] Analyze audio energy levels using RMS and spectral features
  - [ ] Identify natural break points in speech
  - [ ] Balance between low energy points and maximum chunk duration
  - [ ] Prefer segmentation at full integer seconds for timestamp accuracy
- [ ] Implement configurable chunking strategies:
  - [ ] Default: 10-minute chunks with energy-based boundaries
  - [ ] Short content: Process as single chunk if under 5 minutes
  - [ ] Long content: Hierarchical chunking for videos over 2 hours
- [ ] Add overlap handling for chunk boundaries:
  - [ ] 30-second overlap between chunks to prevent word loss
  - [ ] Smart overlap detection during reassembly
  - [ ] Cross-fade analysis for seamless merging

### 2.6 Memory-Efficient Processing

- [ ] Implement streaming audio processing:
  - [ ] Process audio in chunks to avoid memory issues
  - [ ] Support videos up to 10 hours duration
  - [ ] Efficient temporary file management
  - [ ] Automatic cleanup of intermediate files
- [ ] Add memory usage monitoring and limits:
  - [ ] Adaptive chunk sizes based on available memory
  - [ ] Garbage collection optimization for large files
  - [ ] Warning systems for memory pressure situations

### 2.7 Format Handling & Conversion

- [ ] Support multiple input formats:
  - [ ] Video files: MP4, WebM, AVI, MOV, MKV
  - [ ] Audio files: WAV, MP3, FLAC, OGG, M4A
  - [ ] Stream formats: HLS, DASH, RTMP
- [ ] Implement FFmpeg integration for format conversion:
  - [ ] Audio codec optimization for transcription
  - [ ] Sample rate conversion and normalization
  - [ ] Multi-channel to mono conversion
  - [ ] Bitrate optimization based on content type

### 2.8 Metadata Enrichment

- [ ] Extract comprehensive metadata for transcription context:
  - [ ] Video title and description for domain hints
  - [ ] Upload date and channel information
  - [ ] Existing captions in multiple languages
  - [ ] Video categories and tags
- [ ] Create metadata-driven model selection:
  - [ ] Use title/description to identify technical content
  - [ ] Language hints for multilingual model routing
  - [ ] Content type for specialized model selection
  - [ ] Quality metrics for processing parameter tuning

### 2.9 Error Handling & Recovery

- [ ] Comprehensive error handling for all failure modes:
  - [ ] Network timeouts and connection failures
  - [ ] Unsupported formats or codecs
  - [ ] Copyright restrictions and geo-blocking
  - [ ] Disk space and permission issues
- [ ] Implement graceful degradation:
  - [ ] Alternative format selection when preferred fails
  - [ ] Quality reduction for problematic content
  - [ ] Partial processing for partially downloaded content
- [ ] Create detailed error reporting and diagnostics

### 2.10 Performance Optimization

- [ ] Multi-threaded download and processing:
  - [ ] Parallel audio extraction for multiple chunks
  - [ ] Concurrent downloads for batch operations
  - [ ] Efficient CPU utilization for preprocessing
- [ ] Implement caching strategies:
  - [ ] Metadata caching for repeated URLs
  - [ ] Processed audio caching with checksums
  - [ ] Smart cache invalidation and cleanup
- [ ] Add performance monitoring:
  - [ ] Download speed tracking
  - [ ] Processing time metrics
  - [ ] Memory usage profiling

## Technical Specifications

### VideoProcessor Class Interface
```python
class VideoProcessor:
    def __init__(self, 
                 cache_dir: Path,
                 max_duration: int = 36000,  # 10 hours
                 chunk_duration: int = 600,   # 10 minutes
                 overlap_duration: int = 30): # 30 seconds
        
    async def process_url(self, url: str) -> ProcessedVideo:
        """Download and process video/audio from URL"""
        
    async def process_file(self, file_path: Path) -> ProcessedVideo:
        """Process local video/audio file"""
        
    def extract_metadata(self, source: str) -> VideoMetadata:
        """Extract comprehensive metadata"""
        
    def segment_audio(self, audio_path: Path) -> List[AudioChunk]:
        """Smart energy-based audio segmentation"""
```

### Audio Segmentation Algorithm
```python
def energy_based_segmentation(
    audio: np.ndarray, 
    sr: int,
    max_chunk_duration: int = 600,
    min_energy_window: float = 2.0,
    energy_threshold_percentile: int = 20
) -> List[Tuple[float, float]]:
    """
    Segment audio at energy minima within duration constraints
    
    Returns list of (start_time, end_time) tuples in seconds
    """
```

### Configuration Options
```yaml
video_processing:
  max_duration: 36000  # 10 hours maximum
  chunk_duration: 600  # 10 minutes default chunks
  overlap_duration: 30  # 30 seconds overlap
  quality_preference: "best"  # best, medium, fast
  audio_format: "wav"
  sample_rate: 16000
  channels: 1  # mono
  
  yt_dlp:
    retry_attempts: 3
    timeout: 300  # 5 minutes per download
    rate_limit: "1M"  # 1MB/s default rate limit
    
  preprocessing:
    normalize_audio: true
    noise_reduction: "auto"  # auto, aggressive, off
    enhance_speech: true
```

## Dependencies

### Required Libraries
- `yt-dlp >= 2025.01.01` - Video downloading
- `ffmpeg-python >= 0.2.0` - Audio processing
- `librosa >= 0.10.0` - Audio analysis
- `numpy >= 1.24.0` - Numerical processing
- `scipy >= 1.10.0` - Signal processing

### Optional Libraries
- `whisper-timestamped` - Enhanced audio segmentation
- `webrtcvad` - Voice activity detection
- `noisereduce` - Audio noise reduction
- `pyloudnorm` - Audio normalization

## Success Criteria

- [ ] Successfully download and process videos from major platforms
- [ ] Handle videos up to 10 hours duration without memory issues
- [ ] Achieve sub-second accuracy in audio segmentation timing
- [ ] Maintain 99% success rate for supported video formats
- [ ] Process audio at 5x real-time speed or better
- [ ] Generate high-quality metadata for transcription context
- [ ] Robust error handling with graceful degradation
- [ ] Memory usage stays under 2GB for any single video

## Integration Points

### With Part 3 (Multi-Model Transcription)
- Provide optimally segmented audio chunks
- Supply rich metadata for model selection
- Enable parallel processing of multiple chunks

### With Part 4 (Smart Audio Segmentation)  
- Enhance basic segmentation with advanced algorithms
- Provide energy analysis data for intelligent chunking
- Support custom segmentation strategies per content type

### With Part 8 (YouTube Integration)
- Extract YouTube-specific metadata for upload optimization
- Preserve original video information for subtitle upload
- Handle YouTube-specific format requirements

## Timeline

**Week 3-4**: Core yt-dlp integration and basic processing  
**Week 5**: Smart chunking and energy-based segmentation  
**Week 6**: Error handling, optimization, and testing  
**Week 7**: Integration with transcription pipeline  

This robust video processing foundation enables the entire transcription pipeline to operate efficiently on diverse content types while maintaining high reliability and performance standards.
</document_content>
</document>

<document index="19">
<source>plan/part3.md</source>
<document_content>
---
this_file: plan/part3.md
---

# Part 3: Multi-Model Transcription Engine

## Overview

Implement a sophisticated transcription system that leverages multiple state-of-the-art AI models to achieve superior accuracy compared to OpenAI Whisper. The system will intelligently route audio to optimal models based on content characteristics and combine results using ensemble methods for maximum reliability.

## Detailed Tasks

### 3.1 Core Transcription Architecture

- [ ] Design abstract `TranscriptionEngine` base class
- [ ] Implement plugin architecture for adding new models
- [ ] Create `TranscriptionEnsemble` for multi-model coordination
- [ ] Design result merging and confidence scoring system
- [ ] Implement fallback chains for model failures
- [ ] Add performance monitoring and model comparison metrics

### 3.2 Google Gemini 2.0 Flash Integration

- [ ] Implement `GeminiTranscriber` class with advanced features:
  - [ ] Support for Gemini 2.0 Flash's 2M token context window
  - [ ] Native audio input processing (no pre-transcription needed)
  - [ ] Leverage model's multimodal understanding capabilities
  - [ ] Implement streaming transcription for real-time applications
- [ ] Add Gemini-specific optimizations:
  - [ ] Context injection from video metadata (title, description)
  - [ ] Domain-specific prompting for technical content
  - [ ] Custom instructions for better name/brand recognition
  - [ ] Temperature and top-p tuning for consistency
- [ ] Implement rate limiting and quota management
- [ ] Add cost tracking and optimization features

### 3.3 AssemblyAI Universal-2 Integration

- [ ] Implement `AssemblyAITranscriber` for maximum accuracy:
  - [ ] Leverage Universal-2's 600M parameter RNN-T architecture
  - [ ] Enable advanced features: speaker labels, auto highlights, entity detection
  - [ ] Implement confidence-based quality assessment
  - [ ] Add custom vocabulary injection for domain-specific terms
- [ ] Configure for optimal performance:
  - [ ] Automatic language detection and switching
  - [ ] Punctuation and capitalization enhancement
  - [ ] Profanity filtering and content moderation options
  - [ ] Word-level timestamp extraction for precise alignment

### 3.4 Deepgram Nova-3 Integration

- [ ] Implement `DeepgramTranscriber` optimized for speed:
  - [ ] Leverage 2B parameter architecture for fast processing
  - [ ] Enable streaming mode for low-latency applications
  - [ ] Implement custom vocabulary and model adaptation
  - [ ] Support for 30+ languages with automatic detection
- [ ] Add Deepgram-specific features:
  - [ ] Real-time transcription with WebSocket connections
  - [ ] Keyword spotting and phrase detection
  - [ ] Advanced punctuation and formatting
  - [ ] Industry-specific model selection

### 3.5 Mistral Voxtral Integration

- [ ] Implement `VoxtralTranscriber` for open-source alternative:
  - [ ] Local inference setup with optimal hardware utilization
  - [ ] 32K token context window for long-form content
  - [ ] Semantic understanding and simultaneous summarization
  - [ ] Question-answering capabilities about transcribed content
- [ ] Optimize for various deployment scenarios:
  - [ ] GPU acceleration with TensorRT optimization
  - [ ] CPU-only fallback for resource-constrained environments
  - [ ] Model quantization (FP16, INT8) for memory efficiency
  - [ ] Batch processing optimization for throughput

### 3.6 Additional Model Integrations

- [ ] Implement `WhisperXTranscriber` for improved Whisper performance:
  - [ ] Enhanced timestamp accuracy with forced alignment
  - [ ] Speaker diarization integration
  - [ ] Multiple Whisper variant support (large-v3, turbo)
  - [ ] VAD preprocessing for better segmentation
- [ ] Add `SpeechmaticsTranscriber` for specialized use cases:
  - [ ] Industry-leading accuracy for certain domains
  - [ ] Advanced punctuation and formatting capabilities
  - [ ] Real-time and batch processing modes
  - [ ] Custom language model integration

### 3.7 Intelligent Model Routing

- [ ] Develop content-aware routing algorithm:
  - [ ] Audio quality assessment for model selection
  - [ ] Language detection and model capability matching
  - [ ] Content type classification (interview, lecture, podcast, etc.)
  - [ ] Duration-based routing for optimal processing
- [ ] Implement cost-performance optimization:
  - [ ] Dynamic routing based on budget constraints
  - [ ] Quality requirements vs processing time trade-offs
  - [ ] Historical performance data for decision making
  - [ ] A/B testing framework for model comparison

### 3.8 Ensemble Methods & Result Fusion

- [ ] Implement multiple ensemble strategies:
  - [ ] Weighted voting based on model confidence scores
  - [ ] ROVER (Recognizer Output Voting Error Reduction)
  - [ ] Consensus-based merging with conflict resolution
  - [ ] Confidence-weighted word-level combination
- [ ] Add result validation and quality assurance:
  - [ ] Cross-model consistency checking
  - [ ] Outlier detection and correction
  - [ ] Confidence score calibration across models
  - [ ] Automatic quality assessment metrics

### 3.9 Context-Aware Enhancement

- [ ] Implement metadata-driven improvements:
  - [ ] Video title/description analysis for domain hints
  - [ ] Speaker name injection for better diarization
  - [ ] Technical term dictionary creation from context
  - [ ] Brand/entity recognition enhancement
- [ ] Add adaptive prompting system:
  - [ ] Dynamic prompt generation based on content analysis
  - [ ] Few-shot learning with domain-specific examples
  - [ ] Iterative refinement of transcription quality
  - [ ] Custom instruction templates for different use cases

### 3.10 Performance Monitoring & Analytics

- [ ] Implement comprehensive metrics tracking:
  - [ ] Word Error Rate (WER) estimation and tracking
  - [ ] Processing time per model and overall pipeline
  - [ ] Cost analysis and optimization recommendations
  - [ ] Model reliability and uptime monitoring
- [ ] Add quality assessment tools:
  - [ ] Automatic transcription quality scoring
  - [ ] Human evaluation workflow integration
  - [ ] Benchmark testing against standard datasets
  - [ ] Regression testing for model updates

## Technical Specifications

### TranscriptionEngine Base Class
```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional, Dict, Any

@dataclass
class TranscriptionResult:
    text: str
    confidence: float
    word_timestamps: List[Dict[str, Any]]
    processing_time: float
    model_name: str
    language: Optional[str] = None
    metadata: Dict[str, Any] = None

class TranscriptionEngine(ABC):
    @abstractmethod
    async def transcribe(
        self, 
        audio_path: Path, 
        context: Optional[TranscriptionContext] = None
    ) -> TranscriptionResult:
        """Transcribe audio file with optional context"""
        
    @abstractmethod
    def estimate_cost(self, duration_seconds: float) -> float:
        """Estimate processing cost in USD"""
        
    @abstractmethod
    def get_supported_languages(self) -> List[str]:
        """Return list of supported language codes"""
```

### Ensemble Configuration
```yaml
transcription:
  ensemble:
    enabled: true
    strategy: "weighted_voting"  # weighted_voting, rover, consensus
    minimum_models: 2
    confidence_threshold: 0.7
    
  models:
    gemini:
      weight: 0.4
      priority: 1
      max_retries: 3
      context_enhancement: true
      
    assemblyai:
      weight: 0.3
      priority: 2
      enable_features: ["speaker_labels", "auto_highlights"]
      
    deepgram:
      weight: 0.2
      priority: 3
      streaming_mode: false
      custom_vocabulary: true
      
    voxtral:
      weight: 0.1
      priority: 4
      local_inference: true
      quantization: "fp16"

  routing:
    strategy: "quality_based"  # quality_based, cost_optimized, speed_optimized
    fallback_enabled: true
    timeout_seconds: 300
```

### Model Performance Targets
```yaml
performance_targets:
  gemini_flash:
    accuracy_improvement: "35%"  # vs Whisper Large-v3
    processing_speed: "2x real-time"
    cost_per_hour: "$1.20"
    
  assemblyai_universal2:
    accuracy_improvement: "40%"  # Best accuracy
    processing_speed: "1.5x real-time"
    cost_per_hour: "$2.40"
    
  deepgram_nova3:
    accuracy_improvement: "30%"
    processing_speed: "15x real-time"  # Fastest
    cost_per_hour: "$1.44"
    
  voxtral_local:
    accuracy_improvement: "25%"
    processing_speed: "3x real-time"
    cost_per_hour: "$0.00"  # Local inference
```

## Dependencies

### Core Dependencies
- `google-generativeai >= 0.5.0` - Gemini API integration
- `assemblyai >= 0.25.0` - AssemblyAI SDK
- `deepgram-sdk >= 3.0.0` - Deepgram API client
- `transformers >= 4.35.0` - HuggingFace models
- `torch >= 2.0.0` - PyTorch for local inference

### Advanced Dependencies
- `tensorrt >= 8.6.0` - GPU optimization (optional)
- `onnxruntime-gpu >= 1.16.0` - ONNX inference (optional)  
- `whisperx >= 3.1.0` - Enhanced Whisper (optional)
- `speechmatics-python >= 1.12.0` - Speechmatics API (optional)

## Success Criteria

- [ ] Achieve 30-40% accuracy improvement over Whisper Large-v3
- [ ] Process audio at minimum 2x real-time speed
- [ ] Maintain <5% failure rate across all supported models
- [ ] Support videos up to 10 hours with consistent quality
- [ ] Ensemble methods improve accuracy by additional 5-10%
- [ ] Cost per hour of transcribed audio under $2.00 average
- [ ] Response time under 30 seconds for 10-minute audio chunks
- [ ] Support 20+ languages with high accuracy

## Integration Points

### With Part 2 (Video Processing)
- Receive optimally segmented audio chunks
- Utilize metadata for model selection and context enhancement
- Process multiple chunks in parallel for efficiency

### With Part 4 (Smart Audio Segmentation)
- Coordinate with advanced segmentation for optimal chunk boundaries
- Provide transcription feedback for segmentation improvement
- Handle cross-chunk word alignment and merging

### With Part 5 (Speaker Diarization)
- Provide word-level timestamps for speaker alignment
- Coordinate multi-model speaker identification
- Enable speaker-aware transcription improvements

### With Part 6 (Emotion Detection)
- Supply transcription text for multimodal emotion analysis
- Coordinate timing between transcription and emotion detection
- Enable emotion-aware transcription enhancements

## Timeline

**Week 5-6**: Core architecture and base model integrations  
**Week 7-8**: Advanced model integrations and ensemble methods  
**Week 9**: Context enhancement and intelligent routing  
**Week 10**: Performance optimization and testing  
**Week 11**: Integration with other pipeline components

This comprehensive transcription engine forms the heart of vttiro's superior performance, leveraging the best available AI models through intelligent orchestration and ensemble techniques.
</document_content>
</document>

<document index="20">
<source>plan/part4.md</source>
<document_content>
---
this_file: plan/part4.md
---

# Part 4: Smart Audio Segmentation

## Overview

Develop advanced audio segmentation algorithms that intelligently split long-form audio into optimal chunks for transcription processing. The system will balance energy-based boundary detection with maximum processing efficiency while ensuring seamless reassembly with precise timestamps.

## Detailed Tasks

### 4.1 Energy-Based Segmentation Core

- [ ] Implement sophisticated energy analysis algorithms:
  - [ ] Multi-scale energy computation (RMS, spectral centroid, zero-crossing rate)
  - [ ] Voice Activity Detection (VAD) integration for speech boundaries
  - [ ] Silence detection with configurable thresholds
  - [ ] Spectral flux analysis for detecting speech transitions
- [ ] Develop adaptive energy thresholding:
  - [ ] Dynamic threshold calculation based on content characteristics
  - [ ] Percentile-based threshold selection for consistent segmentation
  - [ ] Noise floor estimation and compensation
  - [ ] Content-aware sensitivity adjustment

### 4.2 Linguistic Boundary Detection

- [ ] Implement speech pattern recognition:
  - [ ] Pause detection between sentences and phrases
  - [ ] Prosodic boundary identification using pitch and rhythm
  - [ ] Speaker turn detection for multi-speaker content
  - [ ] Breathing pattern recognition for natural breaks
- [ ] Add syntactic awareness:
  - [ ] Sentence boundary prediction using pre-trained models
  - [ ] Topic change detection through semantic analysis
  - [ ] Paragraph and section boundary identification
  - [ ] Question-answer pair boundary detection

### 4.3 Content-Aware Segmentation

- [ ] Develop content type-specific strategies:
  - [ ] Interview/conversation segmentation (speaker-aware chunking)
  - [ ] Lecture/presentation segmentation (topic-based boundaries)
  - [ ] Podcast segmentation (natural advertising/music breaks)
  - [ ] News/broadcast segmentation (story boundary detection)
- [ ] Implement adaptive chunk sizing:
  - [ ] Variable chunk lengths based on content complexity
  - [ ] Quality-dependent sizing (longer chunks for clear audio)
  - [ ] Processing power-aware adaptation
  - [ ] Memory constraint consideration

### 4.4 Temporal Alignment & Synchronization

- [ ] Ensure millisecond-precision timestamp accuracy:
  - [ ] Frame-accurate boundary detection
  - [ ] Integer second preference for clean reassembly
  - [ ] Drift compensation for long-duration audio
  - [ ] Cross-chunk timestamp validation
- [ ] Implement overlap handling strategies:
  - [ ] Configurable overlap duration (default 30 seconds)
  - [ ] Intelligent overlap content identification
  - [ ] Duplicate word detection and removal
  - [ ] Seamless boundary merging algorithms

### 4.5 Quality-Driven Segmentation

- [ ] Assess audio quality for optimal chunking:
  - [ ] Signal-to-noise ratio analysis per segment
  - [ ] Dynamic range assessment
  - [ ] Compression artifact detection
  - [ ] Background noise characterization
- [ ] Adapt segmentation based on quality metrics:
  - [ ] Shorter chunks for poor quality audio
  - [ ] Longer chunks for high-quality, clear speech
  - [ ] Quality-based processing parameter adjustment
  - [ ] Automatic enhancement recommendations

### 4.6 Multi-Speaker Optimization

- [ ] Develop speaker-aware segmentation:
  - [ ] Speaker change point detection using voice characteristics
  - [ ] Turn-taking pattern analysis
  - [ ] Overlapping speech boundary handling
  - [ ] Speaker-consistent chunk boundaries when possible
- [ ] Integrate with preliminary diarization:
  - [ ] Lightweight speaker embedding extraction
  - [ ] Clustering-based speaker identification
  - [ ] Speaker boundary-aware chunk creation
  - [ ] Cross-chunk speaker consistency validation

### 4.7 Performance Optimization

- [ ] Implement efficient processing algorithms:
  - [ ] Sliding window analysis for real-time capability
  - [ ] Vectorized operations using NumPy/SciPy
  - [ ] GPU acceleration for computationally intensive tasks
  - [ ] Multi-threaded processing for parallel chunk analysis
- [ ] Memory-efficient implementations:
  - [ ] Streaming processing for large files
  - [ ] Incremental analysis without full audio loading
  - [ ] Garbage collection optimization
  - [ ] Temporary file management for intermediate results

### 4.8 Advanced Segmentation Algorithms

- [ ] Implement machine learning-based approaches:
  - [ ] Neural network models for boundary prediction
  - [ ] Transfer learning from pre-trained speech models
  - [ ] Ensemble methods combining multiple detection strategies
  - [ ] Reinforcement learning for adaptive improvement
- [ ] Add signal processing techniques:
  - [ ] Wavelet transform analysis for multi-resolution boundaries
  - [ ] Fourier analysis for periodic pattern detection
  - [ ] Autocorrelation analysis for rhythm identification
  - [ ] Spectral clustering for homogeneous segment identification

### 4.9 Reassembly & Consistency Management

- [ ] Develop robust reassembly algorithms:
  - [ ] Timestamp continuity validation across chunks
  - [ ] Word boundary alignment between overlapping segments
  - [ ] Confidence score propagation through reassembly
  - [ ] Error detection and correction during merging
- [ ] Implement consistency checking:
  - [ ] Cross-chunk speaker identity verification
  - [ ] Emotion continuity validation
  - [ ] Topic coherence checking
  - [ ] Quality metric consistency assessment

### 4.10 Configuration & Tuning System

- [ ] Create comprehensive configuration framework:
  - [ ] Content type-specific parameter sets
  - [ ] User-customizable segmentation strategies
  - [ ] A/B testing framework for parameter optimization
  - [ ] Machine learning-based automatic tuning
- [ ] Add performance monitoring and analytics:
  - [ ] Segmentation quality metrics tracking
  - [ ] Processing efficiency measurement
  - [ ] Boundary accuracy assessment
  - [ ] User satisfaction feedback integration

## Technical Specifications

### SegmentationEngine Class
```python
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict, Any
from enum import Enum

class SegmentationType(Enum):
    ENERGY_BASED = "energy_based"
    LINGUISTIC = "linguistic" 
    SPEAKER_AWARE = "speaker_aware"
    ADAPTIVE = "adaptive"

@dataclass
class SegmentationConfig:
    max_chunk_duration: int = 600  # 10 minutes
    min_chunk_duration: int = 60   # 1 minute
    overlap_duration: int = 30     # 30 seconds
    energy_threshold_percentile: int = 20
    prefer_integer_seconds: bool = True
    content_type: Optional[str] = None
    quality_threshold: float = 0.7

@dataclass
class AudioSegment:
    start_time: float
    end_time: float
    duration: float
    energy_stats: Dict[str, float]
    quality_metrics: Dict[str, float]
    content_hints: Dict[str, Any]
    chunk_id: str

class SegmentationEngine:
    def __init__(self, config: SegmentationConfig):
        self.config = config
        self.vad_model = self._initialize_vad()
        self.boundary_detector = self._initialize_boundary_detector()
    
    def segment_audio(self, 
                     audio_path: Path, 
                     metadata: Optional[AudioMetadata] = None) -> List[AudioSegment]:
        """
        Intelligently segment audio using multiple strategies
        
        Args:
            audio_path: Path to audio file
            metadata: Optional metadata for content-aware segmentation
            
        Returns:
            List of AudioSegment objects with precise boundaries
        """
        
    def analyze_energy_patterns(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """Compute multi-scale energy features"""
        
    def detect_linguistic_boundaries(self, audio: np.ndarray, sr: int) -> List[float]:
        """Find natural speech boundaries"""
        
    def validate_segments(self, segments: List[AudioSegment]) -> List[AudioSegment]:
        """Validate and optimize segment boundaries"""
```

### Energy Analysis Implementation
```python
def compute_energy_features(audio: np.ndarray, 
                          sr: int,
                          window_size: int = 2048,
                          hop_length: int = 512) -> Dict[str, np.ndarray]:
    """
    Compute comprehensive energy features for boundary detection
    
    Returns:
        Dictionary containing RMS energy, spectral centroid, 
        zero-crossing rate, spectral rolloff, and MFCC features
    """
    
    features = {
        'rms_energy': librosa.feature.rms(y=audio, frame_length=window_size, hop_length=hop_length)[0],
        'spectral_centroid': librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=hop_length)[0],
        'zero_crossing_rate': librosa.feature.zero_crossing_rate(audio, frame_length=window_size, hop_length=hop_length)[0],
        'spectral_rolloff': librosa.feature.spectral_rolloff(y=audio, sr=sr, hop_length=hop_length)[0],
        'mfcc': librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, hop_length=hop_length)
    }
    
    return features
```

### Segmentation Configuration Templates
```yaml
segmentation_strategies:
  podcast:
    max_chunk_duration: 900  # 15 minutes for podcast content
    energy_threshold_percentile: 15  # More sensitive to pauses
    prefer_speaker_boundaries: true
    overlap_duration: 45
    
  lecture:
    max_chunk_duration: 1200  # 20 minutes for educational content
    energy_threshold_percentile: 25
    prefer_topic_boundaries: true
    overlap_duration: 30
    
  interview:
    max_chunk_duration: 600   # 10 minutes for conversational content
    energy_threshold_percentile: 20
    prefer_speaker_turns: true
    overlap_duration: 30
    
  news:
    max_chunk_duration: 300   # 5 minutes for news content
    energy_threshold_percentile: 30
    prefer_story_boundaries: true
    overlap_duration: 15

  music_with_speech:
    max_chunk_duration: 600
    energy_threshold_percentile: 40  # Higher threshold for music content
    music_detection_enabled: true
    overlap_duration: 45
```

## Dependencies

### Core Libraries
- `librosa >= 0.10.0` - Audio analysis and feature extraction
- `scipy >= 1.10.0` - Signal processing algorithms  
- `numpy >= 1.24.0` - Numerical computations
- `scikit-learn >= 1.3.0` - Machine learning for boundary detection

### Advanced Libraries
- `webrtcvad >= 2.0.10` - Voice Activity Detection
- `pyannote.audio >= 3.1.0` - Advanced diarization for speaker boundaries
- `speechbrain >= 0.5.15` - Pre-trained models for boundary detection
- `transformers >= 4.35.0` - Transformer models for linguistic analysis

### Optional Dependencies
- `torch >= 2.0.0` - Neural network-based boundary detection
- `onnxruntime >= 1.16.0` - Optimized inference for boundary models
- `numba >= 0.58.0` - JIT compilation for performance optimization

## Success Criteria

- [ ] Achieve sub-second accuracy in boundary detection
- [ ] Process audio segmentation at 20x real-time speed
- [ ] Maintain <2% overlap redundancy after reassembly
- [ ] Support content types: podcast, lecture, interview, news, music+speech
- [ ] Handle audio files up to 10 hours duration efficiently
- [ ] Achieve 95% boundary quality score on validation datasets
- [ ] Memory usage stays under 1GB for any single segmentation task
- [ ] Integration seamless with transcription pipeline timing

## Integration Points

### With Part 2 (Video Processing)
- Receive basic energy-based chunks from initial processing
- Enhance with advanced boundary detection algorithms
- Coordinate with metadata for content-aware segmentation

### With Part 3 (Multi-Model Transcription)
- Provide optimally-sized chunks for each transcription model
- Coordinate chunk boundaries with model processing capabilities
- Enable parallel processing through intelligent chunk distribution

### With Part 5 (Speaker Diarization)
- Pre-identify potential speaker boundaries for diarization hints
- Coordinate with speaker change detection for optimal boundaries
- Enable speaker-consistent chunking strategies

### With Part 7 (WebVTT Generation)
- Ensure segment boundaries align with subtitle timing requirements
- Provide metadata for optimal subtitle break positioning
- Enable seamless timestamp continuity in final output

## Timeline

**Week 8-9**: Core energy analysis and basic boundary detection  
**Week 10**: Advanced linguistic and content-aware segmentation  
**Week 11**: Integration with VAD and preliminary diarization  
**Week 12**: Performance optimization and quality validation  
**Week 13**: Integration testing with transcription pipeline

This advanced segmentation system ensures optimal audio chunk boundaries that maximize transcription accuracy while maintaining processing efficiency and seamless reassembly capabilities.
</document_content>
</document>

<document index="21">
<source>plan/part5.md</source>
<document_content>
---
this_file: plan/part5.md
---

# Part 5: Speaker Diarization Implementation

## Overview

Implement state-of-the-art speaker diarization capabilities using pyannote.audio 3.1 and advanced ensemble methods to achieve sub-10% diarization error rates. The system will identify and separate individual speakers with precise timing, enabling speaker-attributed transcriptions and enhanced subtitle generation.

## Detailed Tasks

### 5.1 Core Diarization Architecture

- [ ] Implement `DiarizationEngine` with multiple algorithm support:
  - [ ] pyannote.audio 3.1 as primary engine (70x faster than previous versions)
  - [ ] NVIDIA NeMo's Sortformer for validation and ensemble
  - [ ] Custom embedding-based clustering algorithms
  - [ ] Hybrid approaches combining multiple methods
- [ ] Design speaker embedding management system:
  - [ ] ECAPA-TDNN embeddings with 192-dimensional representations
  - [ ] Speaker profile database for cross-session consistency
  - [ ] Embedding similarity computation and clustering
  - [ ] Real-time embedding extraction and comparison

### 5.2 Advanced Pyannote.audio Integration

- [ ] Optimize pyannote pipeline configuration:
  - [ ] Fine-tune segmentation thresholds for audio quality
  - [ ] Adaptive clustering parameters based on content analysis
  - [ ] GPU acceleration setup for maximum performance
  - [ ] Memory optimization for long-duration processing
- [ ] Implement production-ready pipeline features:
  - [ ] Automatic speaker count estimation
  - [ ] Manual speaker count override capabilities
  - [ ] Quality-based parameter adaptation
  - [ ] Robustness improvements for noisy audio

### 5.3 Multi-Modal Diarization Enhancement

- [ ] Integrate transcription feedback for improved accuracy:
  - [ ] Speaker-consistent vocabulary and speaking patterns
  - [ ] Gender detection from voice characteristics and text analysis
  - [ ] Age estimation and speaker demographic profiling
  - [ ] Language accent and dialect identification
- [ ] Cross-modal validation and correction:
  - [ ] Text-based speaker identification using linguistic patterns
  - [ ] Confidence score fusion from audio and text features
  - [ ] Inconsistency detection and resolution algorithms
  - [ ] Temporal smoothing using speaker history

### 5.4 Overlapping Speech Handling

- [ ] Develop sophisticated overlap detection:
  - [ ] Multi-channel source separation when available
  - [ ] Single-channel overlap detection using spectral analysis
  - [ ] Speaker-specific frequency range analysis
  - [ ] Temporal alignment of overlapping segments
- [ ] Implement overlap resolution strategies:
  - [ ] Primary/secondary speaker designation
  - [ ] Confidence-based speaker assignment
  - [ ] Partial overlap handling with precise timing
  - [ ] Alternative representation for simultaneous speech

### 5.5 Speaker Verification & Consistency

- [ ] Build speaker identity management system:
  - [ ] Cross-session speaker recognition using embedding similarity
  - [ ] Speaker name assignment from metadata and context
  - [ ] Speaker profile building and refinement over time
  - [ ] Voice biometric-based identity verification
- [ ] Implement consistency validation:
  - [ ] Temporal speaker transition validation
  - [ ] Voice characteristic consistency checking
  - [ ] Cross-chunk speaker alignment and correction
  - [ ] Anomaly detection for speaker identification errors

### 5.6 Real-Time Diarization Capabilities

- [ ] Develop streaming diarization pipeline:
  - [ ] Sliding window processing with 3-second buffers
  - [ ] Online clustering for incremental speaker identification
  - [ ] Low-latency speaker assignment (<2 seconds end-to-end)
  - [ ] Adaptive threshold adjustment based on streaming context
- [ ] Implement real-time optimization features:
  - [ ] Efficient memory management for continuous processing
  - [ ] Dynamic model loading and caching strategies
  - [ ] Quality monitoring and automatic parameter adjustment
  - [ ] Graceful degradation under resource constraints

### 5.7 Quality Assessment & Metrics

- [ ] Implement comprehensive diarization quality metrics:
  - [ ] Diarization Error Rate (DER) calculation and tracking
  - [ ] Speaker confusion matrix generation and analysis
  - [ ] False alarm and missed detection rate monitoring
  - [ ] Temporal accuracy assessment for speaker boundaries
- [ ] Add quality assurance mechanisms:
  - [ ] Confidence score calibration across different audio types
  - [ ] Automatic quality flag generation for manual review
  - [ ] Statistical analysis of diarization performance
  - [ ] Benchmark testing against standard evaluation datasets

### 5.8 Integration with Transcription Pipeline

- [ ] Coordinate diarization with transcription timing:
  - [ ] Word-level speaker assignment using precise timestamps
  - [ ] Cross-model alignment between diarization and transcription
  - [ ] Confidence score integration for speaker-text alignment
  - [ ] Error propagation mitigation between pipeline stages
- [ ] Implement speaker-aware transcription enhancement:
  - [ ] Speaker-specific language model adaptation
  - [ ] Gender-based acoustic model selection
  - [ ] Speaker history-informed transcription improvement
  - [ ] Contextual information injection for better recognition

### 5.9 Advanced Clustering & Embedding Techniques

- [ ] Implement state-of-the-art clustering algorithms:
  - [ ] Spectral clustering with affinity matrix optimization
  - [ ] Agglomerative clustering with distance metric learning
  - [ ] DBSCAN for density-based speaker grouping
  - [ ] Neural clustering using deep embedding spaces
- [ ] Develop embedding enhancement techniques:
  - [ ] Multi-scale embedding extraction and fusion
  - [ ] Adversarial training for robust speaker embeddings
  - [ ] Domain adaptation for different audio conditions
  - [ ] Temporal embedding aggregation for speaker profiles

### 5.10 Deployment & Optimization

- [ ] Optimize for various deployment scenarios:
  - [ ] Local GPU acceleration with CUDA optimization
  - [ ] CPU-only deployment with performance optimization
  - [ ] Cloud deployment with auto-scaling capabilities
  - [ ] Edge deployment with model quantization and pruning
- [ ] Implement performance monitoring and optimization:
  - [ ] Real-time performance metrics tracking
  - [ ] Memory usage optimization and monitoring
  - [ ] Batch processing optimization for throughput
  - [ ] A/B testing framework for algorithm improvement

## Technical Specifications

### DiarizationEngine Class
```python
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import torch

@dataclass
class SpeakerSegment:
    speaker_id: str
    start_time: float
    end_time: float
    confidence: float
    embedding: Optional[np.ndarray] = None
    metadata: Dict[str, Any] = None

@dataclass
class DiarizationResult:
    segments: List[SpeakerSegment]
    speaker_count: int
    processing_time: float
    confidence_score: float
    quality_metrics: Dict[str, float]
    
class DiarizationEngine:
    def __init__(self, 
                 hf_token: str,
                 device: str = 'cuda',
                 min_speakers: Optional[int] = None,
                 max_speakers: Optional[int] = None):
        
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )
        self.pipeline.to(torch.device(device))
        
        # Configure for optimal production performance
        self.configure_pipeline_parameters()
        
    async def diarize_audio(self, 
                           audio_path: Path,
                           metadata: Optional[AudioMetadata] = None) -> DiarizationResult:
        """
        Perform speaker diarization with adaptive parameters
        """
        
    def extract_speaker_embeddings(self, 
                                 audio: np.ndarray, 
                                 segments: List[SpeakerSegment]) -> Dict[str, np.ndarray]:
        """Extract speaker embeddings for each identified speaker"""
        
    def validate_diarization_quality(self, 
                                   result: DiarizationResult) -> Dict[str, float]:
        """Assess diarization quality and reliability"""
```

### Advanced Clustering Configuration
```python
class AdaptiveClusteringConfig:
    def __init__(self):
        self.clustering_methods = {
            'spectral': {
                'n_clusters': 'auto',
                'affinity': 'cosine',
                'assign_labels': 'kmeans'
            },
            'agglomerative': {
                'n_clusters': None,
                'distance_threshold': 0.7,
                'linkage': 'ward'
            },
            'dbscan': {
                'eps': 0.5,
                'min_samples': 5,
                'metric': 'cosine'
            }
        }
        
    def select_optimal_method(self, 
                            embeddings: np.ndarray,
                            audio_quality: float) -> str:
        """Select clustering method based on data characteristics"""
```

### Pipeline Configuration
```yaml
diarization:
  pyannote:
    model: "pyannote/speaker-diarization-3.1"
    segmentation_threshold: 0.5  # Adjust based on audio quality
    clustering_threshold: 0.7    # Speaker separation sensitivity
    min_duration_on: 0.5         # Minimum speech segment duration
    min_duration_off: 0.1        # Minimum silence between segments
    
  processing:
    chunk_overlap: 5.0           # 5-second overlap for continuity
    speaker_consistency_window: 30.0  # 30-second window for validation
    confidence_threshold: 0.6    # Minimum confidence for speaker assignment
    
  optimization:
    gpu_acceleration: true
    batch_processing: true
    memory_efficient: true
    cache_embeddings: true
    
  quality:
    target_der: 0.1              # Target 10% Diarization Error Rate
    min_confidence: 0.6          # Minimum acceptable confidence
    max_speakers: 20             # Maximum speakers to identify
    overlap_handling: "primary_secondary"  # How to handle overlapping speech
```

### Integration Points Configuration
```yaml
integration:
  transcription_alignment:
    word_level_assignment: true
    confidence_fusion_weight: 0.7  # Weight for diarization confidence
    timestamp_tolerance: 0.1     # 100ms tolerance for alignment
    
  emotion_detection:
    speaker_aware_analysis: true
    emotion_consistency_validation: true
    cross_modal_confidence_fusion: true
    
  output_generation:
    speaker_labels_in_subtitles: true
    color_coding_by_speaker: true
    speaker_name_inference: true
```

## Dependencies

### Core Dependencies
- `pyannote.audio >= 3.1.0` - Primary diarization engine
- `torch >= 2.0.0` - PyTorch for neural network models
- `torchaudio >= 2.0.0` - Audio processing for PyTorch
- `scikit-learn >= 1.3.0` - Machine learning algorithms

### Advanced Dependencies
- `nemo-toolkit >= 1.20.0` - NVIDIA NeMo for Sortformer models
- `speechbrain >= 0.5.15` - Alternative diarization models
- `resemblyzer >= 0.1.1` - Speaker embedding extraction
- `spectralcluster >= 0.2.0` - Advanced clustering algorithms

### Optional Dependencies
- `onnxruntime-gpu >= 1.16.0` - Optimized inference
- `tensorrt >= 8.6.0` - GPU optimization for NVIDIA cards
- `librosa >= 0.10.0` - Additional audio analysis features

## Success Criteria

- [ ] Achieve sub-10% Diarization Error Rate (DER) on standard benchmarks
- [ ] Process diarization at 5x real-time speed with GPU acceleration
- [ ] Support up to 20 simultaneous speakers with high accuracy
- [ ] Handle overlapping speech with <5% degradation in accuracy
- [ ] Maintain speaker consistency across 10+ hour content
- [ ] Real-time diarization with <2-second end-to-end latency
- [ ] Cross-session speaker recognition accuracy >90%
- [ ] Memory usage <4GB for any single diarization task

## Integration Points

### With Part 3 (Multi-Model Transcription)
- Provide speaker-labeled segments for transcription alignment
- Enable speaker-aware transcription model selection
- Coordinate confidence scores between diarization and transcription

### With Part 4 (Smart Audio Segmentation)
- Use speaker boundaries to inform segmentation decisions
- Coordinate chunk boundaries with speaker transitions
- Enable speaker-consistent chunk processing

### With Part 6 (Emotion Detection)
- Provide speaker identity for emotion analysis per speaker
- Enable speaker-specific emotion profiling and analysis
- Coordinate timing between speaker and emotion detection

### With Part 7 (WebVTT Generation)
- Supply speaker labels for subtitle attribution
- Enable speaker-based subtitle formatting and styling
- Provide confidence scores for quality-based formatting

## Timeline

**Week 9-10**: Core pyannote.audio integration and basic diarization  
**Week 11**: Advanced clustering and overlapping speech handling  
**Week 12**: Real-time capabilities and quality assessment  
**Week 13**: Integration with transcription pipeline  
**Week 14**: Performance optimization and validation testing

This comprehensive diarization system provides the speaker identification foundation essential for high-quality, speaker-attributed transcriptions and enhanced subtitle generation.
</document_content>
</document>

<document index="22">
<source>plan/part6.md</source>
<document_content>
---
this_file: plan/part6.md
---

# Part 6: Emotion Detection Integration

## Overview

Implement advanced emotion detection capabilities that analyze both audio characteristics and transcribed text to identify speaker emotions with 79%+ accuracy. The system will provide temporal emotion tracking, cultural adaptation, and integration with speaker diarization for comprehensive affective analysis.

## Detailed Tasks

### 6.1 Multi-Modal Emotion Detection Architecture

- [ ] Design `EmotionAnalyzer` with multi-modal fusion capabilities:
  - [ ] Audio-based emotion recognition using transformer models
  - [ ] Text-based sentiment analysis with contextual understanding
  - [ ] Adaptive weight fusion based on confidence scores
  - [ ] Cross-modal validation and inconsistency resolution
- [ ] Implement emotion representation systems:
  - [ ] Categorical emotions (happy, sad, angry, neutral, surprised, fear, disgust)
  - [ ] Dimensional emotions (valence, arousal, dominance)
  - [ ] Continuous emotion intensity scoring (0-1 scale)
  - [ ] Cultural context-aware emotion mapping

### 6.2 Audio-Based Emotion Recognition

- [ ] Integrate state-of-the-art speech emotion models:
  - [ ] SpeechBrain's wav2vec2-based emotion recognition
  - [ ] emotion2vec universal speech emotion representation
  - [ ] Custom fine-tuned models for domain-specific emotions
  - [ ] Ensemble methods combining multiple audio emotion models
- [ ] Implement advanced audio feature extraction:
  - [ ] Prosodic features (pitch, rhythm, stress patterns)
  - [ ] Spectral features (MFCC, spectral centroid, rolloff)
  - [ ] Voice quality features (jitter, shimmer, harmonics-to-noise ratio)
  - [ ] OpenSMILE 6,000+ acoustic features for robustness

### 6.3 Text-Based Emotion Analysis

- [ ] Deploy advanced NLP models for text emotion recognition:
  - [ ] DistilRoBERTa fine-tuned for emotion classification
  - [ ] BERT-based models with domain adaptation
  - [ ] Multi-language emotion models for global content
  - [ ] Context-aware emotion analysis using conversation history
- [ ] Implement linguistic emotion indicators:
  - [ ] Emotion lexicon-based analysis with intensity scoring
  - [ ] Syntactic pattern recognition for emotional expressions
  - [ ] Semantic similarity to emotional prototypes
  - [ ] Negation handling and context modification

### 6.4 Real-Time Emotion Tracking

- [ ] Develop streaming emotion analysis pipeline:
  - [ ] Sliding window processing with 3-second analysis frames
  - [ ] Temporal smoothing to reduce emotion flickering
  - [ ] Real-time confidence scoring and quality assessment
  - [ ] Memory-efficient processing for continuous streams
- [ ] Implement emotion transition detection:
  - [ ] Sudden emotion change identification and validation
  - [ ] Gradual emotion shift tracking over time
  - [ ] Emotion peak and valley detection
  - [ ] Emotional arc analysis for content segments

### 6.5 Speaker-Aware Emotion Analysis

- [ ] Integrate with speaker diarization for per-speaker emotions:
  - [ ] Individual speaker emotion profiling and baselines
  - [ ] Speaker-specific emotion model adaptation
  - [ ] Cross-speaker emotional interaction analysis
  - [ ] Group emotion dynamics and influence patterns
- [ ] Implement speaker emotion consistency validation:
  - [ ] Temporal emotion consistency for each speaker
  - [ ] Personality-based emotion pattern recognition
  - [ ] Anomaly detection for out-of-character emotions
  - [ ] Speaker emotion history and trend analysis

### 6.6 Cultural & Linguistic Adaptation

- [ ] Develop culture-aware emotion recognition:
  - [ ] Cultural emotion expression pattern databases
  - [ ] Language-specific emotion model selection
  - [ ] Regional accent and dialect emotion adaptation
  - [ ] Cross-cultural emotion mapping and normalization
- [ ] Implement multilingual emotion analysis:
  - [ ] Language detection for appropriate model selection
  - [ ] Translation-invariant emotion feature extraction
  - [ ] Code-switching emotion analysis for multilingual speakers
  - [ ] Cultural context preservation in emotion interpretation

### 6.7 Contextual Emotion Enhancement

- [ ] Leverage content metadata for emotion context:
  - [ ] Video title/description sentiment for emotion priming
  - [ ] Content domain-specific emotion expectations
  - [ ] Temporal context from video timestamps and events
  - [ ] Social context from platform and audience information
- [ ] Implement conversational emotion analysis:
  - [ ] Turn-taking emotion influence detection
  - [ ] Emotional contagion modeling between speakers
  - [ ] Conversation topic impact on emotional states
  - [ ] Question-answer emotional pattern recognition

### 6.8 Quality Assessment & Confidence Scoring

- [ ] Develop emotion detection quality metrics:
  - [ ] Confidence score calibration across modalities
  - [ ] Cross-modal agreement measurement
  - [ ] Temporal consistency scoring
  - [ ] Robustness assessment under various audio conditions
- [ ] Implement quality assurance mechanisms:
  - [ ] Automatic quality flag generation for low-confidence emotions
  - [ ] Human validation workflow integration
  - [ ] A/B testing framework for emotion model comparison
  - [ ] Benchmark evaluation against standard emotion datasets

### 6.9 Advanced Emotion Features

- [ ] Implement sophisticated emotion analysis capabilities:
  - [ ] Micro-expression detection from voice characteristics
  - [ ] Emotional intensity progression over conversation segments
  - [ ] Stress and cognitive load estimation from speech patterns
  - [ ] Authenticity assessment for genuine vs performed emotions
- [ ] Add specialized emotion detection features:
  - [ ] Sarcasm and irony detection combining audio and text
  - [ ] Emotional emphasis and focus identification
  - [ ] Mood disorder indicators for health applications
  - [ ] Engagement and attention level estimation

### 6.10 Integration & Output Enhancement

- [ ] Coordinate with other pipeline components:
  - [ ] Emotion-informed transcription accuracy improvement
  - [ ] Speaker diarization enhancement using emotional continuity
  - [ ] Subtitle generation with emotional context indicators
  - [ ] Content analysis and highlight generation based on emotions
- [ ] Implement comprehensive output formats:
  - [ ] Timestamped emotion labels with confidence scores
  - [ ] Emotion intensity curves and visualization data
  - [ ] Speaker-specific emotion summaries and profiles
  - [ ] Aggregated emotional content analysis reports

## Technical Specifications

### EmotionAnalyzer Class
```python
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import numpy as np

class EmotionCategory(Enum):
    HAPPINESS = "happiness"
    SADNESS = "sadness"
    ANGER = "anger"
    FEAR = "fear"
    SURPRISE = "surprise"
    DISGUST = "disgust"
    NEUTRAL = "neutral"

@dataclass
class EmotionResult:
    timestamp: float
    speaker_id: Optional[str]
    categorical_emotion: EmotionCategory
    intensity: float  # 0-1 scale
    valence: float    # -1 to 1 (negative to positive)
    arousal: float    # 0-1 (calm to excited)
    dominance: float  # 0-1 (submissive to dominant)
    confidence: float # 0-1 confidence score
    modality_scores: Dict[str, float]  # audio, text, fusion
    
@dataclass
class EmotionAnalysisResult:
    emotions: List[EmotionResult]
    processing_time: float
    quality_metrics: Dict[str, float]
    speaker_profiles: Dict[str, Dict[str, Any]]
    
class EmotionAnalyzer:
    def __init__(self, 
                 audio_model_path: str = "speechbrain/emotion-recognition-wav2vec2-IEMOCAP",
                 text_model_path: str = "j-hartmann/emotion-english-distilroberta-base",
                 fusion_strategy: str = "adaptive_weighted"):
        
        self.audio_classifier = self._load_audio_model(audio_model_path)
        self.text_classifier = self._load_text_model(text_model_path)
        self.fusion_strategy = fusion_strategy
        
    async def analyze_emotions(self,
                             audio_segments: List[AudioSegment],
                             transcript_segments: List[TranscriptSegment],
                             speaker_info: Optional[Dict] = None) -> EmotionAnalysisResult:
        """
        Analyze emotions using multi-modal approach
        """
        
    def extract_audio_features(self, audio: np.ndarray, sr: int) -> Dict[str, np.ndarray]:
        """Extract comprehensive audio features for emotion recognition"""
        
    def analyze_text_emotions(self, text: str, context: Optional[str] = None) -> Dict[str, float]:
        """Analyze emotions from transcribed text with context"""
        
    def fuse_modal_results(self, 
                          audio_emotions: Dict[str, float],
                          text_emotions: Dict[str, float],
                          confidence_scores: Dict[str, float]) -> EmotionResult:
        """Intelligently fuse audio and text emotion predictions"""
```

### Multi-Modal Fusion Algorithm
```python
class AdaptiveEmotionFusion:
    def __init__(self):
        self.base_weights = {
            'audio': 0.6,
            'text': 0.4
        }
        self.confidence_threshold = 0.7
        
    def compute_adaptive_weights(self, 
                               audio_confidence: float,
                               text_confidence: float,
                               audio_quality: float) -> Dict[str, float]:
        """
        Dynamically adjust fusion weights based on confidence and quality
        """
        # Increase audio weight for high-quality, confident predictions
        if audio_confidence > 0.8 and audio_quality > 0.7:
            return {'audio': 0.75, 'text': 0.25}
        # Increase text weight for low-quality audio but confident text
        elif audio_quality < 0.5 and text_confidence > 0.8:
            return {'audio': 0.25, 'text': 0.75}
        # Use balanced weights for uncertain cases
        else:
            return self.base_weights
            
    def detect_modal_conflicts(self, 
                              audio_emotions: Dict[str, float],
                              text_emotions: Dict[str, float]) -> bool:
        """Detect significant disagreement between modalities"""
        
    def resolve_conflicts(self, conflicting_results: List[Dict]) -> EmotionResult:
        """Resolve conflicts using contextual information and history"""
```

### Configuration Schema
```yaml
emotion_detection:
  models:
    audio:
      primary: "speechbrain/emotion-recognition-wav2vec2-IEMOCAP"
      fallback: "facebook/wav2vec2-large-960h"
      custom_model_path: null
      
    text:
      primary: "j-hartmann/emotion-english-distilroberta-base"
      multilingual: "cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual"
      
  processing:
    window_size: 3.0          # 3-second analysis windows
    hop_length: 1.0           # 1-second hop between windows
    min_confidence: 0.5       # Minimum confidence threshold
    temporal_smoothing: true  # Enable temporal consistency
    
  fusion:
    strategy: "adaptive_weighted"  # fixed_weighted, confidence_based, adaptive_weighted
    audio_weight: 0.6
    text_weight: 0.4
    conflict_resolution: "contextual"  # majority_vote, confidence_based, contextual
    
  features:
    categorical_emotions: true
    dimensional_emotions: true  # valence, arousal, dominance
    intensity_scoring: true
    cultural_adaptation: true
    
  output:
    include_confidence_scores: true
    include_modality_breakdown: true
    generate_emotion_summaries: true
    create_speaker_profiles: true
```

## Dependencies

### Core Dependencies
- `speechbrain >= 0.5.15` - Audio-based emotion recognition
- `transformers >= 4.35.0` - Text-based emotion models
- `torch >= 2.0.0` - PyTorch for neural network inference
- `numpy >= 1.24.0` - Numerical processing
- `scipy >= 1.10.0` - Signal processing

### Advanced Dependencies
- `librosa >= 0.10.0` - Audio feature extraction
- `opensmile >= 2.4.0` - Comprehensive acoustic feature extraction
- `vaderSentiment >= 3.3.2` - Rule-based sentiment analysis
- `textblob >= 0.17.0` - Additional text processing

### Optional Dependencies
- `emotion2vec` - Advanced emotion representation models
- `fairseq >= 0.12.0` - Facebook's sequence modeling toolkit
- `onnxruntime >= 1.16.0` - Optimized inference
- `numba >= 0.58.0` - JIT compilation for performance

## Success Criteria

- [ ] Achieve 79%+ weighted accuracy on IEMOCAP benchmark
- [ ] Process emotion analysis at 10x real-time speed
- [ ] Support 7 categorical emotions plus dimensional analysis
- [ ] Multi-modal fusion improves accuracy by 5-10% over single modality
- [ ] Real-time emotion tracking with <1-second latency
- [ ] Cross-cultural emotion recognition with <15% accuracy degradation
- [ ] Speaker-specific emotion profiling with 85%+ consistency
- [ ] Memory usage <2GB for any single emotion analysis task

## Integration Points

### With Part 3 (Multi-Model Transcription)
- Use transcribed text for text-based emotion analysis
- Provide emotional context to improve transcription accuracy
- Coordinate timing between transcription and emotion detection

### With Part 5 (Speaker Diarization)
- Receive speaker identity for speaker-specific emotion analysis
- Use emotional continuity to validate speaker boundaries
- Enable speaker emotion profiling and comparison

### With Part 7 (WebVTT Generation)
- Provide emotion labels for subtitle enhancement
- Enable emotion-based subtitle styling and formatting
- Supply emotional intensity for visual representation

### With Part 8 (YouTube Integration)
- Generate emotional content summaries for video metadata
- Create emotion-based highlights and chapters
- Provide engagement metrics based on emotional response

## Timeline

**Week 12-13**: Core audio and text emotion model integration  
**Week 14**: Multi-modal fusion and real-time processing  
**Week 15**: Speaker-aware analysis and cultural adaptation  
**Week 16**: Quality assessment and performance optimization  
**Week 17**: Integration with pipeline components and testing

This comprehensive emotion detection system adds rich affective context to transcriptions, enabling enhanced user experiences and deeper content understanding through emotional intelligence.
</document_content>
</document>

<document index="23">
<source>plan/part7.md</source>
<document_content>
---
this_file: plan/part7.md
---

# Part 7: WebVTT Generation with Enhancements

## Overview

Implement sophisticated WebVTT subtitle generation that transforms transcription results into broadcast-quality, accessible captions with precise timing, speaker identification, emotion indicators, and multi-language support. The system will exceed industry standards for readability and platform compatibility.

## Detailed Tasks

### 7.1 Core WebVTT Generation Engine

- [ ] Implement `WebVTTGenerator` with broadcast-quality standards:
  - [ ] Frame-accurate timestamp generation with millisecond precision
  - [ ] Intelligent line breaking at linguistic boundaries
  - [ ] Reading speed optimization (140-180 words per minute)
  - [ ] Character limits per line (42 characters max for readability)
- [ ] Design flexible cue creation system:
  - [ ] Word-level timestamp alignment using forced alignment
  - [ ] Sentence and phrase-aware cue segmentation
  - [ ] Dynamic cue duration based on content complexity
  - [ ] Overlap prevention and timing conflict resolution

### 7.2 Advanced Text Formatting & Typography

- [ ] Implement intelligent text formatting:
  - [ ] Automatic punctuation and capitalization correction
  - [ ] Number and date formatting standardization
  - [ ] Abbreviation expansion for clarity
  - [ ] Technical term and proper noun handling
- [ ] Add typographic enhancements:
  - [ ] Smart quotes and apostrophes
  - [ ] Em dashes and ellipses for natural speech patterns
  - [ ] Mathematical symbols and special characters
  - [ ] Unicode normalization for consistent display

### 7.3 Speaker-Attributed Subtitles

- [ ] Integrate speaker diarization results for attribution:
  - [ ] Speaker name assignment from metadata and context
  - [ ] Color-coded speaker identification using WebVTT styling
  - [ ] Positional speaker differentiation (left/right/center)
  - [ ] Speaker transition handling with smooth visual flow
- [ ] Implement speaker label formatting:
  - [ ] Configurable speaker naming conventions
  - [ ] Dynamic speaker identification (Speaker 1, Speaker 2, etc.)
  - [ ] Known speaker name substitution from metadata
  - [ ] Gender-based or role-based speaker labeling

### 7.4 Emotion-Enhanced Subtitles

- [ ] Integrate emotion detection results for enhanced representation:
  - [ ] Emotion indicators in square brackets [laughing], [crying]
  - [ ] Color-coded emotional intensity using CSS styling
  - [ ] Font weight and style modifications for emotional context
  - [ ] Emotional punctuation enhancement (multiple exclamation marks)
- [ ] Add advanced emotion visualization:
  - [ ] Emoji integration for appropriate emotional expressions
  - [ ] Musical note symbols for singing/humming detection
  - [ ] Visual intensity indicators using opacity and size
  - [ ] Emotion consistency validation across subtitle sequences

### 7.5 Multi-Language Subtitle Generation

- [ ] Implement comprehensive language support:
  - [ ] Automatic language detection and appropriate formatting
  - [ ] Right-to-left (RTL) language support for Arabic, Hebrew
  - [ ] Character set optimization for different languages
  - [ ] Cultural adaptation for punctuation and formatting conventions
- [ ] Add translation and localization features:
  - [ ] Integration with translation APIs for multi-language output
  - [ ] Subtitle timing adjustment for different language lengths
  - [ ] Cultural context preservation in translated subtitles
  - [ ] Subtitle track management for multiple languages simultaneously

### 7.6 Accessibility Compliance

- [ ] Ensure full accessibility standard compliance:
  - [ ] WCAG 2.1 AA compliance for web accessibility
  - [ ] Section 508 compliance for federal accessibility requirements
  - [ ] FCC closed captioning standards for broadcast
  - [ ] Platform-specific accessibility requirements (YouTube, Netflix)
- [ ] Implement advanced accessibility features:
  - [ ] Sound effect descriptions [door slams], [phone rings]
  - [ ] Music and background audio descriptions
  - [ ] Speaker identification for hearing-impaired users
  - [ ] High contrast mode support for visual impairments

### 7.7 Platform Optimization

- [ ] Generate platform-specific subtitle formats:
  - [ ] WebVTT for HTML5 video players and modern platforms
  - [ ] SRT (SubRip) for broad compatibility and legacy support
  - [ ] SSA/ASS for advanced styling and animation features
  - [ ] TTML for broadcast television and streaming services
- [ ] Optimize for major platforms:
  - [ ] YouTube-specific formatting and style requirements
  - [ ] Netflix subtitle standards for streaming quality
  - [ ] Broadcast television timing and display requirements
  - [ ] Social media platform caption specifications

### 7.8 Quality Assurance & Validation

- [ ] Implement comprehensive quality checking:
  - [ ] Reading speed validation and automatic adjustment
  - [ ] Line length and character limit enforcement
  - [ ] Timestamp continuity and overlap detection
  - [ ] Linguistic correctness and grammar checking
- [ ] Add automated testing and validation:
  - [ ] Subtitle timing accuracy verification
  - [ ] Cross-platform compatibility testing
  - [ ] Accessibility compliance validation
  - [ ] Performance benchmarking against industry standards

### 7.9 Advanced Styling & Customization

- [ ] Implement rich styling capabilities:
  - [ ] CSS-based styling with custom themes
  - [ ] Brand-specific color schemes and fonts
  - [ ] Dynamic styling based on content analysis
  - [ ] User preference integration for personalized display
- [ ] Add interactive and enhanced features:
  - [ ] Clickable speaker names for additional information
  - [ ] Searchable subtitle text with highlight functionality
  - [ ] Chapter markers and navigation integration
  - [ ] Subtitle confidence indicators for quality awareness

### 7.10 Performance Optimization & Scalability

- [ ] Optimize subtitle generation performance:
  - [ ] Parallel processing for multiple subtitle tracks
  - [ ] Efficient memory usage for large subtitle files
  - [ ] Streaming subtitle generation for real-time applications
  - [ ] Caching and optimization for repeated processing
- [ ] Implement scalable architecture:
  - [ ] Batch subtitle generation for multiple videos
  - [ ] Cloud-based processing with auto-scaling
  - [ ] API endpoints for subtitle generation services
  - [ ] Integration with content management systems

## Technical Specifications

### WebVTTGenerator Class
```python
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
from enum import Enum

class SubtitleFormat(Enum):
    WEBVTT = "webvtt"
    SRT = "srt"
    SSA = "ssa"
    TTML = "ttml"

@dataclass
class SubtitleCue:
    start_time: float
    end_time: float
    text: str
    speaker_id: Optional[str] = None
    emotion: Optional[str] = None
    confidence: float = 1.0
    styling: Optional[Dict[str, str]] = None
    position: Optional[str] = None

@dataclass
class SubtitleTrack:
    language: str
    cues: List[SubtitleCue]
    metadata: Dict[str, Any]
    styling_info: Optional[Dict[str, str]] = None

class WebVTTGenerator:
    def __init__(self, 
                 max_chars_per_line: int = 42,
                 max_lines_per_cue: int = 2,
                 max_cue_duration: float = 7.0,
                 reading_speed_wpm: int = 160):
        
        self.max_chars_per_line = max_chars_per_line
        self.max_lines_per_cue = max_lines_per_cue
        self.max_cue_duration = max_cue_duration
        self.reading_speed_wpm = reading_speed_wpm
        
    def generate_subtitles(self,
                          transcription_result: TranscriptionResult,
                          diarization_result: Optional[DiarizationResult] = None,
                          emotion_result: Optional[EmotionAnalysisResult] = None,
                          format_type: SubtitleFormat = SubtitleFormat.WEBVTT) -> SubtitleTrack:
        """
        Generate high-quality subtitles with speaker and emotion integration
        """
        
    def optimize_cue_timing(self, cues: List[SubtitleCue]) -> List[SubtitleCue]:
        """Optimize cue timing for readability and flow"""
        
    def apply_intelligent_line_breaks(self, text: str) -> str:
        """Apply linguistic rules for optimal line breaking"""
        
    def generate_speaker_styling(self, speaker_count: int) -> Dict[str, Dict[str, str]]:
        """Generate distinct styling for different speakers"""
```

### Intelligent Line Breaking Algorithm
```python
class IntelligentLineBreaker:
    def __init__(self):
        self.break_priorities = {
            'punctuation': 10,      # Break after punctuation
            'conjunction': 7,       # Break after conjunctions  
            'preposition': 5,       # Break after prepositions
            'article': 3,          # Break after articles
            'natural_pause': 8     # Break at detected pauses
        }
        
    def find_optimal_break_point(self, 
                                text: str, 
                                max_chars: int) -> int:
        """
        Find the best position to break text based on linguistic rules
        
        Priorities:
        1. After punctuation marks
        2. After conjunctions (and, but, or, etc.)
        3. After prepositions when appropriate
        4. At natural pause points detected from audio
        5. Default to word boundaries
        """
        
        words = text.split()
        if len(text) <= max_chars:
            return len(text)  # No break needed
            
        # Find candidate break points
        candidates = []
        char_count = 0
        
        for i, word in enumerate(words):
            char_count += len(word) + (1 if i > 0 else 0)  # +1 for space
            
            if char_count > max_chars:
                break
                
            priority = self._calculate_break_priority(word, i, words)
            candidates.append((i + 1, priority, char_count))
        
        # Select best break point
        if candidates:
            best_break = max(candidates, key=lambda x: x[1])
            return best_break[0]
        
        # Fallback to word boundary
        return self._find_word_boundary(text, max_chars)
```

### Configuration Schema
```yaml
webvtt_generation:
  formatting:
    max_chars_per_line: 42
    max_lines_per_cue: 2
    max_cue_duration: 7.0
    min_cue_duration: 1.0
    reading_speed_wpm: 160
    
  styling:
    speaker_colors:
      - "#FFFFFF"  # White
      - "#FFFF00"  # Yellow
      - "#00FFFF"  # Cyan
      - "#FF00FF"  # Magenta
      - "#00FF00"  # Green
      - "#FF8000"  # Orange
      
    emotion_styling:
      happiness: 
        color: "#FFD700"
        font_weight: "normal"
      sadness:
        color: "#87CEEB"
        font_style: "italic"
      anger:
        color: "#FF4500"
        font_weight: "bold"
        
  features:
    include_speakers: true
    include_emotions: true
    include_confidence_indicators: false
    include_sound_descriptions: true
    
  accessibility:
    wcag_compliance: "AA"
    high_contrast_mode: true
    large_text_support: true
    screen_reader_optimized: true
    
  output_formats:
    primary: "webvtt"
    generate_srt: true
    generate_ass: false
    generate_ttml: false
    
  quality:
    min_confidence_threshold: 0.7
    automatic_quality_flags: true
    linguistic_validation: true
    timing_validation: true
```

## Dependencies

### Core Dependencies
- `webvtt-py >= 0.4.6` - WebVTT format handling
- `pysrt >= 1.1.2` - SRT format support
- `ass >= 0.5.0` - ASS/SSA format support
- `lxml >= 4.9.0` - TTML XML processing

### Advanced Dependencies
- `langdetect >= 1.0.9` - Language detection
- `polyglot >= 16.07.04` - Multi-language text processing
- `nltk >= 3.8.0` - Natural language processing
- `spacy >= 3.6.0` - Advanced linguistic analysis

### Optional Dependencies
- `googletrans >= 4.0.0` - Translation API integration
- `azure-cognitiveservices-language-textanalytics` - Azure translation
- `deep-translator >= 1.11.0` - Multiple translation service support

## Success Criteria

- [ ] Generate broadcast-quality subtitles meeting industry standards
- [ ] Achieve 95%+ readability score on subtitle quality assessments
- [ ] Support 20+ output languages with appropriate formatting
- [ ] Process subtitle generation at 50x real-time speed
- [ ] Maintain <1% timing accuracy error across all generated subtitles
- [ ] Full WCAG 2.1 AA accessibility compliance
- [ ] Support all major platform-specific requirements
- [ ] Memory usage <1GB for any single subtitle generation task

## Integration Points

### With Part 3 (Multi-Model Transcription)
- Receive transcription results with precise word-level timestamps
- Use transcription confidence scores for quality-based formatting
- Coordinate timing with transcription processing completion

### With Part 5 (Speaker Diarization)
- Integrate speaker identification for labeled subtitle generation
- Use speaker boundaries for optimal cue break positioning
- Apply speaker-specific styling and formatting

### With Part 6 (Emotion Detection)
- Incorporate emotion indicators and styling in subtitles
- Use emotional context for enhanced readability decisions
- Apply emotion-based visual enhancements and formatting

### With Part 8 (YouTube Integration)
- Generate YouTube-compatible subtitle formats
- Optimize for YouTube's specific display requirements
- Coordinate with upload API requirements and constraints

## Timeline

**Week 15-16**: Core WebVTT generation and formatting engine  
**Week 17**: Speaker and emotion integration with styling  
**Week 18**: Multi-language support and accessibility compliance  
**Week 19**: Platform optimization and quality assurance  
**Week 20**: Performance optimization and integration testing

This comprehensive subtitle generation system produces professional-quality, accessible captions that exceed industry standards while providing rich speaker and emotional context for enhanced viewer experience.
</document_content>
</document>

<document index="24">
<source>plan/part8.md</source>
<document_content>
---
this_file: plan/part8.md
---

# Part 8: YouTube Integration & Upload

## Overview

Implement comprehensive YouTube API integration for seamless subtitle upload, video metadata enhancement, and channel management automation. The system will handle authentication, quota management, batch operations, and advanced features like chapter generation and analytics integration.

## Detailed Tasks

### 8.1 YouTube API Authentication & Setup

- [ ] Implement robust OAuth 2.0 authentication system:
  - [ ] Secure credential storage with encryption
  - [ ] Automatic token refresh and rotation
  - [ ] Multi-account support for channel management
  - [ ] Service account integration for server deployments
- [ ] Add comprehensive API client management:
  - [ ] Rate limiting and quota tracking
  - [ ] Automatic retry with exponential backoff
  - [ ] Circuit breaker patterns for API failures
  - [ ] Health monitoring and status reporting

### 8.2 Video Metadata Extraction & Enhancement

- [ ] Extract comprehensive video metadata using YouTube Data API:
  - [ ] Video title, description, tags, and categories
  - [ ] Upload date, duration, view count, and engagement metrics
  - [ ] Existing captions and subtitle information
  - [ ] Channel information and branding details
- [ ] Implement metadata-driven transcription enhancement:
  - [ ] Title/description analysis for contextual hints
  - [ ] Tag-based domain identification for model selection  
  - [ ] Channel history analysis for speaker recognition
  - [ ] Thumbnail analysis for visual context

### 8.3 Advanced Subtitle Upload Management

- [ ] Develop intelligent subtitle upload system:
  - [ ] Multiple language track upload with proper language codes
  - [ ] Existing caption detection and update handling
  - [ ] Subtitle format conversion for YouTube compatibility
  - [ ] Quality validation before upload to prevent rejections
- [ ] Implement batch upload optimization:
  - [ ] Parallel upload processing with quota awareness
  - [ ] Priority-based upload queuing system
  - [ ] Resume functionality for interrupted uploads
  - [ ] Progress tracking and status notifications

### 8.4 Quota Management & Optimization

- [ ] Build sophisticated quota management system:
  - [ ] Real-time quota consumption tracking
  - [ ] Predictive quota planning based on workload
  - [ ] Daily quota reset scheduling and optimization
  - [ ] Cost analysis and budget management
- [ ] Add intelligent scheduling features:
  - [ ] Off-peak processing for quota efficiency
  - [ ] Priority-based operation scheduling
  - [ ] Emergency quota reserve management
  - [ ] Multi-API key rotation for increased limits

### 8.5 Video Processing Workflow Integration

- [ ] Create end-to-end YouTube video processing pipeline:
  - [ ] Direct YouTube URL processing without downloads
  - [ ] Video quality assessment and optimization selection
  - [ ] Processing status updates and notifications
  - [ ] Error handling with detailed reporting
- [ ] Implement workflow automation:
  - [ ] Scheduled processing for new uploads
  - [ ] Webhook integration for real-time processing
  - [ ] Batch processing for channel archives
  - [ ] Custom workflow triggers and conditions

### 8.6 Channel Management & Analytics

- [ ] Develop channel-wide management capabilities:
  - [ ] Channel analytics integration for performance tracking
  - [ ] Subtitle coverage analysis and reporting
  - [ ] Language distribution and optimization recommendations
  - [ ] Accessibility compliance monitoring
- [ ] Add advanced analytics features:
  - [ ] Subtitle performance impact analysis
  - [ ] View duration correlation with subtitle availability
  - [ ] Language-specific engagement metrics
  - [ ] Transcription quality impact on discovery

### 8.7 Content Enhancement Features

- [ ] Implement automatic content enhancement:
  - [ ] Chapter generation from transcript analysis
  - [ ] Keyword extraction for improved SEO
  - [ ] Description enhancement with transcript insights
  - [ ] Automatic tag suggestions based on content analysis
- [ ] Add advanced content analysis:
  - [ ] Topic modeling and categorization
  - [ ] Key moment identification for highlights
  - [ ] Sentiment analysis for content insights
  - [ ] Engagement prediction based on content analysis

### 8.8 Multi-Language Channel Support

- [ ] Build comprehensive multi-language capabilities:
  - [ ] Automatic language detection and appropriate processing
  - [ ] Multiple subtitle track management per video
  - [ ] Language-specific processing optimization
  - [ ] Cultural adaptation for different markets
- [ ] Implement localization features:
  - [ ] Auto-translation integration for subtitle expansion
  - [ ] Regional content adaptation
  - [ ] Language-specific accessibility requirements
  - [ ] Cultural context preservation in translations

### 8.9 Error Handling & Recovery

- [ ] Create robust error handling system:
  - [ ] Comprehensive error categorization and logging
  - [ ] Automatic retry strategies for different error types
  - [ ] Fallback processing options for API failures
  - [ ] User notification system for critical errors
- [ ] Add monitoring and alerting:
  - [ ] Real-time API status monitoring
  - [ ] Performance degradation alerts
  - [ ] Quota exhaustion warnings
  - [ ] Quality assurance failure notifications

### 8.10 Integration & Deployment

- [ ] Coordinate with entire transcription pipeline:
  - [ ] Seamless handoff from subtitle generation
  - [ ] Status reporting throughout processing chain
  - [ ] Quality gates before YouTube upload
  - [ ] Post-upload validation and confirmation
- [ ] Implement deployment and scaling features:
  - [ ] Cloud deployment with auto-scaling
  - [ ] Container orchestration for high-volume processing
  - [ ] Load balancing for multiple API keys
  - [ ] Geographic distribution for global processing

## Technical Specifications

### YouTubeManager Class
```python
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import asyncio
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials

@dataclass
class YouTubeVideo:
    video_id: str
    title: str
    description: str
    duration: int
    language: str
    channel_id: str
    existing_captions: List[str]
    metadata: Dict[str, Any]

@dataclass
class UploadResult:
    video_id: str
    language: str
    status: str  # success, failed, pending
    caption_id: Optional[str] = None
    error_message: Optional[str] = None
    processing_time: Optional[float] = None

class YouTubeManager:
    def __init__(self, 
                 credentials_file: Path,
                 quota_limit: int = 10000,
                 max_concurrent_uploads: int = 5):
        
        self.service = self._initialize_service(credentials_file)
        self.quota_tracker = QuotaTracker(quota_limit)
        self.upload_semaphore = asyncio.Semaphore(max_concurrent_uploads)
        
    async def process_video_url(self, url: str) -> YouTubeVideo:
        """Extract video metadata from YouTube URL"""
        
    async def upload_subtitles(self,
                              video_id: str,
                              subtitle_tracks: Dict[str, SubtitleTrack]) -> List[UploadResult]:
        """Upload multiple subtitle tracks to YouTube video"""
        
    async def batch_process_channel(self,
                                  channel_id: str,
                                  filters: Optional[Dict] = None) -> List[UploadResult]:
        """Process all videos in a channel with filtering options"""
        
    def generate_video_chapters(self, transcript: TranscriptionResult) -> List[Dict[str, Any]]:
        """Generate chapter markers from transcript analysis"""
        
    def enhance_video_metadata(self, 
                             video: YouTubeVideo,
                             transcript: TranscriptionResult) -> Dict[str, str]:
        """Enhance video metadata using transcript insights"""
```

### Quota Management System
```python
class QuotaTracker:
    def __init__(self, daily_limit: int = 10000):
        self.daily_limit = daily_limit
        self.current_usage = 0
        self.operations_cost = {
            'captions.list': 50,
            'captions.insert': 400,
            'captions.update': 450,
            'captions.delete': 50,
            'videos.list': 1
        }
        
    def can_perform_operation(self, operation: str, count: int = 1) -> bool:
        """Check if operation can be performed within quota"""
        cost = self.operations_cost.get(operation, 0) * count
        return (self.current_usage + cost) <= self.daily_limit
        
    def record_operation(self, operation: str, count: int = 1):
        """Record quota usage for an operation"""
        cost = self.operations_cost.get(operation, 0) * count
        self.current_usage += cost
        
    def get_remaining_quota(self) -> int:
        """Get remaining quota for the day"""
        return max(0, self.daily_limit - self.current_usage)
        
    def reset_daily_usage(self):
        """Reset usage counter for new day"""
        self.current_usage = 0
        
    def predict_operation_capacity(self, operation: str) -> int:
        """Predict how many operations can still be performed"""
        cost = self.operations_cost.get(operation, 0)
        if cost == 0:
            return float('inf')
        return (self.daily_limit - self.current_usage) // cost
```

### Configuration Schema
```yaml
youtube_integration:
  authentication:
    client_secrets_file: "client_secrets.json"
    token_storage_path: "~/.vttiro/youtube_tokens"
    scopes:
      - "https://www.googleapis.com/auth/youtube.force-ssl"
      - "https://www.googleapis.com/auth/youtube.readonly"
      
  quota_management:
    daily_limit: 10000
    reserve_quota: 1000  # Emergency reserve
    quota_reset_time: "00:00 PST"
    cost_optimization: true
    
  upload_settings:
    max_concurrent_uploads: 5
    retry_attempts: 3
    timeout_seconds: 300
    validate_before_upload: true
    
  processing:
    auto_generate_chapters: true
    enhance_descriptions: true
    extract_keywords: true
    analyze_engagement_potential: true
    
  quality_assurance:
    min_transcription_confidence: 0.8
    validate_subtitle_timing: true
    check_accessibility_compliance: true
    preview_generation: true
    
  batch_processing:
    max_videos_per_batch: 100
    processing_schedule: "off_peak"  # immediate, off_peak, scheduled
    priority_queue: true
    status_notifications: true
    
  analytics:
    track_performance_impact: true
    generate_coverage_reports: true
    monitor_engagement_correlation: true
    export_analytics_data: true
```

### API Error Handling
```python
class YouTubeAPIErrorHandler:
    def __init__(self):
        self.retry_strategies = {
            403: self._handle_quota_exceeded,
            404: self._handle_not_found,
            500: self._handle_server_error,
            503: self._handle_service_unavailable
        }
        
    async def handle_api_error(self, error, operation_context: Dict) -> bool:
        """
        Handle API errors with appropriate retry strategies
        
        Returns:
            bool: True if operation should be retried, False otherwise
        """
        error_code = getattr(error, 'resp', {}).get('status', 0)
        
        if error_code in self.retry_strategies:
            return await self.retry_strategies[error_code](error, operation_context)
        
        # Log unknown errors
        logger.error(f"Unknown API error: {error_code} - {str(error)}")
        return False
        
    async def _handle_quota_exceeded(self, error, context: Dict) -> bool:
        """Handle quota exceeded errors"""
        # Queue for retry during next quota period
        await self.queue_for_retry(context, delay_until_quota_reset=True)
        return False
        
    async def _handle_server_error(self, error, context: Dict) -> bool:
        """Handle server errors with exponential backoff"""
        retry_count = context.get('retry_count', 0)
        if retry_count < 3:
            delay = 2 ** retry_count
            await asyncio.sleep(delay)
            return True
        return False
```

## Dependencies

### Core Dependencies
- `google-api-python-client >= 2.100.0` - YouTube Data API client
- `google-auth-oauthlib >= 1.1.0` - OAuth authentication
- `google-auth-httplib2 >= 0.1.0` - HTTP library for auth
- `google-auth >= 2.23.0` - Google authentication library

### Advanced Dependencies
- `aiohttp >= 3.8.0` - Async HTTP client for performance
- `tenacity >= 8.2.0` - Retry mechanisms with backoff
- `prometheus-client >= 0.17.0` - Metrics and monitoring
- `redis >= 5.0.0` - Caching and queue management

### Optional Dependencies
- `celery >= 5.3.0` - Distributed task processing
- `kubernetes >= 27.2.0` - Kubernetes deployment integration
- `boto3 >= 1.28.0` - AWS integration for additional storage

## Success Criteria

- [ ] Successfully upload subtitles to 99%+ of supported YouTube videos
- [ ] Process YouTube uploads within daily quota constraints
- [ ] Achieve <5% API error rate with automatic recovery
- [ ] Support batch processing of 1000+ videos per day
- [ ] Maintain <30-second response time for single video processing
- [ ] Generate accurate chapter markers for 90%+ of videos
- [ ] Provide comprehensive analytics and performance insights
- [ ] Support multi-account and multi-channel management

## Integration Points

### With Part 2 (Video Processing)
- Coordinate YouTube video downloading with processing pipeline
- Extract YouTube-specific metadata for enhanced processing
- Handle YouTube-specific format requirements and constraints

### With Part 3 (Multi-Model Transcription)
- Use YouTube metadata for improved transcription context
- Leverage channel history for speaker recognition enhancement
- Optimize model selection based on YouTube content characteristics

### With Part 7 (WebVTT Generation)
- Ensure subtitle format compatibility with YouTube requirements
- Generate YouTube-optimized subtitle styling and formatting
- Coordinate subtitle quality validation before upload

### With All Pipeline Components
- Provide end-to-end processing status and progress updates
- Coordinate error handling and recovery across entire pipeline
- Enable comprehensive workflow automation and scheduling

## Timeline

**Week 18-19**: Core YouTube API integration and authentication  
**Week 20**: Subtitle upload system and quota management  
**Week 21**: Batch processing and workflow automation  
**Week 22**: Analytics integration and content enhancement  
**Week 23**: Error handling, monitoring, and deployment optimization

This comprehensive YouTube integration system provides seamless end-to-end processing from video URL to uploaded subtitles, with enterprise-grade reliability, quota management, and performance optimization.
</document_content>
</document>

<document index="25">
<source>plan/part9.md</source>
<document_content>
---
this_file: plan/part9.md
---

# Part 9: Multi-Environment Deployment & Testing

## Overview

Implement comprehensive deployment strategies for local development, Google Colab, cloud production, and edge environments, along with extensive testing frameworks to ensure reliability, performance, and scalability across all deployment scenarios.

## Detailed Tasks

### 9.1 Local Development Environment

- [ ] Create comprehensive local setup with hardware optimization:
  - [ ] GPU acceleration setup for NVIDIA, AMD, and Apple Silicon
  - [ ] CPU-only fallback configurations for resource-constrained systems
  - [ ] Docker containerization for consistent development environments
  - [ ] Development dependency management with uv and virtual environments
- [ ] Implement local model management:
  - [ ] Model downloading and caching system
  - [ ] Model quantization for memory-efficient inference
  - [ ] Automatic model updates and version management
  - [ ] Local model performance benchmarking and optimization

### 9.2 Google Colab Integration

- [ ] Develop Colab-specific optimizations and UI:
  - [ ] Interactive notebook interface with rich widgets
  - [ ] Session management and checkpoint saving
  - [ ] GPU availability detection and optimization
  - [ ] Automatic reconnection handling and state recovery
- [ ] Create Colab installation packages:
  - [ ] One-click setup scripts for easy installation
  - [ ] Colab-optimized model loading and caching
  - [ ] Integration with Google Drive for file management
  - [ ] Real-time progress tracking and visualization

### 9.3 Cloud Production Deployment

- [ ] Implement Kubernetes-based production deployment:
  - [ ] Horizontal Pod Autoscaler configuration for GPU workloads
  - [ ] Multi-zone deployment for high availability
  - [ ] Service mesh integration for traffic management
  - [ ] Secret management for API keys and credentials
- [ ] Add cloud provider-specific optimizations:
  - [ ] AWS deployment with EKS and GPU instances
  - [ ] Google Cloud deployment with GKE and TPU support
  - [ ] Azure deployment with AKS and cognitive services integration
  - [ ] Multi-cloud deployment strategies and failover

### 9.4 Comprehensive Testing Framework

- [ ] Implement multi-layered testing approach:
  - [ ] Unit tests for all core components with >95% coverage
  - [ ] Integration tests for API interactions and model inference
  - [ ] End-to-end tests for complete video processing workflows
  - [ ] Performance benchmarks and regression testing
- [ ] Add specialized testing scenarios:
  - [ ] Stress testing for high-volume processing
  - [ ] Chaos engineering for failure resilience
  - [ ] Load testing for concurrent processing limits
  - [ ] Quality assurance testing for transcription accuracy

### 9.5 Performance Monitoring & Analytics

- [ ] Deploy comprehensive observability stack:
  - [ ] Distributed tracing with OpenTelemetry integration
  - [ ] Metrics collection using Prometheus and Grafana
  - [ ] Log aggregation with ELK stack or similar
  - [ ] Custom dashboards for business and technical metrics
- [ ] Implement application performance monitoring:
  - [ ] Real-time performance metrics tracking
  - [ ] Resource utilization monitoring and alerts
  - [ ] Quality metrics tracking across all components
  - [ ] Cost analysis and optimization recommendations

### 9.6 CI/CD Pipeline Implementation

- [ ] Build robust continuous integration/deployment:
  - [ ] Automated testing pipeline with multiple environments
  - [ ] Model testing and validation before deployment
  - [ ] Blue-green deployments for zero-downtime updates
  - [ ] Rollback mechanisms and canary deployments
- [ ] Add quality gates and automation:
  - [ ] Code quality checks with static analysis
  - [ ] Security scanning for dependencies and containers
  - [ ] Performance regression detection
  - [ ] Automated documentation generation and validation

### 9.7 Scalability & Load Management

- [ ] Implement intelligent scaling strategies:
  - [ ] Queue-based processing with dynamic scaling
  - [ ] GPU resource pooling and efficient allocation
  - [ ] Cost-optimized scaling with spot instances
  - [ ] Geographic load distribution for global processing
- [ ] Add resource optimization features:
  - [ ] Batch processing optimization for throughput
  - [ ] Model caching and sharing across instances
  - [ ] Intelligent workload distribution
  - [ ] Resource usage prediction and planning

### 9.8 Security & Compliance

- [ ] Implement comprehensive security measures:
  - [ ] End-to-end encryption for data in transit and at rest
  - [ ] Role-based access control and authentication
  - [ ] API security with rate limiting and authentication
  - [ ] Compliance with GDPR, CCPA, and other privacy regulations
- [ ] Add security monitoring and audit:
  - [ ] Security event logging and monitoring
  - [ ] Vulnerability scanning and patch management
  - [ ] Access audit trails and compliance reporting
  - [ ] Data retention and deletion policies

### 9.9 Disaster Recovery & Business Continuity

- [ ] Develop robust disaster recovery capabilities:
  - [ ] Automated backup and recovery procedures
  - [ ] Multi-region deployment for geographic redundancy
  - [ ] Data replication and consistency management
  - [ ] Recovery time objectives (RTO) and recovery point objectives (RPO)
- [ ] Implement business continuity planning:
  - [ ] Failover procedures and runbooks
  - [ ] Capacity planning for disaster scenarios
  - [ ] Communication protocols for incident response
  - [ ] Regular disaster recovery testing and validation

### 9.10 Documentation & Maintenance

- [ ] Create comprehensive documentation ecosystem:
  - [ ] API documentation with interactive examples
  - [ ] Deployment guides for all supported environments
  - [ ] Troubleshooting guides and FAQ
  - [ ] Architecture decision records (ADRs)
- [ ] Establish maintenance and support procedures:
  - [ ] Regular health checks and system validation
  - [ ] Model performance monitoring and retraining schedules
  - [ ] Dependency updates and security patches
  - [ ] User support and community engagement

## Technical Specifications

### Multi-Environment Configuration
```python
from typing import Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

class DeploymentEnvironment(Enum):
    LOCAL = "local"
    COLAB = "colab"
    CLOUD_DEV = "cloud_dev"
    CLOUD_PROD = "cloud_prod"
    EDGE = "edge"

@dataclass
class EnvironmentConfig:
    name: str
    compute_resources: Dict[str, Any]
    storage_config: Dict[str, Any]
    api_endpoints: Dict[str, str]
    monitoring_config: Dict[str, Any]
    security_config: Dict[str, Any]

class DeploymentManager:
    def __init__(self, environment: DeploymentEnvironment):
        self.environment = environment
        self.config = self._load_environment_config()
        
    def _load_environment_config(self) -> EnvironmentConfig:
        """Load configuration specific to deployment environment"""
        
    def setup_environment(self) -> bool:
        """Initialize environment-specific setup"""
        
    def validate_deployment(self) -> Dict[str, bool]:
        """Validate deployment health and configuration"""
        
    def scale_resources(self, target_capacity: int) -> bool:
        """Scale resources based on demand"""
```

### Testing Framework Configuration
```yaml
testing:
  unit_tests:
    coverage_threshold: 95
    test_runner: "pytest"
    parallel_execution: true
    mock_external_apis: true
    
  integration_tests:
    test_environments:
      - local
      - staging
    api_test_timeout: 300
    model_accuracy_threshold: 0.85
    
  performance_tests:
    load_testing:
      concurrent_users: 100
      duration_minutes: 30
      ramp_up_seconds: 60
    stress_testing:
      max_concurrent_videos: 1000
      memory_limit_gb: 32
      
  quality_assurance:
    transcription_accuracy:
      benchmark_datasets:
        - "librispeech_test_clean"
        - "common_voice_test"
      minimum_wer: 0.05  # 5% Word Error Rate
    diarization_accuracy:
      benchmark_datasets:
        - "ami_test"
        - "voxconverse_test"
      maximum_der: 0.10  # 10% Diarization Error Rate
```

### Kubernetes Deployment Configuration
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vttiro-transcription
spec:
  replicas: 3
  selector:
    matchLabels:
      app: vttiro-transcription
  template:
    metadata:
      labels:
        app: vttiro-transcription
    spec:
      containers:
      - name: vttiro
        image: vttiro:latest
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
            nvidia.com/gpu: "1"
          limits:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: GPU_ENABLED
          value: "true"
        volumeMounts:
        - name: model-cache
          mountPath: /app/models
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: vttiro-service
spec:
  selector:
    app: vttiro-transcription
  ports:
  - port: 8000
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vttiro-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vttiro-transcription
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Docker Configuration
```dockerfile
# Multi-stage build for optimization
FROM nvidia/cuda:12.1-devel-ubuntu22.04 as builder

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.12 \
    python3.12-dev \
    python3-pip \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv for fast package management
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.cargo/bin:$PATH"

# Set up working directory
WORKDIR /app

# Copy and install dependencies
COPY pyproject.toml uv.lock ./
RUN uv sync --frozen

# Copy application code
COPY src/ ./src/
COPY tests/ ./tests/

# Run tests and build
RUN uv run pytest tests/ --cov=src/vttiro --cov-report=term-missing
RUN uv build

# Production image
FROM nvidia/cuda:12.1-runtime-ubuntu22.04

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    python3.12 \
    python3-pip \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.cargo/bin:$PATH"

# Set up working directory
WORKDIR /app

# Copy built wheel and install
COPY --from=builder /app/dist/*.whl .
RUN uv pip install --system *.whl[all] && rm *.whl

# Create non-root user
RUN useradd -m -u 1000 vttiro
USER vttiro

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python3 -c "import vttiro; print('OK')" || exit 1

# Default command
CMD ["python3", "-m", "vttiro.cli", "serve", "--host", "0.0.0.0", "--port", "8000"]
```

## Dependencies

### Core Deployment Dependencies
- `docker >= 24.0.0` - Container runtime
- `kubernetes >= 1.28.0` - Container orchestration  
- `helm >= 3.13.0` - Kubernetes package manager
- `terraform >= 1.6.0` - Infrastructure as code

### Testing Dependencies
- `pytest >= 7.4.0` - Primary testing framework
- `pytest-cov >= 4.1.0` - Coverage reporting
- `pytest-asyncio >= 0.21.0` - Async test support
- `locust >= 2.17.0` - Load testing framework

### Monitoring Dependencies
- `prometheus-client >= 0.17.0` - Metrics collection
- `opentelemetry-distro >= 0.44b0` - Distributed tracing
- `structlog >= 23.2.0` - Structured logging
- `sentry-sdk >= 1.38.0` - Error tracking

### Optional Dependencies
- `ansible >= 8.5.0` - Configuration management
- `vault >= 1.15.0` - Secrets management
- `consul >= 1.17.0` - Service discovery

## Success Criteria

- [ ] Support all four installation modes (basic, local, colab, all)
- [ ] Achieve 99.9% uptime in production deployments
- [ ] Scale automatically from 2 to 100+ instances based on demand
- [ ] Complete test suite with >95% code coverage
- [ ] Zero-downtime deployments with automated rollback
- [ ] Processing latency <30 seconds for 10-minute videos
- [ ] Support 1000+ concurrent video processing jobs
- [ ] Comprehensive monitoring with <5-minute alert resolution

## Integration Points

### With All Pipeline Components
- Provide deployment infrastructure for entire transcription pipeline
- Enable comprehensive end-to-end testing across all components
- Support scaling and load management for complete system

### With External Systems
- Integrate with cloud provider services (AWS, GCP, Azure)
- Connect with monitoring and alerting systems
- Support integration with customer CI/CD pipelines

## Timeline

**Week 20-21**: Local and Colab deployment optimization  
**Week 22-23**: Cloud production deployment and Kubernetes setup  
**Week 24**: Comprehensive testing framework implementation  
**Week 25**: Performance monitoring and CI/CD pipeline  
**Week 26**: Security, compliance, and disaster recovery  
**Week 27-28**: Documentation, validation, and final integration testing

This comprehensive deployment and testing framework ensures vttiro can reliably operate across diverse environments while maintaining high performance, security, and scalability standards suitable for production use at any scale.
</document_content>
</document>

<document index="26">
<source>pyproject.toml</source>
<document_content>
# this_file: pyproject.toml
#==============================================================================
# VTTIRO PACKAGE CONFIGURATION
# This pyproject.toml defines the package metadata, dependencies, build system,
# and development environment for the vttiro package.
#==============================================================================

#------------------------------------------------------------------------------
# PROJECT METADATA
# Core package information used by PyPI and package managers.
#------------------------------------------------------------------------------
[project]
name = 'vttiro' # Package name on PyPI
description = 'Simple video transcription to WebVTT subtitles using AI models (Gemini, AssemblyAI, Deepgram)' # Short description
readme = 'README.md' # Path to README file
requires-python = '>=3.12' # Minimum Python version (updated to 3.12+ for modern features)
keywords = [
    'transcription', 'video', 'audio', 'webvtt', 'subtitles', 'ai', 'speech-to-text',
    'speaker-diarization', 'emotion-detection', 'youtube', 'whisper', 'gemini'
] # Keywords for PyPI search
dynamic = ["version"] # Fields set dynamically at build time

# PyPI classifiers for package categorization
classifiers = [
    'Development Status :: 4 - Beta', # Package maturity level
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Operating System :: OS Independent',
    'License :: OSI Approved :: MIT License',
    'Intended Audience :: Developers',
    'Topic :: Multimedia :: Video',
    'Topic :: Multimedia :: Sound/Audio :: Speech',
    'Topic :: Scientific/Engineering :: Artificial Intelligence',
    'Topic :: Text Processing :: Linguistic',
]

# Core dependencies required for all installation modes
dependencies = [
    'pydantic>=2.5.0',          # Data validation and settings management
    'loguru>=0.7.2',            # Advanced logging with structured output
    'fire>=0.5.0',              # CLI framework
    'rich>=13.7.0',             # Beautiful terminal output and progress bars
    'pyyaml>=6.0.1',            # YAML configuration support
    'httpx>=0.26.0',            # Modern HTTP client for async requests
    'yt-dlp>=2024.12.13',       # Video downloading and processing
    'ffmpeg-python>=0.2.0',     # FFmpeg integration for audio processing
]

# Author information
[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

# License information
[project.license]
text = 'MIT'

# Project URLs
[project.urls]
Documentation = 'https://github.com/twardoch/vttiro#readme'
Issues = 'https://github.com/twardoch/vttiro/issues'
Source = 'https://github.com/twardoch/vttiro'

#------------------------------------------------------------------------------
# OPTIONAL DEPENDENCIES
# Additional dependencies for optional features, development, and testing.
#------------------------------------------------------------------------------
[project.optional-dependencies]

# Basic mode: API-only transcription services (default installation)
basic = [
    'google-generativeai>=0.8.3',    # Google Gemini 2.0 Flash API
    'assemblyai>=0.36.1',             # AssemblyAI Universal-2 API
    'deepgram-sdk>=3.7.2',            # Deepgram Nova-3 API
    'google-api-python-client>=2.151.0',  # YouTube Data API
    'google-auth-oauthlib>=1.2.1',    # YouTube OAuth authentication
]

# Local mode: Local inference capabilities with GPU/CPU support
local = [
    'torch>=2.1.2',                   # PyTorch for local inference
    'torchaudio>=2.1.2',              # Audio processing with PyTorch
    'transformers>=4.36.2',           # HuggingFace transformers
    'pyannote.audio>=3.1.1',          # Speaker diarization (requires HF token)
    'speechbrain>=1.0.0',             # Speech emotion recognition
    'librosa>=0.10.1',                # Audio analysis and feature extraction
    'numpy>=1.26.2',                  # Numerical processing
    'scipy>=1.11.4',                  # Signal processing
    'scikit-learn>=1.3.2',            # Machine learning utilities
    'whisperx>=3.1.5',                # Enhanced Whisper with alignment
    'faster-whisper>=1.0.3',          # Optimized Whisper inference
    'openai-whisper>=20231117',       # Original OpenAI Whisper
]

# Colab mode: Google Colab integration with UI widgets
colab = [
    'ipython>=8.18.1',                # Interactive Python for Colab
    'ipywidgets>=8.1.1',              # Interactive widgets for Jupyter/Colab
    'jupyter>=1.0.0',                 # Jupyter notebook support
    'matplotlib>=3.8.2',              # Plotting for visualizations
    'seaborn>=0.13.0',                # Enhanced plotting
    'plotly>=5.17.0',                 # Interactive plots
    'tqdm>=4.66.1',                   # Progress bars in notebooks
]

# All mode: Complete installation with all features
all = [
    'google-generativeai>=0.8.3',     # API dependencies
    'assemblyai>=0.36.1',
    'deepgram-sdk>=3.7.2',
    'google-api-python-client>=2.151.0',
    'google-auth-oauthlib>=1.2.1',
    'torch>=2.1.2',                   # Local inference dependencies
    'torchaudio>=2.1.2',
    'transformers>=4.36.2',
    'pyannote.audio>=3.1.1',
    'speechbrain>=1.0.0',
    'librosa>=0.10.1',
    'numpy>=1.26.2',
    'scipy>=1.11.4',
    'scikit-learn>=1.3.2',
    'whisperx>=3.1.5',
    'faster-whisper>=1.0.3',
    'openai-whisper>=20231117',
    'ipython>=8.18.1',                # Colab dependencies
    'ipywidgets>=8.1.1',
    'jupyter>=1.0.0',
    'matplotlib>=3.8.2',
    'seaborn>=0.13.0',
    'plotly>=5.17.0',
    'tqdm>=4.66.1',
]

# Development tools
dev = [
    'pre-commit>=4.1.0',
    'ruff>=0.9.7',
    'mypy>=1.15.0',
    'absolufy-imports>=0.3.1',
    'pyupgrade>=3.19.1',
    'isort>=6.0.1',
]

# Testing tools and frameworks
test = [
    'pytest>=8.3.4',
    'pytest-cov>=6.0.0',
    'pytest-xdist>=3.6.1',
    'pytest-benchmark[histogram]>=5.1.0',
    'pytest-asyncio>=0.25.3',
    'pytest-timeout>=2.3.1',        # Test timeout protection
    'pytest-mock>=3.12.0',          # Enhanced mocking capabilities
    'coverage[toml]>=7.6.12',
    'hypothesis>=6.98.0',            # Property-based testing
    'memory-profiler>=0.61.0',      # Memory usage profiling
    'factory-boy>=3.3.0',           # Test data factories
    'freezegun>=1.4.0',             # Time mocking for tests
]

# Documentation dependencies
docs = [
    "sphinx>=7.2.6",
    "sphinx-rtd-theme>=2.0.0",
    "sphinx-autodoc-typehints>=2.0.0",
    "myst-parser>=3.0.0",
]

#------------------------------------------------------------------------------
# COMMAND-LINE SCRIPTS
# Entry points for command-line executables installed with the package.
#------------------------------------------------------------------------------
[project.scripts]
vttiro = "vttiro.cli:main"

#------------------------------------------------------------------------------
# BUILD SYSTEM CONFIGURATION
# Defines the tools required to build the package and the build backend.
#------------------------------------------------------------------------------
[build-system]
# Hatchling is a modern build backend for Python packaging
# hatch-vcs integrates with version control systems for versioning
requires = [
    'hatchling>=1.27.0', # Keep hatchling as is, update if newer hatchling version is required
    'hatch-vcs>=0.4.0', # Keep hatch-vcs as is, update if newer hatch-vcs version is required
]
build-backend = 'hatchling.build' # Specifies Hatchling as the build backend


#------------------------------------------------------------------------------
# HATCH BUILD CONFIGURATION
# Configures the build process, specifying which packages to include and
# how to handle versioning.
#------------------------------------------------------------------------------
[tool.hatch.build]
# Include package data files
include = [
    "src/vttiro/py.typed", # For better type checking support
    "src/vttiro/data/**/*", # Include data files if any

]
exclude = ["**/__pycache__", "**/.pytest_cache", "**/.mypy_cache"]

[tool.hatch.build.targets.wheel]
packages = ["src/vttiro"]
reproducible = true


# Version control system hook configuration
# Automatically updates the version file from git tags
[tool.hatch.build.hooks.vcs]
version-file = "src/vttiro/__version__.py"

# Version source configuration
[tool.hatch.version]
source = 'vcs' # Get version from git tags or other VCS info

# Metadata handling configuration
[tool.hatch.metadata]
allow-direct-references = true # Allow direct references in metadata (useful for local dependencies)


#------------------------------------------------------------------------------
# DEVELOPMENT ENVIRONMENTS

[tool.hatch.envs.default]
features = ['dev', 'test', 'all']
dependencies = [
]

# Commands available in the default environment
[tool.hatch.envs.default.scripts]
# Run tests with optional arguments
test = 'pytest {args:tests}'
# Run tests with coverage reporting
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vttiro --cov=tests {args:tests}"
# Run type checking
type-check = "mypy src/vttiro tests"
# Run linting and formatting
lint = ["ruff check src/vttiro tests", "ruff format --respect-gitignore src/vttiro tests"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore src/vttiro tests", "ruff check --fix src/vttiro tests"]
fix = ["ruff check --fix --unsafe-fixes src/vttiro tests", "ruff format --respect-gitignore src/vttiro tests"]

# Matrix configuration to test across multiple Python versions

[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]

#------------------------------------------------------------------------------
# SPECIALIZED ENVIRONMENTS
# Additional environments for specific development tasks.
#------------------------------------------------------------------------------

# Dedicated environment for linting and code quality checks
[tool.hatch.envs.lint]
detached = true # Create a separate, isolated environment
features = ['dev'] # Use dev extras  dependencies 

# Linting environment commands
[tool.hatch.envs.lint.scripts]
# Type checking with automatic type installation
typing = "mypy --install-types --non-interactive {args:src/vttiro tests}"
# Check style and format code
style = ["ruff check {args:.}", "ruff format --respect-gitignore {args:.}"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore {args:.}", "ruff check --fix {args:.}"]
fix = ["ruff check --fix --unsafe-fixes {args:.}", "ruff format --respect-gitignore {args:.}"]
# Run all ops
all = ["style", "typing", "fix"]

# Dedicated environment for testing
[tool.hatch.envs.test]
features = ['test'] # Use test extras as dependencies

# Testing environment commands
[tool.hatch.envs.test.scripts]
# Run tests in parallel
test = "python -m pytest -n auto {args:tests}"
# Run tests with coverage in parallel
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vttiro --cov=tests {args:tests}"
# Run benchmarks
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
# Run benchmarks and save results
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

# Documentation environment
[tool.hatch.envs.docs]
features = ['docs']

# Documentation environment commands
[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs/source docs/build"

# GitHub Actions workflow configuration
[tool.hatch.envs.ci]
features = ['test']


[tool.hatch.envs.ci.scripts]
test = "pytest --cov=src/vttiro --cov-report=xml"


#------------------------------------------------------------------------------
# CODE QUALITY TOOLS
# Configuration for linting, formatting, and code quality enforcement.
#------------------------------------------------------------------------------

#------------------------------------------------------------------------------
# COVERAGE CONFIGURATION
# Settings for test coverage measurement and reporting.
#------------------------------------------------------------------------------

# Path mapping for coverage in different environments
[tool.coverage.paths]
vttiro = ["src/vttiro", "*/vttiro/src/vttiro"]
tests = ["tests", "*/vttiro/tests"]

# Coverage report configuration
[tool.coverage.report]
# Lines to exclude from coverage reporting
exclude_lines = [
    'no cov', # Custom marker to skip coverage
    'if __name__ == .__main__.:', # Script execution guard
    'if TYPE_CHECKING:', # Type checking imports and code
    'pass', # Empty pass statements
    'raise NotImplementedError', # Unimplemented method placeholders
    'raise ImportError', # Import error handling
    'except ImportError', # Import error handling
    'except KeyError', # Common error handling
    'except AttributeError', # Common error handling
    'except NotImplementedError', # Common error handling
]

[tool.coverage.run]
source_pkgs = ["vttiro", "tests"]
branch = true # Measure branch coverage (if/else statements)
parallel = true # Support parallel test execution
omit = [
    "src/vttiro/__about__.py",
]

#------------------------------------------------------------------------------
# MYPY CONFIGURATION
# Configuration for type checking with mypy.
#------------------------------------------------------------------------------

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ["tests.*"]
disallow_untyped_defs = false
disallow_incomplete_defs = false

#------------------------------------------------------------------------------
# PYTEST CONFIGURATION
# Configuration for pytest, including markers, options, and benchmark settings.
#------------------------------------------------------------------------------

[tool.pytest.ini_options]
addopts = "-v --durations=10 -p no:briefcase"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
console_output_style = "progress"
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
log_cli = true
log_cli_level = "INFO"
markers = [
    "benchmark: marks tests as benchmarks (select with '-m benchmark')",
    "unit: mark a test as a unit test",
    "integration: mark a test as an integration test",
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "network: marks tests that require network access",
    "api: marks tests that require API keys",
    "gpu: marks tests that require GPU",
    "property: marks property-based tests using Hypothesis",
    "performance: marks performance and scalability tests",
]
norecursedirs = [
    ".*",
    "build",
    "dist", 
    "venv",
    "__pycache__",
    "*.egg-info",
    "_private",
]
python_classes = ["Test*"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
testpaths = ["tests"]

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min",    # Minimum time
    "max",    # Maximum time
    "mean",   # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr",    # Inter-quartile range
    "ops",    # Operations per second
    "rounds", # Number of rounds
]

#------------------------------------------------------------------------------
# RUFF CONFIGURATION
# Configuration for Ruff, including linter and formatter settings.
#------------------------------------------------------------------------------ 

# Ruff linter and formatter configuration
[tool.ruff]
target-version = "py310"
line-length = 120

# Linting rules configuration
[tool.ruff.lint]
# Rule sets to enable, organized by category
select = [
    # flake8 plugins and extensions
    'A', # flake8-builtins: checks for shadowed builtins
    'ARG', # flake8-unused-arguments: checks for unused function arguments
    'ASYNC', # flake8-async: checks for async/await issues
    'B', # flake8-bugbear: finds likely bugs and design problems
    'C', # flake8-comprehensions: helps write better list/dict comprehensions
    'DTZ', # flake8-datetimez: checks for datetime timezone issues
    'E', # pycodestyle errors: PEP 8 style guide errors
    'EM', # flake8-errmsg: checks for better error messages
    'F', # pyflakes: detects various errors
    'FBT', # flake8-boolean-trap: checks for boolean traps in function signatures
    'I', # isort: sorts imports
    'ICN', # flake8-import-conventions: checks for import conventions
    'ISC', # flake8-implicit-str-concat: checks for implicit string concatenation
    'LOG', # flake8-logging: checks for logging issues
    'N', # pep8-naming: checks naming conventions
    'PLC', # pylint convention: checks for convention issues
    'PLE', # pylint error: checks for errors
    'PLR', # pylint refactor: suggests refactors
    'PLW', # pylint warning: checks for suspicious code
    'PT', # flake8-pytest-style: checks pytest-specific style
    'PTH', # flake8-use-pathlib: checks for stdlib path usage vs pathlib
    'PYI', # flake8-pyi: checks stub files
    'RET', # flake8-return: checks return statement consistency
    'RSE', # flake8-raise: checks raise statements
    'RUF', # Ruff-specific rules
    'S', # flake8-bandit: checks for security issues
    'SIM', # flake8-simplify: checks for code simplification opportunities
    'T', # flake8-print: checks for print statements
    'TCH', # flake8-type-checking: helps with type-checking
    'TID', # flake8-tidy-imports: checks for tidy import statements
    'UP', # pyupgrade: checks for opportunities to use newer Python features
    'W', # pycodestyle warnings: PEP 8 style guide warnings
    'YTT', # flake8-2020: checks for misuse of sys.version or sys.version_info

]
# Rules to ignore (with reasons)
ignore = [
    'B027', # Empty method in abstract base class - sometimes needed for interfaces
    'C901', # Function is too complex - sometimes complexity is necessary
    'FBT003', # Boolean positional argument in function definition - sometimes unavoidable
    'PLR0911', # Too many return statements - sometimes needed for readability
    'PLR0912', # Too many branches - sometimes needed for complex logic
    'PLR0913', # Too many arguments - sometimes needed in APIs
    'PLR0915', # Too many statements - sometimes needed for comprehensive functions
    'PLR1714', # Consider merging multiple comparisons - sometimes less readable
    'PLW0603', # Using the global statement - sometimes necessary
    'PT013', # Pytest explicit test parameter - sometimes clearer
    'PTH123', # Path traversal - sometimes needed
    'PYI056', # Calling open() in pyi file - sometimes needed in type stubs
    'S105', # Possible hardcoded password - often false positives
    'S106', # Possible hardcoded password - often false positives
    'S107', # Possible hardcoded password - often false positives
    'S110', # try-except-pass - sometimes valid for suppressing exceptions
    'SIM102'
    # Nested if statements - sometimes more readable than combined conditions
]
# Rules that should not be automatically fixed
unfixable = [
    'F401', # Don't automatically remove unused imports - may be needed later

]
# Configure extend-exclude to ignore specific directories
extend-exclude = [".git", ".venv", "venv", "dist", "build"]

# isort configuration within Ruff
[tool.ruff.lint.isort]
known-first-party = ['vttiro'] # Treat as first-party imports for sorting

# flake8-tidy-imports configuration within Ruff
[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = 'all' # Ban all relative imports for consistency

# Per-file rule exceptions
[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
'tests/**/*' = [
    'PLR2004', # Allow magic values in tests for readability
    'S101', # Allow assertions in tests
    'TID252'
    # Allow relative imports in tests for convenience
]

</document_content>
</document>

<document index="27">
<source>pytest.ini</source>
<document_content>
# this_file: pytest.ini
[tool:pytest]
# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Output options
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --cov=src/vttiro
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=85
    --no-cov-on-fail

# Markers
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    api: marks tests that require API keys
    network: marks tests that require network access
    gpu: marks tests that require GPU
    benchmark: marks tests for performance benchmarking

# Asyncio configuration
asyncio_mode = auto

# Minimum version
minversion = 6.0

# Test timeout (in seconds)
timeout = 300
timeout_method = thread

# Warnings
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning

# Test fixtures configuration
usefixtures = 
    tmp_path
    monkeypatch
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/__init__.py
# Language: python

from vttiro.__version__ import __version__
from vttiro.core.transcription import TranscriptionResult
from vttiro.core.config import VttiroConfig
from vttiro.core.transcriber import Transcriber


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/cli.py
# Language: python

import sys
import asyncio
from pathlib import Path
from typing import Optional
import ffmpeg
import fire
from rich.console import Console
from rich.progress import track
from loguru import logger
from vttiro.__version__ import __version__
from vttiro.core.file_transcriber import FileTranscriber
from vttiro.models import (
    TranscriptionEngine, 
    GeminiModel, 
    AssemblyAIModel, 
    DeepgramModel,
    ModelCapability,
    get_default_model,
    get_available_models,
    validate_engine_model_combination,
    get_model_capabilities,
    get_models_by_capability,
    estimate_transcription_cost
)
import google.generativeai
import assemblyai
from deepgram import DeepgramClient

class VttiroCLI:
    """Simple command-line interface for vttiro file transcription."""
    def __init__((self)):
    def version((self)) -> str:
        """Show vttiro version information."""
    def _analyze_file_and_recommend_model((self, file_path: Path, engine: str, model: str)) -> None:
        """Analyze input file and provide model recommendations with warnings."""
    def transcribe((
        self,
        input_file: str,
        output: Optional[str] = None,
        engine: str = "gemini",
        model: Optional[str] = None
    )) -> str:
        """Transcribe audio/video file to WebVTT subtitles."""
    def formats((self)) -> None:
        """Show supported input formats."""
    def engines((self)) -> None:
        """List available AI engines."""
    def models((self, engine: Optional[str] = None, detailed: bool = False)) -> None:
        """List available models with capabilities, optionally filtered by engine."""
    def _is_supported_format((self, file_path: Path)) -> bool:
        """Check if file format is supported."""
    def _check_engine_dependencies((self, engine: str)) -> bool:
        """Check if engine dependencies are available."""
    def help((self)) -> None:
        """Show help information."""

def __init__((self)):

def version((self)) -> str:
    """Show vttiro version information."""

def _analyze_file_and_recommend_model((self, file_path: Path, engine: str, model: str)) -> None:
    """Analyze input file and provide model recommendations with warnings."""

def transcribe((
        self,
        input_file: str,
        output: Optional[str] = None,
        engine: str = "gemini",
        model: Optional[str] = None
    )) -> str:
    """Transcribe audio/video file to WebVTT subtitles."""

def formats((self)) -> None:
    """Show supported input formats."""

def engines((self)) -> None:
    """List available AI engines."""

def models((self, engine: Optional[str] = None, detailed: bool = False)) -> None:
    """List available models with capabilities, optionally filtered by engine."""

def format_duration((seconds: Optional[int])) -> str:
    """Format duration limit for display."""

def format_cost((cost_per_min: Optional[float])) -> str:
    """Format cost per minute for display."""

def format_features((capability: ModelCapability)) -> str:
    """Format feature list for display."""

def _is_supported_format((self, file_path: Path)) -> bool:
    """Check if file format is supported."""

def _check_engine_dependencies((self, engine: str)) -> bool:
    """Check if engine dependencies are available."""

def help((self)) -> None:
    """Show help information."""

def main(()) -> None:
    """Main entry point for vttiro CLI."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/config/__init__.py
# Language: python



# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/config/enhanced.py
# Language: python

import os
import re
import hashlib
import base64
import tempfile
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Literal, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import threading
import time
from pydantic import BaseModel, Field, field_validator, model_validator, SecretStr
from pydantic.types import StrictInt, StrictFloat, StrictBool, StrictStr, conint, confloat, constr
import yaml
from cryptography.fernet import Fernet
from loguru import logger
from vttiro.utils.exceptions import ConfigurationError, ValidationError
import os
import psutil

class LogLevel(s, t, r, ,,  , E, n, u, m):
    """Logging levels enumeration."""

class OutputFormat(s, t, r, ,,  , E, n, u, m):
    """Supported output formats."""

class TranscriptionModel(s, t, r, ,,  , E, n, u, m):
    """Supported transcription models."""

class WCAGLevel(s, t, r, ,,  , E, n, u, m):
    """WCAG compliance levels."""

class EnvironmentType(s, t, r, ,,  , E, n, u, m):
    """Environment types for configuration profiles."""

class ValidationContext:
    """Context for configuration validation."""
    def add_error((self, error: str)) -> None:
        """Add validation error."""
    def has_errors((self)) -> bool:
        """Check if validation has errors."""

class SecretManager:
    """Secure secret management with encryption."""
    def __init__((self, encryption_key: Optional[bytes] = None)):
        """Initialize secret manager."""
    def encrypt_secret((self, secret: str)) -> str:
        """Encrypt a secret value."""
    def decrypt_secret((self, encrypted_secret: str)) -> str:
        """Decrypt a secret value."""
    def get_secret((self, key: str, encrypted_value: Optional[str] = None)) -> Optional[str]:
        """Get secret value with caching."""
    def set_secret((self, key: str, value: str, encrypt: bool = True)) -> str:
        """Set secret value with optional encryption."""
    def clear_cache((self)) -> None:
        """Clear secrets cache."""
    def get_encryption_key((self)) -> str:
        """Get encryption key as base64 string."""

class SecureApiConfig(B, a, s, e, M, o, d, e, l):
    """Secure API configuration with comprehensive validation."""
    def get_available_models((self)) -> Set[TranscriptionModel]:
        """Get available transcription models based on configured API keys."""

class ProcessingConfig(B, a, s, e, M, o, d, e, l):
    """Enhanced processing configuration with comprehensive validation."""

class ValidationConfig(B, a, s, e, M, o, d, e, l):
    """Input validation and security configuration."""

class MonitoringConfig(B, a, s, e, M, o, d, e, l):
    """Monitoring and observability configuration."""

class EnhancedVttiroConfig(B, a, s, e, M, o, d, e, l):
    """Enhanced main configuration with comprehensive validation and security."""
    def __init__((self, **data)):
        """Initialize with timestamp."""
    def validate_comprehensive((self, context: Optional[ValidationContext] = None)) -> ValidationContext:
        """Perform comprehensive validation with context."""
    def get_secret_value((self, secret_path: str, secret_manager: Optional[SecretManager] = None)) -> Optional[str]:
        """Get decrypted secret value."""
    def update_timestamp((self)) -> None:
        """Update the last modified timestamp."""
    def to_dict_safe((self)) -> Dict[str, Any]:
        """Convert to dictionary with secrets masked."""
    def save_to_file((
        self, 
        file_path: Union[str, Path], 
        mask_secrets: bool = True,
        secret_manager: Optional[SecretManager] = None
    )) -> None:
        """Save configuration to file with optional secret encryption."""

class Config:
    """Pydantic configuration."""

def add_error((self, error: str)) -> None:
    """Add validation error."""

def has_errors((self)) -> bool:
    """Check if validation has errors."""

def __init__((self, encryption_key: Optional[bytes] = None)):
    """Initialize secret manager."""

def encrypt_secret((self, secret: str)) -> str:
    """Encrypt a secret value."""

def decrypt_secret((self, encrypted_secret: str)) -> str:
    """Decrypt a secret value."""

def get_secret((self, key: str, encrypted_value: Optional[str] = None)) -> Optional[str]:
    """Get secret value with caching."""

def set_secret((self, key: str, value: str, encrypt: bool = True)) -> str:
    """Set secret value with optional encryption."""

def clear_cache((self)) -> None:
    """Clear secrets cache."""

def get_encryption_key((self)) -> str:
    """Get encryption key as base64 string."""

def validate_secret_format((cls, v)):
    """Validate secret format and length."""

def validate_api_configuration((cls, values)):
    """Validate overall API configuration."""

def get_available_models((self)) -> Set[TranscriptionModel]:
    """Get available transcription models based on configured API keys."""

def validate_chunk_duration((cls, v)):
    """Validate chunk duration is reasonable."""

def validate_max_workers((cls, v)):
    """Validate max workers against system capabilities."""

def validate_performance_settings((cls, values)):
    """Validate overall performance configuration."""

def validate_extensions((cls, v)):
    """Validate file extensions format."""

def __init__((self, **data)):
    """Initialize with timestamp."""

def validate_config_version((cls, v)):
    """Validate configuration version."""

def validate_environment_specific_settings((cls, values)):
    """Validate settings specific to environment type."""

def validate_comprehensive((self, context: Optional[ValidationContext] = None)) -> ValidationContext:
    """Perform comprehensive validation with context."""

def get_secret_value((self, secret_path: str, secret_manager: Optional[SecretManager] = None)) -> Optional[str]:
    """Get decrypted secret value."""

def update_timestamp((self)) -> None:
    """Update the last modified timestamp."""

def to_dict_safe((self)) -> Dict[str, Any]:
    """Convert to dictionary with secrets masked."""

def mask_secrets((obj: Any, path: str = "")) -> Any:

def from_file((
        cls, 
        file_path: Union[str, Path], 
        secret_manager: Optional[SecretManager] = None
    )) -> 'EnhancedVttiroConfig':
    """Load configuration from file with secret decryption."""

def save_to_file((
        self, 
        file_path: Union[str, Path], 
        mask_secrets: bool = True,
        secret_manager: Optional[SecretManager] = None
    )) -> None:
    """Save configuration to file with optional secret encryption."""

def create_profile((cls, environment: EnvironmentType)) -> 'EnhancedVttiroConfig':
    """Create configuration profile for specific environment."""


<document index="28">
<source>src/vttiro/config/templates/development.yaml</source>
<document_content>
# this_file: src/vttiro/config/templates/development.yaml
# Development Configuration Template for vttiro
# This template is optimized for local development with debugging enabled

config_version: "2.0"
environment: "development"

# API Configuration for Development
api:
  # Add your API keys here or use environment variables
  gemini_api_key: null  # Set GEMINI_API_KEY environment variable
  assemblyai_api_key: null  # Set ASSEMBLYAI_API_KEY environment variable
  deepgram_api_key: null  # Set DEEPGRAM_API_KEY environment variable
  huggingface_token: null  # Set HUGGINGFACE_TOKEN environment variable
  
  # Development-friendly timeouts
  timeout_seconds: 60
  max_retries: 2
  retry_delay: 1.0
  
  # Rate limiting (relaxed for development)
  rate_limit_per_minute: 100
  burst_limit: 20

# Processing Configuration for Development
processing:
  # Smaller chunks for faster development iteration
  chunk_duration: 300  # 5 minutes
  overlap_duration: 15  # 15 seconds
  max_duration: 18000  # 5 hours
  sample_rate: 16000
  prefer_integer_seconds: true
  
  # Development-optimized settings
  energy_threshold_percentile: 25
  min_energy_window: 1.5
  
  # Parallel processing (conservative for development)
  max_workers: 2
  worker_timeout: 300
  enable_gpu: false  # Disable GPU for development unless needed

# Validation Configuration
validation:
  # Relaxed validation for development
  strict_mode: false
  validate_on_load: true
  
  # File validation
  max_file_size_mb: 500
  allowed_domains: ["youtube.com", "youtu.be", "localhost"]
  allowed_file_types: [".mp4", ".avi", ".mov", ".mp3", ".wav", ".m4a"]

# Caching Configuration for Development
caching:
  enabled: true
  
  # Memory cache (smaller for development)
  memory_cache_size: 50
  memory_ttl_seconds: 1800  # 30 minutes
  
  # Disk cache (local development directory)
  disk_cache_enabled: true
  disk_cache_path: "~/.cache/vttiro/dev"
  disk_cache_size_mb: 500
  disk_ttl_seconds: 86400  # 1 day
  

# Monitoring Configuration for Development
monitoring:
  enabled: true
  
  # Logging
  log_level: "DEBUG"
  log_to_file: true
  log_file_path: "logs/vttiro-dev.log"
  log_rotation_size_mb: 50
  log_retention_days: 7
  
  # Metrics (lightweight for development)
  metrics_enabled: true
  metrics_port: 8080
  detailed_metrics: true
  
  # Health checks
  health_check_enabled: true
  health_check_port: 8081

# Security Configuration for Development
security:
  # Relaxed security for development convenience
  encryption_enabled: false  # Can be disabled for development
  api_key_validation: false
  rate_limiting_enabled: false
  
  # CORS (permissive for development)
  cors_enabled: true
  cors_origins: ["http://localhost:*", "http://127.0.0.1:*"]

# Output Configuration
output:
  default_format: "webvtt"
  max_chars_per_line: 42
  max_lines_per_cue: 2
  max_cue_duration: 7.0
  reading_speed_wpm: 160
  
  # Accessibility (standard settings)
  wcag_compliance: "AA"
  include_sound_descriptions: true
  
  # Development output settings
  output_directory: "output"
  backup_enabled: false

# AI Model Configuration for Development
models:
  # Default model selection for development
  default_provider: "gemini"
  fallback_providers: ["assemblyai", "deepgram"]
  
  # Model-specific settings optimized for development speed
  gemini:
    model_name: "gemini-2.0-flash-exp"
    temperature: 0.1
    enable_streaming: true
  
  assemblyai:
    model_name: "universal-2"
    enable_diarization: true
    enable_punctuation: true
  
  deepgram:
    model_name: "nova-2"
    enable_smart_format: true
    enable_profanity_filter: false

# Feature Flags for Development
features:
  # Enable experimental features in development
  speaker_diarization: true
  emotion_detection: false  # Disable for faster processing
  auto_segmentation: true
  context_aware_prompting: true
  
  # YouTube integration
  youtube_integration: false  # Disable unless testing YouTube features
  auto_upload_subtitles: false

# Development-specific settings
development:
  # Debug settings
  debug_mode: true
  verbose_logging: true
  save_intermediate_files: true
  intermediate_files_path: "debug"
  
  # Development tools
  enable_profiling: false
  profile_output_path: "profiles"
  
  # Hot reloading
  hot_reload_config: true
  config_watch_interval: 5  # seconds
</document_content>
</document>

<document index="29">
<source>src/vttiro/config/templates/production.yaml</source>
<document_content>
# this_file: src/vttiro/config/templates/production.yaml
# Production Configuration Template for vttiro
# This template is optimized for production deployment with security and performance

config_version: "2.0"
environment: "production"

# API Configuration for Production
api:
  # API keys MUST be set via environment variables in production
  gemini_api_key: null  # REQUIRED: Set GEMINI_API_KEY environment variable
  assemblyai_api_key: null  # REQUIRED: Set ASSEMBLYAI_API_KEY environment variable
  deepgram_api_key: null  # REQUIRED: Set DEEPGRAM_API_KEY environment variable
  huggingface_token: null  # REQUIRED: Set HUGGINGFACE_TOKEN environment variable
  
  # Production-optimized timeouts
  timeout_seconds: 120
  max_retries: 5
  retry_delay: 2.0
  
  # Strict rate limiting for production
  rate_limit_per_minute: 300
  burst_limit: 50

# Processing Configuration for Production
processing:
  # Optimized chunks for production throughput
  chunk_duration: 600  # 10 minutes
  overlap_duration: 30  # 30 seconds
  max_duration: 36000  # 10 hours
  sample_rate: 16000
  prefer_integer_seconds: true
  
  # Production-optimized settings
  energy_threshold_percentile: 20
  min_energy_window: 2.0
  
  # Parallel processing (optimized for production)
  max_workers: null  # Auto-detect based on CPU cores
  worker_timeout: 600
  enable_gpu: true  # Use GPU acceleration when available

# Validation Configuration for Production
validation:
  # Strict validation for production security
  strict_mode: true
  validate_on_load: true
  
  # File validation (security-focused)
  max_file_size_mb: 2048  # 2GB limit
  allowed_domains: ["youtube.com", "youtu.be"]  # Restrict to known safe domains
  allowed_file_types: [".mp4", ".avi", ".mov", ".mp3", ".wav", ".m4a", ".webm"]

# Caching Configuration for Production
caching:
  enabled: true
  
  # Memory cache (larger for production)
  memory_cache_size: 200
  memory_ttl_seconds: 3600  # 1 hour
  
  # Disk cache (production storage)
  disk_cache_enabled: true
  disk_cache_path: "/var/cache/vttiro"
  disk_cache_size_mb: 10240  # 10GB
  disk_ttl_seconds: 604800  # 1 week
  

# Monitoring Configuration for Production
monitoring:
  enabled: true
  
  # Logging (production-optimized)
  log_level: "INFO"
  log_to_file: true
  log_file_path: "/var/log/vttiro/vttiro.log"
  log_rotation_size_mb: 100
  log_retention_days: 30
  structured_logging: true
  
  # Metrics (comprehensive for production)
  metrics_enabled: true
  metrics_port: 9090  # Prometheus metrics port
  detailed_metrics: true
  export_prometheus: true
  
  # Health checks
  health_check_enabled: true
  health_check_port: 8080
  health_check_path: "/health"
  readiness_check_path: "/ready"
  
  # Alerting
  alerting_enabled: true
  error_threshold: 0.05  # 5% error rate threshold
  latency_threshold_seconds: 30

# Security Configuration for Production
security:
  # Strict security for production
  encryption_enabled: true
  api_key_validation: true
  rate_limiting_enabled: true
  
  # CORS (restrictive for production)
  cors_enabled: false  # Disable unless specifically needed
  cors_origins: []  # Specify allowed origins if CORS is enabled
  
  # Input sanitization
  input_sanitization: true
  path_traversal_protection: true
  
  # Audit logging
  audit_logging: true
  audit_log_path: "/var/log/vttiro/audit.log"

# Output Configuration for Production
output:
  default_format: "webvtt"
  max_chars_per_line: 42
  max_lines_per_cue: 2
  max_cue_duration: 7.0
  reading_speed_wpm: 160
  
  # Accessibility (full compliance)
  wcag_compliance: "AA"
  include_sound_descriptions: true
  
  # Production output settings
  output_directory: "/var/lib/vttiro/output"
  backup_enabled: true
  backup_directory: "/var/lib/vttiro/backups"
  backup_retention_days: 7

# AI Model Configuration for Production
models:
  # Production model selection strategy
  default_provider: "gemini"
  fallback_providers: ["assemblyai", "deepgram"]
  
  # Circuit breaker settings for production
  circuit_breaker_enabled: true
  failure_threshold: 5
  recovery_timeout: 60
  
  # Model-specific settings optimized for production quality
  gemini:
    model_name: "gemini-2.0-flash"
    temperature: 0.0  # Deterministic for production
    enable_streaming: true
    timeout_seconds: 120
  
  assemblyai:
    model_name: "universal-2"
    enable_diarization: true
    enable_punctuation: true
    timeout_seconds: 180
  
  deepgram:
    model_name: "nova-3"
    enable_smart_format: true
    enable_profanity_filter: true
    timeout_seconds: 120

# Feature Flags for Production
features:
  # Production-ready features only
  speaker_diarization: true
  emotion_detection: true
  auto_segmentation: true
  context_aware_prompting: true
  
  # YouTube integration (production settings)
  youtube_integration: true
  auto_upload_subtitles: false  # Require explicit user action

# Production-specific settings
production:
  # Performance settings
  enable_performance_monitoring: true
  performance_metrics_interval: 60  # seconds
  
  # Resource management
  memory_monitoring: true
  memory_threshold_mb: 8192  # 8GB
  disk_monitoring: true
  disk_threshold_percent: 85
  
  # Scaling
  auto_scaling_enabled: false  # Handled by orchestration platform
  max_concurrent_jobs: 50
  job_queue_size: 200
  
  # Maintenance
  maintenance_mode: false
  maintenance_message: "Service temporarily unavailable for maintenance"
  
  # Backup and recovery
  backup_enabled: true
  backup_schedule: "0 2 * * *"  # Daily at 2 AM
  backup_retention_days: 30
  
  # Deployment
  deployment_environment: "production"
  service_name: "vttiro"
  service_version: null  # Set via CI/CD pipeline
  
# Container and Orchestration Settings
container:
  # Resource limits for containerized deployment
  memory_limit: "16Gi"
  cpu_limit: "8"
  memory_request: "8Gi"
  cpu_request: "4"
  
  # Health check settings
  liveness_probe_path: "/health"
  readiness_probe_path: "/ready"
  startup_probe_path: "/health"
  
  # Volume mounts
  cache_volume: "/var/cache/vttiro"
  log_volume: "/var/log/vttiro"
  config_volume: "/etc/vttiro"
</document_content>
</document>

<document index="30">
<source>src/vttiro/config/templates/testing.yaml</source>
<document_content>
# this_file: src/vttiro/config/templates/testing.yaml
# Testing Configuration Template for vttiro
# This template is optimized for automated testing with mocking and test isolation

config_version: "2.0"
environment: "testing"

# API Configuration for Testing
api:
  # Use mock API keys for testing (never real keys in tests)
  gemini_api_key: "test-gemini-key-123"
  assemblyai_api_key: "test-assemblyai-key-123"
  deepgram_api_key: "test-deepgram-key-123"
  huggingface_token: "test-hf-token-123"
  
  # Fast timeouts for testing
  timeout_seconds: 10
  max_retries: 1
  retry_delay: 0.1
  
  # No rate limiting in tests
  rate_limit_per_minute: 10000
  burst_limit: 1000

# Processing Configuration for Testing
processing:
  # Small chunks for fast tests
  chunk_duration: 30  # 30 seconds
  overlap_duration: 5   # 5 seconds
  max_duration: 300     # 5 minutes max for tests
  sample_rate: 16000
  prefer_integer_seconds: true
  
  # Fast processing for tests
  energy_threshold_percentile: 30
  min_energy_window: 0.5
  
  # Minimal parallel processing for test determinism
  max_workers: 1
  worker_timeout: 30
  enable_gpu: false  # Always use CPU for consistent test results

# Validation Configuration for Testing
validation:
  # Relaxed validation for test flexibility
  strict_mode: false
  validate_on_load: false  # Allow test data without validation
  
  # Permissive file validation for test files
  max_file_size_mb: 100  # Smaller for test files
  allowed_domains: ["*"]  # Allow any domain for testing
  allowed_file_types: [".mp4", ".avi", ".mov", ".mp3", ".wav", ".m4a", ".webm", ".test"]

# Caching Configuration for Testing
caching:
  enabled: false  # Disable caching for test isolation
  
  # Memory cache (disabled for tests)
  memory_cache_size: 0
  memory_ttl_seconds: 0
  
  # Disk cache (use temporary directories)
  disk_cache_enabled: false
  disk_cache_path: "/tmp/vttiro-test-cache"
  disk_cache_size_mb: 10
  disk_ttl_seconds: 60
  

# Monitoring Configuration for Testing
monitoring:
  enabled: true
  
  # Testing-specific logging
  log_level: "DEBUG"
  log_to_file: false  # Log to stdout/stderr for test runners
  log_file_path: "/tmp/vttiro-test.log"
  log_rotation_size_mb: 10
  log_retention_days: 1
  structured_logging: true
  
  # Minimal metrics for testing
  metrics_enabled: false
  metrics_port: 0  # Disable metrics server
  detailed_metrics: false
  
  # No health checks needed in tests
  health_check_enabled: false
  health_check_port: 0

# Security Configuration for Testing
security:
  # Minimal security for testing speed
  encryption_enabled: false
  api_key_validation: false
  rate_limiting_enabled: false
  
  # Permissive CORS for testing
  cors_enabled: true
  cors_origins: ["*"]
  
  # Disable security features that slow down tests
  input_sanitization: false
  path_traversal_protection: false
  audit_logging: false

# Output Configuration for Testing
output:
  default_format: "webvtt"
  max_chars_per_line: 42
  max_lines_per_cue: 2
  max_cue_duration: 7.0
  reading_speed_wpm: 160
  
  # Standard accessibility for testing
  wcag_compliance: "AA"
  include_sound_descriptions: true
  
  # Test output settings (use temporary directories)
  output_directory: "/tmp/vttiro-test-output"
  backup_enabled: false

# AI Model Configuration for Testing
models:
  # Use mock providers for testing
  default_provider: "mock"
  fallback_providers: ["mock"]
  
  # Disable circuit breaker for deterministic tests
  circuit_breaker_enabled: false
  failure_threshold: 100
  recovery_timeout: 1
  
  # Mock model settings
  mock:
    model_name: "test-model"
    temperature: 0.0
    enable_streaming: false
    timeout_seconds: 5
    mock_responses: true
  
  # Real model settings (for integration tests)
  gemini:
    model_name: "gemini-2.0-flash-exp"
    temperature: 0.0
    enable_streaming: false
    timeout_seconds: 10
  
  assemblyai:
    model_name: "universal-2"
    enable_diarization: false  # Faster for tests
    enable_punctuation: true
    timeout_seconds: 15
  
  deepgram:
    model_name: "nova-2"
    enable_smart_format: false  # Faster for tests
    enable_profanity_filter: false
    timeout_seconds: 10

# Feature Flags for Testing
features:
  # Enable all features for comprehensive testing
  speaker_diarization: true
  emotion_detection: true
  auto_segmentation: true
  context_aware_prompting: true
  
  # YouTube integration (with mocking)
  youtube_integration: true
  auto_upload_subtitles: false

# Testing-specific settings
testing:
  # Test isolation
  use_temporary_directories: true
  cleanup_after_tests: true
  test_data_directory: "tests/data"
  
  # Mock settings
  mock_api_responses: true
  mock_file_operations: false
  mock_network_requests: true
  
  # Test data
  use_synthetic_audio: true
  synthetic_audio_duration: 10  # seconds
  test_video_urls: [
    "https://example.com/test-video-1.mp4",
    "https://example.com/test-video-2.mp4"
  ]
  
  # Performance testing
  performance_testing: false
  benchmark_iterations: 1
  
  # Integration testing
  integration_testing: false
  real_api_testing: false  # Set to true for API integration tests
  
  # Property-based testing
  property_testing: true
  hypothesis_max_examples: 100
  hypothesis_deadline: 5000  # 5 seconds
  
  # Fixtures and mocks
  fixture_directory: "tests/fixtures"
  mock_responses_directory: "tests/mocks"
  
  # Test database settings
  test_database_url: "sqlite:///:memory:"
  reset_database_per_test: true
  
  # Parallel testing
  parallel_tests: false  # Disable for consistent results
  test_workers: 1

# Test Environment Variables
test_env:
  # Override environment variables for testing
  VTTIRO_ENV: "testing"
  VTTIRO_DEBUG: "true"
  VTTIRO_LOG_LEVEL: "DEBUG"
  PYTHONPATH: "src:tests"
  
  # Test-specific paths
  VTTIRO_CONFIG_PATH: "/tmp/vttiro-test-config"
  VTTIRO_CACHE_PATH: "/tmp/vttiro-test-cache"
  VTTIRO_LOG_PATH: "/tmp/vttiro-test-logs"

# Pytest Configuration
pytest:
  # Test discovery
  testpaths: ["tests"]
  python_files: ["test_*.py", "*_test.py"]
  python_classes: ["Test*"]
  python_functions: ["test_*"]
  
  # Test execution
  addopts: [
    "-v",                    # Verbose output
    "--tb=short",           # Short traceback format
    "--strict-markers",     # Strict marker checking
    "--disable-warnings",   # Disable warnings for cleaner output
    "--color=yes"           # Colored output
  ]
  
  # Markers
  markers: [
    "unit: Unit tests",
    "integration: Integration tests",
    "performance: Performance tests",
    "slow: Slow tests",
    "api: Tests requiring real API calls",
    "gpu: Tests requiring GPU"
  ]
  
  # Coverage
  coverage_enabled: true
  coverage_threshold: 85
  coverage_fail_under: 80
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/core/__init__.py
# Language: python



# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/core/config.py
# Language: python

import os
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from pydantic import BaseModel, Field, field_validator
import yaml

class TranscriptionConfig(B, a, s, e, M, o, d, e, l):
    """Configuration for transcription engines."""

class ProcessingConfig(B, a, s, e, M, o, d, e, l):
    """Configuration for audio/video processing."""

class DiarizationConfig(B, a, s, e, M, o, d, e, l):
    """Configuration for speaker diarization."""

class EmotionConfig(B, a, s, e, M, o, d, e, l):
    """Configuration for emotion detection."""

class OutputConfig(B, a, s, e, M, o, d, e, l):
    """Configuration for subtitle output generation."""

class YouTubeConfig(B, a, s, e, M, o, d, e, l):
    """Configuration for YouTube integration."""

class VttiroConfig(B, a, s, e, M, o, d, e, l):
    """Main configuration class for vttiro."""
    def save_to_file((self, file_path: Union[str, Path])) -> None:
        """Save configuration to YAML file."""
    def update_from_env((self)) -> None:
        """Update configuration from environment variables with VTTIRO_ prefix."""

class Config:
    """Pydantic configuration."""

class TranscriptionResult:
    """Result of transcription processing."""

def validate_confidence((cls, v)):

def validate_chunk_duration((cls, v)):

def load_from_file((cls, file_path: Union[str, Path])) -> 'VttiroConfig':
    """Load configuration from YAML file."""

def save_to_file((self, file_path: Union[str, Path])) -> None:
    """Save configuration to YAML file."""

def get_default_config_path((cls)) -> Path:
    """Get the default configuration file path."""

def load_default((cls)) -> 'VttiroConfig':
    """Load configuration from default location or create new one."""

def update_from_env((self)) -> None:
    """Update configuration from environment variables with VTTIRO_ prefix."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/core/file_transcriber.py
# Language: python

import asyncio
import uuid
import time
from pathlib import Path
from typing import Optional, Union, List, Dict, Any
from loguru import logger
import logging as logger
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.config.enhanced import EnhancedVttiroConfig
from vttiro.processing.simple_audio import SimpleAudioProcessor
from vttiro.output.simple_webvtt import SimpleWebVTTGenerator, SimpleTranscriptSegment
from vttiro.models import (
    TranscriptionEngine,
    get_default_model,
    validate_engine_model_combination
)
from vttiro.utils.exceptions import (
    VttiroError,
    ProcessingError,
    TranscriptionError,
    ValidationError
)
from vttiro.models.gemini import GeminiTranscriber
from vttiro.models.base import GeminiModel
from vttiro.models.assemblyai import AssemblyAITranscriber
from vttiro.models.base import AssemblyAIModel
from vttiro.models.deepgram import DeepgramTranscriber
from vttiro.models.base import DeepgramModel

class FileTranscriber:
    """Simple file transcriber for local audio/video files."""
    def __init__((self, config: Optional[Union[VttiroConfig, EnhancedVttiroConfig]] = None)):
        """Initialize the file transcriber."""
    def _convert_enhanced_config((self, enhanced_config: EnhancedVttiroConfig)) -> VttiroConfig:
        """Convert enhanced config to simple config for backward compatibility."""
    def validate_file((self, file_path: Path)) -> bool:
        """Validate input file format and existence."""
    def _extract_audio((self, video_path: Path)) -> Path:
        """Extract audio from video file using the audio processor."""
    def _create_transcriber((self, engine: str, model: str)):
        """Create appropriate transcriber based on engine and model."""
    def _transcribe_with_retry((self, transcriber, audio_path: Path, correlation_id: str, max_retries: int = 3, timeout_seconds: int = 300)):
        """Transcribe with retry logic and timeout handling."""
    def transcribe_file((
        self, 
        file_path: Union[str, Path], 
        output_path: Optional[Union[str, Path]] = None,
        engine: Optional[str] = None,
        model: Optional[str] = None
    )) -> Path:
        """Transcribe audio/video file to WebVTT subtitles."""
    def _save_webvtt((self, result: TranscriptionResult, output_path: Path)) -> None:
        """Save transcription result to WebVTT file."""
    def _create_webvtt_segments((self, result: TranscriptionResult)) -> List[SimpleTranscriptSegment]:
        """Convert TranscriptionResult to SimpleTranscriptSegment objects."""
    def get_supported_formats((self)) -> List[str]:
        """Get list of supported input formats."""
    def __repr__((self)) -> str:
        """String representation of FileTranscriber."""

def __init__((self, config: Optional[Union[VttiroConfig, EnhancedVttiroConfig]] = None)):
    """Initialize the file transcriber."""

def _convert_enhanced_config((self, enhanced_config: EnhancedVttiroConfig)) -> VttiroConfig:
    """Convert enhanced config to simple config for backward compatibility."""

def validate_file((self, file_path: Path)) -> bool:
    """Validate input file format and existence."""

def _extract_audio((self, video_path: Path)) -> Path:
    """Extract audio from video file using the audio processor."""

def _create_transcriber((self, engine: str, model: str)):
    """Create appropriate transcriber based on engine and model."""

def _transcribe_with_retry((self, transcriber, audio_path: Path, correlation_id: str, max_retries: int = 3, timeout_seconds: int = 300)):
    """Transcribe with retry logic and timeout handling."""

def transcribe_file((
        self, 
        file_path: Union[str, Path], 
        output_path: Optional[Union[str, Path]] = None,
        engine: Optional[str] = None,
        model: Optional[str] = None
    )) -> Path:
    """Transcribe audio/video file to WebVTT subtitles."""

def _save_webvtt((self, result: TranscriptionResult, output_path: Path)) -> None:
    """Save transcription result to WebVTT file."""

def _create_webvtt_segments((self, result: TranscriptionResult)) -> List[SimpleTranscriptSegment]:
    """Convert TranscriptionResult to SimpleTranscriptSegment objects."""

def get_supported_formats((self)) -> List[str]:
    """Get list of supported input formats."""

def __repr__((self)) -> str:
    """String representation of FileTranscriber."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/core/prompts.py
# Language: python

from typing import Optional, Dict, Any, List
from enum import Enum
from loguru import logger
import logging as logger

class PromptTemplate(E, n, u, m):
    """Predefined prompt templates for different use cases."""

class WebVTTPromptGenerator:
    """Advanced WebVTT prompt generator for AI transcription engines."""
    def __init__((
        self, 
        include_examples: bool = True,
        include_diarization: bool = True,
        include_emotions: bool = True,
        template: PromptTemplate = PromptTemplate.BASIC_WEBVTT
    )):
        """Initialize the WebVTT prompt generator."""
    def generate_webvtt_prompt((
        self, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        custom_instructions: Optional[str] = None
    )) -> str:
        """Generate comprehensive WebVTT format prompt for AI engines."""
    def _get_base_prompt((self)) -> str:
        """Get base prompt based on selected template."""
    def _get_format_requirements((self)) -> str:
        """Get WebVTT format requirements."""
    def _get_language_instructions((self, language: str)) -> str:
        """Get language-specific instructions."""
    def _get_context_instructions((self, context: Dict[str, Any])) -> str:
        """Generate context-aware instructions from video/audio metadata."""
    def _get_webvtt_examples((self)) -> str:
        """Get properly formatted WebVTT examples."""
    def _get_diarization_instructions((self)) -> str:
        """Get speaker diarization instructions."""
    def _get_emotion_instructions((self)) -> str:
        """Get emotion detection instructions."""
    def _get_final_instructions((self)) -> str:
        """Get final formatting instructions."""
    def get_webvtt_example((self, include_speakers: bool = True, include_emotions: bool = True)) -> str:
        """Get a standalone WebVTT example for reference."""
    def create_custom_prompt((
        self,
        base_instructions: str,
        format_requirements: Optional[str] = None,
        examples: Optional[str] = None,
        additional_context: Optional[str] = None
    )) -> str:
        """Create a custom WebVTT prompt from provided components."""
    def get_supported_languages((self)) -> List[str]:
        """Get list of supported language codes for multilingual prompts."""
    def validate_prompt((self, prompt: str)) -> bool:
        """Validate that a prompt contains necessary WebVTT instructions."""
    def __repr__((self)) -> str:
        """String representation of the prompt generator."""

def __init__((
        self, 
        include_examples: bool = True,
        include_diarization: bool = True,
        include_emotions: bool = True,
        template: PromptTemplate = PromptTemplate.BASIC_WEBVTT
    )):
    """Initialize the WebVTT prompt generator."""

def generate_webvtt_prompt((
        self, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        custom_instructions: Optional[str] = None
    )) -> str:
    """Generate comprehensive WebVTT format prompt for AI engines."""

def _get_base_prompt((self)) -> str:
    """Get base prompt based on selected template."""

def _get_format_requirements((self)) -> str:
    """Get WebVTT format requirements."""

def _get_language_instructions((self, language: str)) -> str:
    """Get language-specific instructions."""

def _get_context_instructions((self, context: Dict[str, Any])) -> str:
    """Generate context-aware instructions from video/audio metadata."""

def _get_webvtt_examples((self)) -> str:
    """Get properly formatted WebVTT examples."""

def _get_diarization_instructions((self)) -> str:
    """Get speaker diarization instructions."""

def _get_emotion_instructions((self)) -> str:
    """Get emotion detection instructions."""

def _get_final_instructions((self)) -> str:
    """Get final formatting instructions."""

def get_webvtt_example((self, include_speakers: bool = True, include_emotions: bool = True)) -> str:
    """Get a standalone WebVTT example for reference."""

def create_custom_prompt((
        self,
        base_instructions: str,
        format_requirements: Optional[str] = None,
        examples: Optional[str] = None,
        additional_context: Optional[str] = None
    )) -> str:
    """Create a custom WebVTT prompt from provided components."""

def get_supported_languages((self)) -> List[str]:
    """Get list of supported language codes for multilingual prompts."""

def validate_prompt((self, prompt: str)) -> bool:
    """Validate that a prompt contains necessary WebVTT instructions."""

def __repr__((self)) -> str:
    """String representation of the prompt generator."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/core/transcriber.py
# Language: python

from pathlib import Path
from typing import Optional, Union, Dict, Any, List
import asyncio
import uuid
from loguru import logger
import logging as logger
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.config.enhanced import EnhancedVttiroConfig
from vttiro.core.transcription import TranscriptionEngine, TranscriptionEnsemble, MockTranscriptionEngine
from vttiro.processing.video import VideoProcessor, AudioChunk
from vttiro.utils import (
    VttiroError,
    ConfigurationError,
    ValidationError,
    TranscriptionError,
    ProcessingError,
    ModelError,
    OutputGenerationError,
    create_api_resilience_manager,
    create_error,
)
from vttiro.models import GeminiTranscriber, AssemblyAITranscriber, DeepgramTranscriber
from vttiro.core.config import (
            TranscriptionConfig, ProcessingConfig, DiarizationConfig,
            EmotionConfig, OutputConfig, YouTubeConfig
        )
from vttiro.core.config import TranscriptionResult
import re

class Transcriber:
    """Main transcriber class that orchestrates the entire transcription pipeline."""
    def __init__((self, config: Optional[Union[VttiroConfig, EnhancedVttiroConfig]] = None)):
        """Initialize transcriber with configuration."""
    def _initialize_engines((self)) -> None:
        """Initialize available transcription engines based on configuration and API keys."""
    def _create_legacy_config_from_enhanced((self, enhanced_config: EnhancedVttiroConfig)) -> VttiroConfig:
        """Create a legacy VttiroConfig from EnhancedVttiroConfig for backward compatibility."""
    def get_config_value((self, path: str, default=None)):
        """Get configuration value with fallback to legacy config."""
    def is_feature_enabled((self, feature: str)) -> bool:
        """Check if a feature is enabled in configuration."""
    def validate_configuration((self)) -> Dict[str, Any]:
        """Validate current configuration and return health status."""
    def log_configuration_summary((self)) -> None:
        """Log a summary of current configuration for debugging."""
    def transcribe((
        self,
        source: Union[str, Path],
        output: Optional[Union[str, Path]] = None,
        language: Optional[str] = None,
        **kwargs
    )) -> str:
        """Transcribe video or audio source to WebVTT format."""
    def _perform_transcription((
        self,
        source: Union[str, Path],
        output: Optional[Union[str, Path]],
        language: Optional[str],
        correlation_id: str,
        **kwargs
    )) -> str:
        """Perform the actual transcription operation."""
    def _generate_webvtt((self, result: TranscriptionResult)) -> str:
        """Generate WebVTT content from transcription result."""
    def _generate_comprehensive_webvtt((
        self, 
        results: List[TranscriptionResult], 
        metadata,
        include_metadata: bool = True
    )) -> str:
        """Generate comprehensive WebVTT content from multiple transcription results."""
    def _seconds_to_webvtt_time((self, seconds: float)) -> str:
        """Convert seconds to WebVTT timestamp format (HH:MM:SS.mmm)."""
    def _sanitize_filename((self, filename: str)) -> str:
        """Sanitize filename by removing invalid characters."""
    def get_available_models((self)) -> List[str]:
        """Get list of available transcription models."""
    def get_supported_languages((self)) -> List[str]:
        """Get list of supported languages across all engines."""
    def estimate_cost((self, source: Union[str, Path], duration: Optional[float] = None)) -> float:
        """Estimate transcription cost."""
    def batch_transcribe((
        self,
        sources: List[Union[str, Path]],
        output_dir: Optional[Union[str, Path]] = None,
        **kwargs
    )) -> List[str]:
        """Batch transcribe multiple sources with enhanced error handling."""

def __init__((self, config: Optional[Union[VttiroConfig, EnhancedVttiroConfig]] = None)):
    """Initialize transcriber with configuration."""

def _initialize_engines((self)) -> None:
    """Initialize available transcription engines based on configuration and API keys."""

def _create_legacy_config_from_enhanced((self, enhanced_config: EnhancedVttiroConfig)) -> VttiroConfig:
    """Create a legacy VttiroConfig from EnhancedVttiroConfig for backward compatibility."""

def get_config_value((self, path: str, default=None)):
    """Get configuration value with fallback to legacy config."""

def is_feature_enabled((self, feature: str)) -> bool:
    """Check if a feature is enabled in configuration."""

def validate_configuration((self)) -> Dict[str, Any]:
    """Validate current configuration and return health status."""

def log_configuration_summary((self)) -> None:
    """Log a summary of current configuration for debugging."""

def transcribe((
        self,
        source: Union[str, Path],
        output: Optional[Union[str, Path]] = None,
        language: Optional[str] = None,
        **kwargs
    )) -> str:
    """Transcribe video or audio source to WebVTT format."""

def _perform_transcription((
        self,
        source: Union[str, Path],
        output: Optional[Union[str, Path]],
        language: Optional[str],
        correlation_id: str,
        **kwargs
    )) -> str:
    """Perform the actual transcription operation."""

def process_video(()):

def transcribe_chunk(()):

def _generate_webvtt((self, result: TranscriptionResult)) -> str:
    """Generate WebVTT content from transcription result."""

def _generate_comprehensive_webvtt((
        self, 
        results: List[TranscriptionResult], 
        metadata,
        include_metadata: bool = True
    )) -> str:
    """Generate comprehensive WebVTT content from multiple transcription results."""

def _seconds_to_webvtt_time((self, seconds: float)) -> str:
    """Convert seconds to WebVTT timestamp format (HH:MM:SS.mmm)."""

def _sanitize_filename((self, filename: str)) -> str:
    """Sanitize filename by removing invalid characters."""

def get_available_models((self)) -> List[str]:
    """Get list of available transcription models."""

def get_supported_languages((self)) -> List[str]:
    """Get list of supported languages across all engines."""

def estimate_cost((self, source: Union[str, Path], duration: Optional[float] = None)) -> float:
    """Estimate transcription cost."""

def batch_transcribe((
        self,
        sources: List[Union[str, Path]],
        output_dir: Optional[Union[str, Path]] = None,
        **kwargs
    )) -> List[str]:
    """Batch transcribe multiple sources with enhanced error handling."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/core/transcription.py
# Language: python

from abc import ABC, abstractmethod
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Dict, Any, Union
import asyncio
from loguru import logger
import logging as logger
from vttiro.core.config import VttiroConfig, TranscriptionResult

class TranscriptionEngine(A, B, C):
    """Abstract base class for transcription engines."""
    def __init__((self, config: VttiroConfig)):

class MockTranscriptionEngine(T, r, a, n, s, c, r, i, p, t, i, o, n, E, n, g, i, n, e):
    """Mock transcription engine for testing and development."""
    def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
        """Mock transcription implementation."""
    def estimate_cost((self, duration_seconds: float)) -> float:
        """Mock cost estimation (free)."""
    def get_supported_languages((self)) -> List[str]:
        """Return mock supported languages."""

class TranscriptionEnsemble:
    """Ensemble of multiple transcription engines with intelligent routing and result fusion."""
    def __init__((self, engines: List[TranscriptionEngine], config: VttiroConfig)):
    def transcribe((
        self,
        audio_path: Path,
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
        """Transcribe using intelligent engine selection and ensemble methods."""
    def _select_optimal_engine((
        self, 
        audio_path: Path, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> TranscriptionEngine:
        """Select optimal transcription engine based on content analysis."""
    def _analyze_content((
        self, 
        audio_path: Path, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> Dict[str, Any]:
        """Analyze content characteristics for optimal engine selection."""
    def _estimate_audio_duration((self, audio_path: Path)) -> float:
        """Estimate audio duration in seconds."""
    def _score_engine_for_content((
        self, 
        engine: TranscriptionEngine, 
        content_analysis: Dict[str, Any]
    )) -> float:
        """Score an engine's suitability for the given content."""
    def estimate_cost((self, duration_seconds: float)) -> float:
        """Estimate ensemble processing cost based on likely engine selection."""

def __init__((self, config: VttiroConfig)):

def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
    """Transcribe audio file."""

def estimate_cost((self, duration_seconds: float)) -> float:
    """Estimate processing cost in USD."""

def get_supported_languages((self)) -> List[str]:
    """Return list of supported language codes."""

def name((self)) -> str:
    """Return engine name."""

def name((self)) -> str:

def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
    """Mock transcription implementation."""

def estimate_cost((self, duration_seconds: float)) -> float:
    """Mock cost estimation (free)."""

def get_supported_languages((self)) -> List[str]:
    """Return mock supported languages."""

def __init__((self, engines: List[TranscriptionEngine], config: VttiroConfig)):

def transcribe((
        self,
        audio_path: Path,
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
    """Transcribe using intelligent engine selection and ensemble methods."""

def _select_optimal_engine((
        self, 
        audio_path: Path, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> TranscriptionEngine:
    """Select optimal transcription engine based on content analysis."""

def _analyze_content((
        self, 
        audio_path: Path, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> Dict[str, Any]:
    """Analyze content characteristics for optimal engine selection."""

def _estimate_audio_duration((self, audio_path: Path)) -> float:
    """Estimate audio duration in seconds."""

def _score_engine_for_content((
        self, 
        engine: TranscriptionEngine, 
        content_analysis: Dict[str, Any]
    )) -> float:
    """Score an engine's suitability for the given content."""

def estimate_cost((self, duration_seconds: float)) -> float:
    """Estimate ensemble processing cost based on likely engine selection."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/diarization/__init__.py
# Language: python

from .core import DiarizationEngine, SpeakerSegment, DiarizationResult, DiarizationConfig
from .embeddings import SpeakerEmbeddingManager, EmbeddingExtractor
from .clustering import AdaptiveClusteringEngine, ClusteringMethod
from .quality import DiarizationQualityAssessment, QualityMetrics


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/diarization/core.py
# Language: python

from typing import List, Dict, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from pathlib import Path
import asyncio
import time
import logging
import numpy as np
from pydantic import BaseModel, Field
import torch
from pyannote.audio import Pipeline
from pyannote.core import Timeline, Annotation

class Pipeline:

class Timeline:

class Annotation:

class SpeakerSegment:
    """Represents a segment of audio attributed to a specific speaker."""
    def overlaps_with((self, other: 'SpeakerSegment')) -> bool:
        """Check if this segment overlaps with another."""

class DiarizationResult:
    """Complete diarization result with quality metrics."""
    def get_speaker_segments((self, speaker_id: str)) -> List[SpeakerSegment]:
        """Get all segments for a specific speaker."""
    def get_speakers((self)) -> List[str]:
        """Get list of unique speaker IDs."""
    def get_total_speech_duration((self)) -> float:
        """Get total duration of speech across all speakers."""

class DiarizationConfig(B, a, s, e, M, o, d, e, l):
    """Configuration for speaker diarization engine."""

class DiarizationEngine:
    """Advanced speaker diarization engine using pyannote.audio 3.1."""
    def __init__((self, config: DiarizationConfig)):
        """Initialize the diarization engine."""
    def _setup_device((self)) -> str:
        """Setup computational device based on configuration and availability."""
    def _initialize_pipeline((self)):
        """Initialize the pyannote.audio pipeline."""
    def _configure_pipeline_parameters((self)):
        """Configure pipeline parameters for optimal performance."""
    def diarize_audio((
        self, 
        audio_path: Path,
        metadata: Optional[Dict[str, Any]] = None
    )) -> DiarizationResult:
        """ Perform speaker diarization on audio file...."""
    def _run_diarization((self, audio_path: str)):
        """Run pyannote diarization pipeline."""
    def _convert_diarization_result((self, diarization)) -> List[SpeakerSegment]:
        """Convert pyannote diarization to our SpeakerSegment format."""
    def _calculate_quality_metrics((
        self, 
        segments: List[SpeakerSegment], 
        diarization
    )) -> Dict[str, float]:
        """Calculate quality metrics for the diarization result."""
    def _dummy_diarization((
        self, 
        audio_path: Path, 
        metadata: Optional[Dict[str, Any]] = None
    )) -> DiarizationResult:
        """Dummy diarization implementation for testing without pyannote."""
    def extract_speaker_embeddings((
        self, 
        audio: np.ndarray, 
        segments: List[SpeakerSegment]
    )) -> Dict[str, np.ndarray]:
        """Extract speaker embeddings for each identified speaker."""
    def validate_diarization_quality((
        self, 
        result: DiarizationResult
    )) -> Dict[str, float]:
        """Assess diarization quality and reliability."""

def from_pretrained((*args, **kwargs)):

def duration((self)) -> float:
    """Duration of the speaker segment in seconds."""

def overlaps_with((self, other: 'SpeakerSegment')) -> bool:
    """Check if this segment overlaps with another."""

def get_speaker_segments((self, speaker_id: str)) -> List[SpeakerSegment]:
    """Get all segments for a specific speaker."""

def get_speakers((self)) -> List[str]:
    """Get list of unique speaker IDs."""

def get_total_speech_duration((self)) -> float:
    """Get total duration of speech across all speakers."""

def __init__((self, config: DiarizationConfig)):
    """Initialize the diarization engine."""

def _setup_device((self)) -> str:
    """Setup computational device based on configuration and availability."""

def _initialize_pipeline((self)):
    """Initialize the pyannote.audio pipeline."""

def _configure_pipeline_parameters((self)):
    """Configure pipeline parameters for optimal performance."""

def diarize_audio((
        self, 
        audio_path: Path,
        metadata: Optional[Dict[str, Any]] = None
    )) -> DiarizationResult:
    """ Perform speaker diarization on audio file...."""

def _run_diarization((self, audio_path: str)):
    """Run pyannote diarization pipeline."""

def _convert_diarization_result((self, diarization)) -> List[SpeakerSegment]:
    """Convert pyannote diarization to our SpeakerSegment format."""

def _calculate_quality_metrics((
        self, 
        segments: List[SpeakerSegment], 
        diarization
    )) -> Dict[str, float]:
    """Calculate quality metrics for the diarization result."""

def _dummy_diarization((
        self, 
        audio_path: Path, 
        metadata: Optional[Dict[str, Any]] = None
    )) -> DiarizationResult:
    """Dummy diarization implementation for testing without pyannote."""

def extract_speaker_embeddings((
        self, 
        audio: np.ndarray, 
        segments: List[SpeakerSegment]
    )) -> Dict[str, np.ndarray]:
    """Extract speaker embeddings for each identified speaker."""

def validate_diarization_quality((
        self, 
        result: DiarizationResult
    )) -> Dict[str, float]:
    """Assess diarization quality and reliability."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/emotion/__init__.py
# Language: python



# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/integrations/__init__.py
# Language: python



# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/models/__init__.py
# Language: python

from .base import (
    TranscriptionEngine,
    GeminiModel,
    AssemblyAIModel,
    DeepgramModel,
    ModelCapability,
    get_default_model,
    get_available_models,
    validate_engine_model_combination,
    get_model_enum_class,
    get_model_capabilities,
    get_models_by_capability,
    estimate_transcription_cost
)
from .gemini import GeminiTranscriber
from .assemblyai import AssemblyAITranscriber
from .deepgram import DeepgramTranscriber


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/models/assemblyai.py
# Language: python

import asyncio
import time
from pathlib import Path
from typing import Optional, Dict, Any, List
from loguru import logger
import logging as logger
import assemblyai as aai
from vttiro.core.transcription import TranscriptionEngine
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.models.base import AssemblyAIModel

class AssemblyAITranscriber(T, r, a, n, s, c, r, i, p, t, i, o, n, E, n, g, i, n, e):
    """AssemblyAI Universal-2 transcription engine optimized for maximum accuracy."""
    def __init__((self, config: VttiroConfig, model: AssemblyAIModel = AssemblyAIModel.UNIVERSAL_2)):
    def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
        """Transcribe audio using AssemblyAI Universal-2 for maximum accuracy."""
    def _build_transcription_config((
        self, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> Dict[str, Any]:
        """Build AssemblyAI transcription configuration."""
    def _get_speech_model((self)) -> str:
        """Get the appropriate AssemblyAI speech model based on model variant."""
    def _build_custom_vocabulary((self, context: Dict[str, Any])) -> List[str]:
        """Build custom vocabulary from video context for better recognition."""
    def _transcribe_with_assemblyai((
        self, 
        audio_path: Path, 
        config: Dict[str, Any]
    )) -> Any:
        """Perform transcription using AssemblyAI API."""
    def _extract_word_timestamps((self, transcript: Any)) -> List[Dict[str, Any]]:
        """Extract word-level timestamps from AssemblyAI response."""
    def _get_enabled_features((self, config: Dict[str, Any])) -> List[str]:
        """Get list of enabled AssemblyAI features for metadata."""
    def estimate_cost((self, duration_seconds: float)) -> float:
        """Estimate AssemblyAI transcription cost in USD."""
    def get_supported_languages((self)) -> List[str]:
        """Return list of supported language codes."""

def __init__((self, config: VttiroConfig, model: AssemblyAIModel = AssemblyAIModel.UNIVERSAL_2)):

def name((self)) -> str:

def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
    """Transcribe audio using AssemblyAI Universal-2 for maximum accuracy."""

def _build_transcription_config((
        self, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> Dict[str, Any]:
    """Build AssemblyAI transcription configuration."""

def _get_speech_model((self)) -> str:
    """Get the appropriate AssemblyAI speech model based on model variant."""

def _build_custom_vocabulary((self, context: Dict[str, Any])) -> List[str]:
    """Build custom vocabulary from video context for better recognition."""

def _transcribe_with_assemblyai((
        self, 
        audio_path: Path, 
        config: Dict[str, Any]
    )) -> Any:
    """Perform transcription using AssemblyAI API."""

def _extract_word_timestamps((self, transcript: Any)) -> List[Dict[str, Any]]:
    """Extract word-level timestamps from AssemblyAI response."""

def _get_enabled_features((self, config: Dict[str, Any])) -> List[str]:
    """Get list of enabled AssemblyAI features for metadata."""

def estimate_cost((self, duration_seconds: float)) -> float:
    """Estimate AssemblyAI transcription cost in USD."""

def get_supported_languages((self)) -> List[str]:
    """Return list of supported language codes."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/models/base.py
# Language: python

from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Set

class ModelCapability:
    """Represents the capabilities and limitations of a transcription model."""
    def __post_init__((self)):
        """Initialize default collections if None."""

class TranscriptionEngine(s, t, r, ,,  , E, n, u, m):
    """AI transcription engines (providers)."""

class GeminiModel(s, t, r, ,,  , E, n, u, m):
    """Google Gemini model variants."""

class AssemblyAIModel(s, t, r, ,,  , E, n, u, m):
    """AssemblyAI model variants."""

class DeepgramModel(s, t, r, ,,  , E, n, u, m):
    """Deepgram model variants."""

def __post_init__((self)):
    """Initialize default collections if None."""

def get_default_model((engine: TranscriptionEngine)) -> str:
    """Get the default model for an engine."""

def get_available_models((engine: TranscriptionEngine)) -> List[str]:
    """Get available models for an engine."""

def validate_engine_model_combination((engine: str, model: str)) -> bool:
    """Validate that an engine and model combination is valid."""

def get_model_enum_class((engine: TranscriptionEngine)):
    """Get the model enum class for an engine."""

def get_model_capabilities((model_id: str)) -> ModelCapability:
    """Get capabilities for a specific model."""

def get_models_by_capability((requirement: str, engine: Optional[TranscriptionEngine] = None)) -> List[str]:
    """Find models that match specific capability requirements."""

def estimate_transcription_cost((model_id: str, duration_minutes: float)) -> Optional[float]:
    """Estimate the cost of transcribing with a specific model."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/models/deepgram.py
# Language: python

import asyncio
import time
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from loguru import logger
import logging as logger
from deepgram import DeepgramClient, PrerecordedOptions
from vttiro.core.transcription import TranscriptionEngine
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.models.base import DeepgramModel

class DeepgramTranscriber(T, r, a, n, s, c, r, i, p, t, i, o, n, E, n, g, i, n, e):
    """Deepgram Nova-3 transcription engine optimized for speed and multilingual support."""
    def __init__((self, config: VttiroConfig, model: DeepgramModel = DeepgramModel.NOVA_3)):
    def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
        """Transcribe audio using Deepgram Nova-3 for fast, accurate results."""
    def _build_transcription_options((
        self, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> PrerecordedOptions:
        """Build Deepgram transcription options for optimal performance."""
    def _get_deepgram_model((self)) -> str:
        """Get the appropriate Deepgram model based on model variant."""
    def _normalize_language_code((self, language: str)) -> str:
        """Normalize language code to Deepgram format."""
    def _build_keywords((self, context: Dict[str, Any])) -> List[str]:
        """Build keywords from video context for better recognition."""
    def _transcribe_with_deepgram((
        self, 
        audio_path: Path, 
        options: PrerecordedOptions
    )) -> Any:
        """Perform transcription using Deepgram API."""
    def _extract_word_timestamps((self, alternative: Any)) -> List[Dict[str, Any]]:
        """Extract word-level timestamps from Deepgram response."""
    def _detect_language((self, result: Any, specified_language: Optional[str])) -> str:
        """Detect or return language from Deepgram response."""
    def _get_enabled_features((self, options: PrerecordedOptions)) -> List[str]:
        """Get list of enabled Deepgram features for metadata."""
    def estimate_cost((self, duration_seconds: float)) -> float:
        """Estimate Deepgram transcription cost in USD."""
    def get_supported_languages((self)) -> List[str]:
        """Return list of supported language codes."""

def __init__((self, config: VttiroConfig, model: DeepgramModel = DeepgramModel.NOVA_3)):

def name((self)) -> str:

def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
    """Transcribe audio using Deepgram Nova-3 for fast, accurate results."""

def _build_transcription_options((
        self, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> PrerecordedOptions:
    """Build Deepgram transcription options for optimal performance."""

def _get_deepgram_model((self)) -> str:
    """Get the appropriate Deepgram model based on model variant."""

def _normalize_language_code((self, language: str)) -> str:
    """Normalize language code to Deepgram format."""

def _build_keywords((self, context: Dict[str, Any])) -> List[str]:
    """Build keywords from video context for better recognition."""

def _transcribe_with_deepgram((
        self, 
        audio_path: Path, 
        options: PrerecordedOptions
    )) -> Any:
    """Perform transcription using Deepgram API."""

def _extract_word_timestamps((self, alternative: Any)) -> List[Dict[str, Any]]:
    """Extract word-level timestamps from Deepgram response."""

def _detect_language((self, result: Any, specified_language: Optional[str])) -> str:
    """Detect or return language from Deepgram response."""

def _get_enabled_features((self, options: PrerecordedOptions)) -> List[str]:
    """Get list of enabled Deepgram features for metadata."""

def estimate_cost((self, duration_seconds: float)) -> float:
    """Estimate Deepgram transcription cost in USD."""

def get_supported_languages((self)) -> List[str]:
    """Return list of supported language codes."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/models/gemini.py
# Language: python

import asyncio
from pathlib import Path
from typing import Optional, Dict, Any, List
import time
from loguru import logger
import logging as logger
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from vttiro.core.transcription import TranscriptionEngine
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.models.base import GeminiModel
from vttiro.core.prompts import WebVTTPromptGenerator, PromptTemplate

class GeminiTranscriber(T, r, a, n, s, c, r, i, p, t, i, o, n, E, n, g, i, n, e):
    """Google Gemini 2.0 Flash transcription engine with factual prompting and context awareness."""
    def __init__((self, config: VttiroConfig, model: GeminiModel = GeminiModel.GEMINI_2_0_FLASH)):
    def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
        """Transcribe audio using Gemini 2.0 Flash with context-aware prompting."""
    def _upload_audio_file((self, audio_path: Path)) -> Any:
        """Upload audio file to Gemini for processing."""
    def _generate_transcription_prompt((
        self, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> str:
        """Generate WebVTT format prompt using WebVTTPromptGenerator."""
    def _transcribe_with_gemini((self, audio_file: Any, prompt: str)) -> Any:
        """Perform transcription using Gemini API."""
    def _extract_word_timestamps((self, response: Any, text: str)) -> List[Dict[str, Any]]:
        """Extract word-level timestamps from Gemini response (if available)."""
    def _estimate_confidence((self, response: Any, text: str)) -> float:
        """Estimate confidence score from Gemini response quality."""
    def estimate_cost((self, duration_seconds: float)) -> float:
        """Estimate Gemini transcription cost in USD."""
    def get_supported_languages((self)) -> List[str]:
        """Return list of supported language codes."""

def __init__((self, config: VttiroConfig, model: GeminiModel = GeminiModel.GEMINI_2_0_FLASH)):

def name((self)) -> str:

def transcribe((
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    )) -> TranscriptionResult:
    """Transcribe audio using Gemini 2.0 Flash with context-aware prompting."""

def _upload_audio_file((self, audio_path: Path)) -> Any:
    """Upload audio file to Gemini for processing."""

def _generate_transcription_prompt((
        self, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    )) -> str:
    """Generate WebVTT format prompt using WebVTTPromptGenerator."""

def _transcribe_with_gemini((self, audio_file: Any, prompt: str)) -> Any:
    """Perform transcription using Gemini API."""

def _extract_word_timestamps((self, response: Any, text: str)) -> List[Dict[str, Any]]:
    """Extract word-level timestamps from Gemini response (if available)."""

def _estimate_confidence((self, response: Any, text: str)) -> float:
    """Estimate confidence score from Gemini response quality."""

def estimate_cost((self, duration_seconds: float)) -> float:
    """Estimate Gemini transcription cost in USD."""

def get_supported_languages((self)) -> List[str]:
    """Return list of supported language codes."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/output/__init__.py
# Language: python



# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/output/simple_webvtt.py
# Language: python

from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Union
import re
from loguru import logger
import logging as logger
from vttiro.utils.exceptions import ProcessingError

class SimpleTranscriptSegment:
    """Simple transcript segment for WebVTT generation."""

class SimpleWebVTTGenerator:
    """Simple WebVTT generator for clean subtitle output."""
    def __init__((
        self,
        max_chars_per_line: int = 50,
        max_lines_per_cue: int = 2,
        max_cue_duration: float = 7.0,
        reading_speed_wpm: int = 160
    )):
        """Initialize the WebVTT generator."""
    def generate_webvtt((
        self, 
        segments: List[SimpleTranscriptSegment], 
        output_path: Union[str, Path],
        title: Optional[str] = None,
        language: Optional[str] = None
    )) -> Path:
        """Generate WebVTT file from transcript segments."""
    def _process_segments((self, segments: List[SimpleTranscriptSegment])) -> List[SimpleTranscriptSegment]:
        """Process segments for optimal readability and timing."""
    def _clean_text((self, text: str)) -> str:
        """Clean and normalize text for subtitle display."""
    def _split_long_segment((
        self, 
        segment: SimpleTranscriptSegment, 
        text: str
    )) -> List[SimpleTranscriptSegment]:
        """Split overly long segments for better readability."""
    def _split_text_into_chunks((self, words: List[str])) -> List[str]:
        """Split words into optimal chunks for subtitles."""
    def _format_text_lines((self, text: str)) -> str:
        """Format text with proper line breaks for readability."""
    def _fix_timing_overlaps((
        self, 
        segments: List[SimpleTranscriptSegment]
    )) -> List[SimpleTranscriptSegment]:
        """Fix overlapping timestamps between segments."""
    def _build_webvtt_content((
        self, 
        segments: List[SimpleTranscriptSegment],
        title: Optional[str] = None,
        language: Optional[str] = None
    )) -> str:
        """Build complete WebVTT file content."""
    def _format_timestamp((self, seconds: float)) -> str:
        """Format timestamp for WebVTT (HH:MM:SS.mmm)."""
    def estimate_reading_time((self, text: str)) -> float:
        """Estimate reading time for text based on reading speed."""
    def __repr__((self)) -> str:
        """String representation."""

def __init__((
        self,
        max_chars_per_line: int = 50,
        max_lines_per_cue: int = 2,
        max_cue_duration: float = 7.0,
        reading_speed_wpm: int = 160
    )):
    """Initialize the WebVTT generator."""

def generate_webvtt((
        self, 
        segments: List[SimpleTranscriptSegment], 
        output_path: Union[str, Path],
        title: Optional[str] = None,
        language: Optional[str] = None
    )) -> Path:
    """Generate WebVTT file from transcript segments."""

def _process_segments((self, segments: List[SimpleTranscriptSegment])) -> List[SimpleTranscriptSegment]:
    """Process segments for optimal readability and timing."""

def _clean_text((self, text: str)) -> str:
    """Clean and normalize text for subtitle display."""

def _split_long_segment((
        self, 
        segment: SimpleTranscriptSegment, 
        text: str
    )) -> List[SimpleTranscriptSegment]:
    """Split overly long segments for better readability."""

def _split_text_into_chunks((self, words: List[str])) -> List[str]:
    """Split words into optimal chunks for subtitles."""

def _format_text_lines((self, text: str)) -> str:
    """Format text with proper line breaks for readability."""

def _fix_timing_overlaps((
        self, 
        segments: List[SimpleTranscriptSegment]
    )) -> List[SimpleTranscriptSegment]:
    """Fix overlapping timestamps between segments."""

def _build_webvtt_content((
        self, 
        segments: List[SimpleTranscriptSegment],
        title: Optional[str] = None,
        language: Optional[str] = None
    )) -> str:
    """Build complete WebVTT file content."""

def _format_timestamp((self, seconds: float)) -> str:
    """Format timestamp for WebVTT (HH:MM:SS.mmm)."""

def estimate_reading_time((self, text: str)) -> float:
    """Estimate reading time for text based on reading speed."""

def __repr__((self)) -> str:
    """String representation."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/processing/__init__.py
# Language: python



# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/processing/optimized_audio.py
# Language: python

import asyncio
import numpy as np
import gc
from pathlib import Path
from typing import Iterator, List, Optional, Tuple, Union, AsyncIterator, Dict, Any
from dataclasses import dataclass
import time
import psutil
from contextlib import contextmanager
from loguru import logger
import logging as logger
import librosa
import soundfile as sf
import scipy.signal
from vttiro.utils.exceptions import ProcessingError, ResourceError

class AudioSegment:
    """Represents an audio segment with metadata."""
    def normalize((self, target_db: float = -23.0)) -> 'AudioSegment':
        """Normalize audio to target loudness."""
    def apply_bandpass_filter((self, low_freq: float = 80.0, high_freq: float = 8000.0)) -> 'AudioSegment':
        """Apply bandpass filter to improve speech recognition."""
    def reduce_noise((self, noise_gate_db: float = -40.0)) -> 'AudioSegment':
        """Apply simple noise reduction."""

class MemoryEfficientAudioLoader:
    """Memory-efficient audio loader with streaming capabilities."""
    def __init__((self, chunk_size_seconds: float = 30.0, overlap_seconds: float = 1.0)):
        """Initialize audio loader."""
    def get_audio_info((self, file_path: Union[str, Path])) -> Dict[str, Any]:
        """Get audio file information with caching."""
    def stream_audio_chunks((
        self, 
        file_path: Union[str, Path],
        target_sample_rate: Optional[int] = None
    )) -> Iterator[AudioSegment]:
        """Stream audio file in chunks for memory-efficient processing."""
    def stream_audio_chunks_async((
        self, 
        file_path: Union[str, Path],
        target_sample_rate: Optional[int] = None
    )) -> AsyncIterator[AudioSegment]:
        """Async version of stream_audio_chunks."""
    def load_audio_optimized((
        self, 
        file_path: Union[str, Path],
        target_sample_rate: int = 16000,
        max_duration: Optional[float] = None,
        apply_preprocessing: bool = True
    )) -> AudioSegment:
        """Load audio file with optimizations."""
    def apply_preprocessing((self, segment: AudioSegment)) -> AudioSegment:
        """Apply standard preprocessing to audio segment."""

class EnergyBasedSegmenter:
    """Energy-based audio segmentation for intelligent chunking."""
    def __init__((
        self,
        frame_length: int = 2048,
        hop_length: int = 512,
        energy_threshold: float = 0.01,
        min_silence_duration: float = 0.5,
        min_segment_duration: float = 5.0,
        max_segment_duration: float = 30.0
    )):
        """Initialize energy-based segmenter."""
    def calculate_energy((self, audio_data: np.ndarray, sample_rate: int)) -> np.ndarray:
        """Calculate short-time energy of audio signal."""
    def find_silence_regions((
        self, 
        energy: np.ndarray, 
        sample_rate: int
    )) -> List[Tuple[float, float]]:
        """Find silence regions in audio based on energy."""
    def segment_audio((self, segment: AudioSegment)) -> List[AudioSegment]:
        """Segment audio based on energy analysis."""
    def _split_by_duration((self, segment: AudioSegment)) -> List[AudioSegment]:
        """Split audio segment by fixed duration as fallback."""

class OptimizedAudioProcessor:
    """High-level optimized audio processor combining all optimization techniques."""
    def __init__((
        self,
        chunk_size_seconds: float = 30.0,
        target_sample_rate: int = 16000,
        max_memory_mb: float = 512.0
    )):
        """Initialize optimized audio processor."""
    def check_memory_usage((self)) -> None:
        """Check and enforce memory limits."""
    def process_audio_file((
        self, 
        file_path: Union[str, Path],
        apply_preprocessing: bool = True,
        use_energy_segmentation: bool = True
    )) -> List[AudioSegment]:
        """Process audio file with all optimizations."""
    def get_performance_stats((self)) -> Dict[str, Any]:
        """Get performance statistics."""

def duration((self)) -> float:
    """Get duration in seconds."""

def sample_count((self)) -> int:
    """Get number of samples."""

def memory_usage_mb((self)) -> float:
    """Get memory usage in MB."""

def normalize((self, target_db: float = -23.0)) -> 'AudioSegment':
    """Normalize audio to target loudness."""

def apply_bandpass_filter((self, low_freq: float = 80.0, high_freq: float = 8000.0)) -> 'AudioSegment':
    """Apply bandpass filter to improve speech recognition."""

def reduce_noise((self, noise_gate_db: float = -40.0)) -> 'AudioSegment':
    """Apply simple noise reduction."""

def __init__((self, chunk_size_seconds: float = 30.0, overlap_seconds: float = 1.0)):
    """Initialize audio loader."""

def memory_monitor((self, operation_name: str)):
    """Context manager for monitoring memory usage."""

def get_audio_info((self, file_path: Union[str, Path])) -> Dict[str, Any]:
    """Get audio file information with caching."""

def stream_audio_chunks((
        self, 
        file_path: Union[str, Path],
        target_sample_rate: Optional[int] = None
    )) -> Iterator[AudioSegment]:
    """Stream audio file in chunks for memory-efficient processing."""

def stream_audio_chunks_async((
        self, 
        file_path: Union[str, Path],
        target_sample_rate: Optional[int] = None
    )) -> AsyncIterator[AudioSegment]:
    """Async version of stream_audio_chunks."""

def sync_generator(()):

def load_audio_optimized((
        self, 
        file_path: Union[str, Path],
        target_sample_rate: int = 16000,
        max_duration: Optional[float] = None,
        apply_preprocessing: bool = True
    )) -> AudioSegment:
    """Load audio file with optimizations."""

def apply_preprocessing((self, segment: AudioSegment)) -> AudioSegment:
    """Apply standard preprocessing to audio segment."""

def __init__((
        self,
        frame_length: int = 2048,
        hop_length: int = 512,
        energy_threshold: float = 0.01,
        min_silence_duration: float = 0.5,
        min_segment_duration: float = 5.0,
        max_segment_duration: float = 30.0
    )):
    """Initialize energy-based segmenter."""

def calculate_energy((self, audio_data: np.ndarray, sample_rate: int)) -> np.ndarray:
    """Calculate short-time energy of audio signal."""

def find_silence_regions((
        self, 
        energy: np.ndarray, 
        sample_rate: int
    )) -> List[Tuple[float, float]]:
    """Find silence regions in audio based on energy."""

def segment_audio((self, segment: AudioSegment)) -> List[AudioSegment]:
    """Segment audio based on energy analysis."""

def _split_by_duration((self, segment: AudioSegment)) -> List[AudioSegment]:
    """Split audio segment by fixed duration as fallback."""

def __init__((
        self,
        chunk_size_seconds: float = 30.0,
        target_sample_rate: int = 16000,
        max_memory_mb: float = 512.0
    )):
    """Initialize optimized audio processor."""

def check_memory_usage((self)) -> None:
    """Check and enforce memory limits."""

def process_audio_file((
        self, 
        file_path: Union[str, Path],
        apply_preprocessing: bool = True,
        use_energy_segmentation: bool = True
    )) -> List[AudioSegment]:
    """Process audio file with all optimizations."""

def get_performance_stats((self)) -> Dict[str, Any]:
    """Get performance statistics."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/processing/parallel.py
# Language: python

import asyncio
import multiprocessing
import concurrent.futures
from typing import List, Any, Callable, Optional, Dict, Union
from pathlib import Path
import time
import psutil
from dataclasses import dataclass
from loguru import logger
import logging as logger
from vttiro.utils.exceptions import ProcessingError, ResourceError
import hashlib

class ProcessingTask:
    """Represents a processing task with metadata."""
    def __post_init__((self)):
        """Post-initialization validation."""

class ProcessingResult:
    """Represents the result of a processing task."""

class ResourceMonitor:
    """Monitors system resources during processing."""
    def __init__((self)):
        """Initialize resource monitor."""
    def update((self)) -> Dict[str, float]:
        """Update resource monitoring data."""
    def get_available_memory_mb((self)) -> float:
        """Get available memory in MB."""
    def get_cpu_count((self)) -> int:
        """Get number of CPU cores."""

class AdaptiveWorkerPool:
    """Adaptive worker pool that adjusts based on system resources and workload."""
    def __init__((
        self,
        min_workers: int = 1,
        max_workers: Optional[int] = None,
        memory_limit_mb: float = 2048.0,
        cpu_threshold: float = 90.0
    )):
        """Initialize adaptive worker pool."""
    def should_scale_up((self)) -> bool:
        """Determine if pool should scale up workers."""
    def should_scale_down((self)) -> bool:
        """Determine if pool should scale down workers."""
    def adjust_worker_count((self)) -> int:
        """Adjust worker count based on current conditions."""
    def estimate_task_duration((self, task: ProcessingTask)) -> float:
        """Estimate task duration based on historical data."""
    def update_performance_metrics((self, result: ProcessingResult)) -> None:
        """Update performance metrics with completed task."""

class ParallelProcessor:
    """High-performance parallel processor for audio/video operations."""
    def __init__((
        self,
        max_workers: Optional[int] = None,
        memory_limit_mb: float = 2048.0,
        use_async: bool = True,
        chunk_size: int = 1000
    )):
        """Initialize parallel processor."""
    def get_cache_key((self, func: Callable, *args, **kwargs)) -> str:
        """Generate cache key for function and arguments."""
    def get_cached_result((self, cache_key: str)) -> Optional[Any]:
        """Get cached result if available."""
    def cache_result((self, cache_key: str, result: Any)) -> None:
        """Cache a result. ..."""
    def process_batch_async((
        self,
        tasks: List[ProcessingTask],
        process_func: Callable,
        progress_callback: Optional[Callable] = None
    )) -> List[ProcessingResult]:
        """Process a batch of tasks asynchronously."""
    def process_batch_sync((
        self,
        tasks: List[ProcessingTask],
        process_func: Callable,
        progress_callback: Optional[Callable] = None
    )) -> List[ProcessingResult]:
        """Process a batch of tasks synchronously using multiprocessing."""
    def process_batch((
        self,
        tasks: List[ProcessingTask],
        process_func: Callable,
        progress_callback: Optional[Callable] = None
    )) -> List[ProcessingResult]:
        """Process a batch of tasks using optimal method."""
    def get_performance_stats((self)) -> Dict[str, Any]:
        """Get performance statistics."""

def __post_init__((self)):
    """Post-initialization validation."""

def __init__((self)):
    """Initialize resource monitor."""

def update((self)) -> Dict[str, float]:
    """Update resource monitoring data."""

def get_available_memory_mb((self)) -> float:
    """Get available memory in MB."""

def get_cpu_count((self)) -> int:
    """Get number of CPU cores."""

def __init__((
        self,
        min_workers: int = 1,
        max_workers: Optional[int] = None,
        memory_limit_mb: float = 2048.0,
        cpu_threshold: float = 90.0
    )):
    """Initialize adaptive worker pool."""

def should_scale_up((self)) -> bool:
    """Determine if pool should scale up workers."""

def should_scale_down((self)) -> bool:
    """Determine if pool should scale down workers."""

def adjust_worker_count((self)) -> int:
    """Adjust worker count based on current conditions."""

def estimate_task_duration((self, task: ProcessingTask)) -> float:
    """Estimate task duration based on historical data."""

def update_performance_metrics((self, result: ProcessingResult)) -> None:
    """Update performance metrics with completed task."""

def __init__((
        self,
        max_workers: Optional[int] = None,
        memory_limit_mb: float = 2048.0,
        use_async: bool = True,
        chunk_size: int = 1000
    )):
    """Initialize parallel processor."""

def get_cache_key((self, func: Callable, *args, **kwargs)) -> str:
    """Generate cache key for function and arguments."""

def get_cached_result((self, cache_key: str)) -> Optional[Any]:
    """Get cached result if available."""

def cache_result((self, cache_key: str, result: Any)) -> None:
    """Cache a result. ..."""

def process_batch_async((
        self,
        tasks: List[ProcessingTask],
        process_func: Callable,
        progress_callback: Optional[Callable] = None
    )) -> List[ProcessingResult]:
    """Process a batch of tasks asynchronously."""

def process_single_task((task: ProcessingTask)) -> ProcessingResult:
    """Process a single task with resource management."""

def process_batch_sync((
        self,
        tasks: List[ProcessingTask],
        process_func: Callable,
        progress_callback: Optional[Callable] = None
    )) -> List[ProcessingResult]:
    """Process a batch of tasks synchronously using multiprocessing."""

def process_single_task_sync((task: ProcessingTask)) -> ProcessingResult:
    """Process a single task synchronously."""

def process_batch((
        self,
        tasks: List[ProcessingTask],
        process_func: Callable,
        progress_callback: Optional[Callable] = None
    )) -> List[ProcessingResult]:
    """Process a batch of tasks using optimal method."""

def get_performance_stats((self)) -> Dict[str, Any]:
    """Get performance statistics."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/processing/simple_audio.py
# Language: python

import subprocess
import tempfile
from pathlib import Path
from typing import Optional, Set
import shutil
from loguru import logger
import logging as logger
from vttiro.utils.exceptions import ProcessingError, ValidationError
import json

class SimpleAudioProcessor:
    """Simple audio processor for extracting audio from video files."""
    def __init__((self)):
        """Initialize the simple audio processor."""
    def validate_file((self, file_path: Path)) -> bool:
        """Validate input file format and basic properties."""
    def is_audio_file((self, file_path: Path)) -> bool:
        """Check if file is already in audio format."""
    def extract_audio((self, input_path: Path, output_path: Optional[Path] = None)) -> Path:
        """Extract audio from video file using ffmpeg."""
    def get_audio_info((self, file_path: Path)) -> dict:
        """Get basic audio file information using ffprobe."""
    def cleanup_temp_files((self)) -> None:
        """Clean up temporary files created during processing."""
    def get_supported_formats((self)) -> list[str]:
        """Get list of supported input formats."""
    def __del__((self)):
        """Cleanup on destruction."""
    def __repr__((self)) -> str:
        """String representation."""

def __init__((self)):
    """Initialize the simple audio processor."""

def validate_file((self, file_path: Path)) -> bool:
    """Validate input file format and basic properties."""

def is_audio_file((self, file_path: Path)) -> bool:
    """Check if file is already in audio format."""

def extract_audio((self, input_path: Path, output_path: Optional[Path] = None)) -> Path:
    """Extract audio from video file using ffmpeg."""

def get_audio_info((self, file_path: Path)) -> dict:
    """Get basic audio file information using ffprobe."""

def cleanup_temp_files((self)) -> None:
    """Clean up temporary files created during processing."""

def get_supported_formats((self)) -> list[str]:
    """Get list of supported input formats."""

def __del__((self)):
    """Cleanup on destruction."""

def __repr__((self)) -> str:
    """String representation."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/processing/video.py
# Language: python

import os
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from urllib.parse import urlparse
import yt_dlp
from loguru import logger
import ffmpeg
from pydantic import BaseModel, Field
from vttiro.core.config import VttiroConfig
from vttiro.segmentation import SegmentationEngine, SegmentationConfig, AudioSegment as AdvancedAudioSegment
import librosa
import numpy as np
import numpy as np
import librosa
import numpy as np

class VideoMetadata:
    """Metadata extracted from video source."""

class AudioChunk:
    """Represents a segmented audio chunk."""

class VideoProcessor:
    """Enhanced video processing with yt-dlp integration and smart segmentation."""
    def __init__((self, config: VttiroConfig, temp_dir: Optional[Path] = None)):
        """Initialize video processor."""
    def process_source((
        self,
        source: Union[str, Path],
        extract_audio: bool = True,
        segment_audio: bool = True
    )) -> Dict[str, Any]:
        """Process video source and extract audio with metadata."""
    def extract_metadata((self, source: Union[str, Path])) -> VideoMetadata:
        """Extract comprehensive metadata from video source."""
    def extract_audio((
        self,
        source: Union[str, Path],
        metadata: Optional[VideoMetadata] = None
    )) -> Optional[Path]:
        """Extract audio from video source."""
    def segment_audio((
        self,
        audio_file: Path,
        metadata: Optional[VideoMetadata] = None,
        chunk_duration: Optional[int] = None
    )) -> List[AudioChunk]:
        """Segment audio into intelligent chunks using energy-based analysis."""
    def _extract_metadata_from_url((self, url: str)) -> VideoMetadata:
        """Extract metadata from video URL using yt-dlp."""
    def _extract_metadata_from_file((self, file_path: Path)) -> VideoMetadata:
        """Extract metadata from local video file using ffprobe."""
    def _extract_audio_from_url((self, url: str)) -> Optional[Path]:
        """Extract audio from URL using yt-dlp."""
    def _extract_audio_from_file((self, file_path: Path)) -> Optional[Path]:
        """Extract audio from local video file using ffmpeg."""
    def _compute_energy_based_segments((
        self,
        audio: 'np.ndarray',
        sr: int,
        max_chunk_duration: int
    )) -> List[tuple[float, float]]:
        """Compute segment boundaries using energy-based analysis."""
    def _extract_audio_chunk((
        self,
        audio_file: Path,
        start_time: float,
        end_time: float,
        chunk_id: str
    )) -> Optional[Path]:
        """Extract a specific time range from audio file."""
    def _calculate_energy_stats((self, audio: 'np.ndarray')) -> Dict[str, float]:
        """Calculate energy statistics for audio segment."""
    def _assess_audio_quality((self, audio_file: Path)) -> Dict[str, float]:
        """Assess audio quality metrics."""
    def _should_preprocess_audio((self, quality_metrics: Dict[str, float])) -> bool:
        """Determine if audio preprocessing is needed."""
    def _preprocess_audio((self, audio_file: Path, quality_metrics: Dict[str, float])) -> Path:
        """Apply audio preprocessing to improve quality."""
    def _simple_time_based_segmentation((
        self,
        audio_file: Path,
        max_chunk_duration: int
    )) -> List[AudioChunk]:
        """Fallback simple time-based segmentation."""
    def _is_url((self, source: str)) -> bool:
        """Check if source is a URL."""
    def cleanup_temp_files((self)) -> None:
        """Clean up temporary files."""

def __init__((self, config: VttiroConfig, temp_dir: Optional[Path] = None)):
    """Initialize video processor."""

def process_source((
        self,
        source: Union[str, Path],
        extract_audio: bool = True,
        segment_audio: bool = True
    )) -> Dict[str, Any]:
    """Process video source and extract audio with metadata."""

def extract_metadata((self, source: Union[str, Path])) -> VideoMetadata:
    """Extract comprehensive metadata from video source."""

def extract_audio((
        self,
        source: Union[str, Path],
        metadata: Optional[VideoMetadata] = None
    )) -> Optional[Path]:
    """Extract audio from video source."""

def segment_audio((
        self,
        audio_file: Path,
        metadata: Optional[VideoMetadata] = None,
        chunk_duration: Optional[int] = None
    )) -> List[AudioChunk]:
    """Segment audio into intelligent chunks using energy-based analysis."""

def _extract_metadata_from_url((self, url: str)) -> VideoMetadata:
    """Extract metadata from video URL using yt-dlp."""

def _extract_metadata_from_file((self, file_path: Path)) -> VideoMetadata:
    """Extract metadata from local video file using ffprobe."""

def _extract_audio_from_url((self, url: str)) -> Optional[Path]:
    """Extract audio from URL using yt-dlp."""

def _extract_audio_from_file((self, file_path: Path)) -> Optional[Path]:
    """Extract audio from local video file using ffmpeg."""

def _compute_energy_based_segments((
        self,
        audio: 'np.ndarray',
        sr: int,
        max_chunk_duration: int
    )) -> List[tuple[float, float]]:
    """Compute segment boundaries using energy-based analysis."""

def _extract_audio_chunk((
        self,
        audio_file: Path,
        start_time: float,
        end_time: float,
        chunk_id: str
    )) -> Optional[Path]:
    """Extract a specific time range from audio file."""

def _calculate_energy_stats((self, audio: 'np.ndarray')) -> Dict[str, float]:
    """Calculate energy statistics for audio segment."""

def _assess_audio_quality((self, audio_file: Path)) -> Dict[str, float]:
    """Assess audio quality metrics."""

def _should_preprocess_audio((self, quality_metrics: Dict[str, float])) -> bool:
    """Determine if audio preprocessing is needed."""

def _preprocess_audio((self, audio_file: Path, quality_metrics: Dict[str, float])) -> Path:
    """Apply audio preprocessing to improve quality."""

def _simple_time_based_segmentation((
        self,
        audio_file: Path,
        max_chunk_duration: int
    )) -> List[AudioChunk]:
    """Fallback simple time-based segmentation."""

def _is_url((self, source: str)) -> bool:
    """Check if source is a URL."""

def cleanup_temp_files((self)) -> None:
    """Clean up temporary files."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/segmentation/__init__.py
# Language: python

from .core import SegmentationEngine, SegmentationType, SegmentationConfig, AudioSegment
from .energy import EnergyAnalyzer, EnergyFeatures
from .boundaries import BoundaryDetector, BoundaryType


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/segmentation/boundaries.py
# Language: python

from enum import Enum
from typing import List, Dict, Tuple, Optional
import time
from loguru import logger
import logging as logger
import numpy as np
import librosa
from scipy import signal
import webrtcvad
from .core import SegmentationConfig

class BoundaryType(E, n, u, m):
    """Types of detected boundaries."""

class BoundaryDetector:
    """Advanced boundary detector using multiple detection strategies."""
    def __init__((self, config: SegmentationConfig)):
        """Initialize boundary detector."""
    def detect_linguistic_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[float]:
        """Detect linguistic boundaries (sentences, phrases, topics)."""
    def _detect_pause_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
        """Detect boundaries based on pauses in speech."""
    def _detect_prosodic_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
        """Detect boundaries based on prosodic features (pitch, rhythm)."""
    def _smooth_pitch_contour((
        self, 
        f0: np.ndarray, 
        voiced_flag: np.ndarray
    )) -> np.ndarray:
        """Smooth pitch contour for boundary detection."""
    def _detect_pitch_boundaries((
        self, 
        f0_smooth: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
        """Detect boundaries based on pitch changes."""
    def _detect_tempo_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
        """Detect boundaries based on tempo/rhythm changes."""
    def _detect_vad_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
        """Detect boundaries using Voice Activity Detection."""
    def _detect_spectral_change_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
        """Detect boundaries based on spectral characteristics changes."""
    def _combine_linguistic_boundaries((
        self, 
        boundaries: List[Tuple[float, str, float]]
    )) -> List[float]:
        """Combine and filter linguistic boundaries from different methods."""
    def _select_best_boundary((
        self, 
        boundary_group: List[Tuple[float, str, float]]
    )) -> float:
        """Select the best boundary from a group of nearby boundaries."""

def __init__((self, config: SegmentationConfig)):
    """Initialize boundary detector."""

def detect_linguistic_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[float]:
    """Detect linguistic boundaries (sentences, phrases, topics)."""

def _detect_pause_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
    """Detect boundaries based on pauses in speech."""

def _detect_prosodic_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
    """Detect boundaries based on prosodic features (pitch, rhythm)."""

def _smooth_pitch_contour((
        self, 
        f0: np.ndarray, 
        voiced_flag: np.ndarray
    )) -> np.ndarray:
    """Smooth pitch contour for boundary detection."""

def _detect_pitch_boundaries((
        self, 
        f0_smooth: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
    """Detect boundaries based on pitch changes."""

def _detect_tempo_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
    """Detect boundaries based on tempo/rhythm changes."""

def _detect_vad_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
    """Detect boundaries using Voice Activity Detection."""

def _detect_spectral_change_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[Tuple[float, str, float]]:
    """Detect boundaries based on spectral characteristics changes."""

def _combine_linguistic_boundaries((
        self, 
        boundaries: List[Tuple[float, str, float]]
    )) -> List[float]:
    """Combine and filter linguistic boundaries from different methods."""

def _select_best_boundary((
        self, 
        boundary_group: List[Tuple[float, str, float]]
    )) -> float:
    """Select the best boundary from a group of nearby boundaries."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/segmentation/core.py
# Language: python

from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Dict, Any, Union
from enum import Enum
from pathlib import Path
import time
from loguru import logger
import logging as logger
import numpy as np
import librosa
from vttiro.core.config import VttiroConfig
from .energy import EnergyAnalyzer
from .boundaries import BoundaryDetector
import webrtcvad

class SegmentationType(E, n, u, m):
    """Types of audio segmentation strategies."""

class SegmentationConfig:
    """Configuration for audio segmentation parameters."""

class AudioSegment:
    """Represents a segmented audio chunk with metadata."""
    def __post_init__((self)):
        """Generate chunk ID if not provided."""

class SegmentationEngine:
    """Advanced audio segmentation engine with intelligent boundary detection."""
    def __init__((self, config: Union[SegmentationConfig, VttiroConfig])):
        """Initialize segmentation engine."""
    def _build_segmentation_config((self, main_config: VttiroConfig)) -> SegmentationConfig:
        """Build segmentation config from main vttiro config."""
    def _initialize_components((self)):
        """Initialize segmentation components."""
    def _initialize_vad((self)):
        """Initialize Voice Activity Detection model."""
    def segment_audio((
        self, 
        audio_path: Path, 
        metadata: Optional[Dict[str, Any]] = None
    )) -> List[AudioSegment]:
        """Intelligently segment audio using multiple strategies."""
    def _analyze_content((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        metadata: Optional[Dict[str, Any]]
    )) -> Dict[str, Any]:
        """Analyze audio content characteristics for strategy selection."""
    def _select_segmentation_strategy((self, content_analysis: Dict[str, Any])) -> SegmentationType:
        """Select optimal segmentation strategy based on content analysis."""
    def _generate_segments((
        self,
        audio_data: np.ndarray,
        sr: int,
        audio_path: Path,
        strategy: SegmentationType,
        content_analysis: Dict[str, Any]
    )) -> List[AudioSegment]:
        """Generate audio segments using the selected strategy."""
    def _segment_by_energy((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
        """Segment audio using advanced energy-based analysis."""
    def _segment_by_linguistics((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
        """Segment audio using linguistic boundary detection."""
    def _segment_by_speakers((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
        """Segment audio with speaker-aware boundaries."""
    def _segment_by_content((
        self,
        audio_data: np.ndarray,
        sr: int,
        audio_path: Path,
        content_analysis: Dict[str, Any]
    )) -> List[AudioSegment]:
        """Segment audio using content-aware strategies."""
    def _segment_by_quality((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
        """Segment audio based on quality characteristics."""
    def _segment_adaptively((
        self,
        audio_data: np.ndarray,
        sr: int,
        audio_path: Path,
        content_analysis: Dict[str, Any]
    )) -> List[AudioSegment]:
        """Segment audio using adaptive strategy combining multiple approaches."""
    def _refine_with_linguistic_boundaries((
        self,
        segments: List[AudioSegment],
        linguistic_boundaries: List[float]
    )) -> List[AudioSegment]:
        """Refine segments using linguistic boundary information."""
    def _basic_energy_segmentation((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
        """Basic energy-based segmentation fallback."""
    def _validate_and_optimize_segments((
        self, 
        segments: List[AudioSegment], 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[AudioSegment]:
        """Validate and optimize segment boundaries."""
    def _fallback_segmentation((
        self, 
        audio_path: Path, 
        metadata: Optional[Dict[str, Any]]
    )) -> List[AudioSegment]:
        """Fallback segmentation when advanced processing is not available."""

def __post_init__((self)):
    """Generate chunk ID if not provided."""

def __init__((self, config: Union[SegmentationConfig, VttiroConfig])):
    """Initialize segmentation engine."""

def _build_segmentation_config((self, main_config: VttiroConfig)) -> SegmentationConfig:
    """Build segmentation config from main vttiro config."""

def _initialize_components((self)):
    """Initialize segmentation components."""

def _initialize_vad((self)):
    """Initialize Voice Activity Detection model."""

def segment_audio((
        self, 
        audio_path: Path, 
        metadata: Optional[Dict[str, Any]] = None
    )) -> List[AudioSegment]:
    """Intelligently segment audio using multiple strategies."""

def _analyze_content((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        metadata: Optional[Dict[str, Any]]
    )) -> Dict[str, Any]:
    """Analyze audio content characteristics for strategy selection."""

def _select_segmentation_strategy((self, content_analysis: Dict[str, Any])) -> SegmentationType:
    """Select optimal segmentation strategy based on content analysis."""

def _generate_segments((
        self,
        audio_data: np.ndarray,
        sr: int,
        audio_path: Path,
        strategy: SegmentationType,
        content_analysis: Dict[str, Any]
    )) -> List[AudioSegment]:
    """Generate audio segments using the selected strategy."""

def _segment_by_energy((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
    """Segment audio using advanced energy-based analysis."""

def _segment_by_linguistics((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
    """Segment audio using linguistic boundary detection."""

def _segment_by_speakers((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
    """Segment audio with speaker-aware boundaries."""

def _segment_by_content((
        self,
        audio_data: np.ndarray,
        sr: int,
        audio_path: Path,
        content_analysis: Dict[str, Any]
    )) -> List[AudioSegment]:
    """Segment audio using content-aware strategies."""

def _segment_by_quality((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
    """Segment audio based on quality characteristics."""

def _segment_adaptively((
        self,
        audio_data: np.ndarray,
        sr: int,
        audio_path: Path,
        content_analysis: Dict[str, Any]
    )) -> List[AudioSegment]:
    """Segment audio using adaptive strategy combining multiple approaches."""

def _refine_with_linguistic_boundaries((
        self,
        segments: List[AudioSegment],
        linguistic_boundaries: List[float]
    )) -> List[AudioSegment]:
    """Refine segments using linguistic boundary information."""

def _basic_energy_segmentation((
        self, 
        audio_data: np.ndarray, 
        sr: int, 
        audio_path: Path
    )) -> List[AudioSegment]:
    """Basic energy-based segmentation fallback."""

def _validate_and_optimize_segments((
        self, 
        segments: List[AudioSegment], 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[AudioSegment]:
    """Validate and optimize segment boundaries."""

def _fallback_segmentation((
        self, 
        audio_path: Path, 
        metadata: Optional[Dict[str, Any]]
    )) -> List[AudioSegment]:
    """Fallback segmentation when advanced processing is not available."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/segmentation/energy.py
# Language: python

from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
import time
from loguru import logger
import logging as logger
import numpy as np
import librosa
from scipy import signal, stats
from scipy.ndimage import gaussian_filter1d
from .core import SegmentationConfig

class EnergyFeatures:
    """Container for multi-scale energy features."""

class EnergyAnalyzer:
    """Advanced energy analyzer for intelligent boundary detection."""
    def __init__((self, config: SegmentationConfig)):
        """Initialize energy analyzer."""
    def compute_energy_features((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> EnergyFeatures:
        """Compute comprehensive energy features for boundary detection."""
    def _compute_spectral_flux((self, magnitude: np.ndarray)) -> np.ndarray:
        """Compute spectral flux (rate of change in spectrum)."""
    def _compute_energy_envelope((self, rms_energy: np.ndarray)) -> np.ndarray:
        """Compute smoothed energy envelope."""
    def _compute_energy_derivative((self, rms_energy: np.ndarray)) -> np.ndarray:
        """Compute energy derivative for detecting rapid changes."""
    def _estimate_snr((self, audio_data: np.ndarray)) -> float:
        """Estimate signal-to-noise ratio."""
    def _compute_dynamic_range((self, rms_energy: np.ndarray)) -> float:
        """Compute dynamic range of the signal."""
    def _estimate_noise_floor((self, audio_data: np.ndarray)) -> float:
        """Estimate noise floor level."""
    def detect_energy_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[float]:
        """Detect optimal boundaries using energy-based analysis."""
    def _find_energy_minima((self, features: EnergyFeatures)) -> List[Tuple[float, float]]:
        """Find local minima in energy signal as boundary candidates."""
    def _find_spectral_changes((self, features: EnergyFeatures)) -> List[Tuple[float, float]]:
        """Find boundaries based on spectral changes."""
    def _find_silence_boundaries((self, features: EnergyFeatures)) -> List[Tuple[float, float]]:
        """Find boundaries in silence regions."""
    def _combine_boundary_candidates((
        self, 
        energy_boundaries: List[Tuple[float, float]],
        spectral_boundaries: List[Tuple[float, float]],
        silence_boundaries: List[Tuple[float, float]]
    )) -> List[Tuple[float, float]]:
        """Combine boundary candidates from different methods."""
    def _merge_boundary_group((self, boundaries: List[Tuple[float, float]])) -> Tuple[float, float]:
        """Merge a group of nearby boundaries."""
    def _filter_boundaries((
        self, 
        boundaries: List[Tuple[float, float]], 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[float]:
        """Filter boundaries based on constraints and quality."""
    def assess_quality((self, audio_data: np.ndarray, sr: int)) -> Dict[str, float]:
        """Assess overall audio quality metrics."""
    def _compute_overall_quality_score((self, features: EnergyFeatures)) -> float:
        """Compute overall quality score (0-1)."""

def __init__((self, config: SegmentationConfig)):
    """Initialize energy analyzer."""

def compute_energy_features((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> EnergyFeatures:
    """Compute comprehensive energy features for boundary detection."""

def _compute_spectral_flux((self, magnitude: np.ndarray)) -> np.ndarray:
    """Compute spectral flux (rate of change in spectrum)."""

def _compute_energy_envelope((self, rms_energy: np.ndarray)) -> np.ndarray:
    """Compute smoothed energy envelope."""

def _compute_energy_derivative((self, rms_energy: np.ndarray)) -> np.ndarray:
    """Compute energy derivative for detecting rapid changes."""

def _estimate_snr((self, audio_data: np.ndarray)) -> float:
    """Estimate signal-to-noise ratio."""

def _compute_dynamic_range((self, rms_energy: np.ndarray)) -> float:
    """Compute dynamic range of the signal."""

def _estimate_noise_floor((self, audio_data: np.ndarray)) -> float:
    """Estimate noise floor level."""

def detect_energy_boundaries((
        self, 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[float]:
    """Detect optimal boundaries using energy-based analysis."""

def _find_energy_minima((self, features: EnergyFeatures)) -> List[Tuple[float, float]]:
    """Find local minima in energy signal as boundary candidates."""

def _find_spectral_changes((self, features: EnergyFeatures)) -> List[Tuple[float, float]]:
    """Find boundaries based on spectral changes."""

def _find_silence_boundaries((self, features: EnergyFeatures)) -> List[Tuple[float, float]]:
    """Find boundaries in silence regions."""

def _combine_boundary_candidates((
        self, 
        energy_boundaries: List[Tuple[float, float]],
        spectral_boundaries: List[Tuple[float, float]],
        silence_boundaries: List[Tuple[float, float]]
    )) -> List[Tuple[float, float]]:
    """Combine boundary candidates from different methods."""

def _merge_boundary_group((self, boundaries: List[Tuple[float, float]])) -> Tuple[float, float]:
    """Merge a group of nearby boundaries."""

def _filter_boundaries((
        self, 
        boundaries: List[Tuple[float, float]], 
        audio_data: np.ndarray, 
        sr: int
    )) -> List[float]:
    """Filter boundaries based on constraints and quality."""

def assess_quality((self, audio_data: np.ndarray, sr: int)) -> Dict[str, float]:
    """Assess overall audio quality metrics."""

def _compute_overall_quality_score((self, features: EnergyFeatures)) -> float:
    """Compute overall quality score (0-1)."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/utils/__init__.py
# Language: python

from .exceptions import (
    # Base exceptions
    VttiroError,
    ConfigurationError,
    ValidationError,
    SecurityError,
    
    # Transcription-related exceptions
    TranscriptionError,
    APIError,
    NetworkError,
    AuthenticationError,
    RateLimitError,
    ServiceUnavailableError,
    ModelError,
    ProcessingError,
    SegmentationError,
    DiarizationError,
    EmotionDetectionError,
    
    # Output and resource exceptions
    OutputGenerationError,
    CacheError,
    ResourceError,
    
    # Utility functions
    create_error,
    from_error_code,
    ERROR_CODE_MAP,
)


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/utils/exceptions.py
# Language: python

from typing import Optional, Dict, Any
import uuid
from datetime import datetime

class VttiroError(E, x, c, e, p, t, i, o, n):
    """Base exception class for all vttiro errors."""
    def __init__((
        self, 
        message: str, 
        error_code: Optional[str] = None,
        correlation_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None
    )):
        """Initialize VttiroError with comprehensive error information."""
    def to_dict((self)) -> Dict[str, Any]:
        """Convert error to dictionary for structured logging and serialization."""
    def __str__((self)) -> str:
        """String representation with correlation ID for easy tracking."""

class ConfigurationError(V, t, t, i, r, o, E, r, r, o, r):
    """Errors related to configuration validation and setup."""

class ValidationError(V, t, t, i, r, o, E, r, r, o, r):
    """Errors related to input validation and sanitization."""

class TranscriptionError(V, t, t, i, r, o, E, r, r, o, r):
    """Base class for all transcription-related errors."""

class APIError(T, r, a, n, s, c, r, i, p, t, i, o, n, E, r, r, o, r):
    """Base class for external API-related errors."""
    def __init__((
        self,
        message: str,
        service_name: Optional[str] = None,
        status_code: Optional[int] = None,
        response_body: Optional[str] = None,
        **kwargs
    )):
        """Initialize APIError with service-specific information."""

class NetworkError(A, P, I, E, r, r, o, r):
    """Network-related errors for external service calls."""

class AuthenticationError(A, P, I, E, r, r, o, r):
    """Authentication and authorization errors."""

class RateLimitError(A, P, I, E, r, r, o, r):
    """Rate limiting errors from external services."""
    def __init__((
        self,
        message: str,
        retry_after: Optional[int] = None,
        current_usage: Optional[int] = None,
        limit: Optional[int] = None,
        **kwargs
    )):
        """Initialize RateLimitError with rate limiting details."""

class ServiceUnavailableError(A, P, I, E, r, r, o, r):
    """Service unavailability errors."""

class ModelError(T, r, a, n, s, c, r, i, p, t, i, o, n, E, r, r, o, r):
    """AI model-related errors."""
    def __init__((
        self,
        message: str,
        model_name: Optional[str] = None,
        model_version: Optional[str] = None,
        **kwargs
    )):
        """Initialize ModelError with model-specific information."""

class ProcessingError(T, r, a, n, s, c, r, i, p, t, i, o, n, E, r, r, o, r):
    """Audio/video processing errors."""
    def __init__((
        self,
        message: str,
        file_path: Optional[str] = None,
        processing_stage: Optional[str] = None,
        **kwargs
    )):
        """Initialize ProcessingError with processing details."""

class SegmentationError(P, r, o, c, e, s, s, i, n, g, E, r, r, o, r):
    """Audio segmentation-related errors."""

class DiarizationError(T, r, a, n, s, c, r, i, p, t, i, o, n, E, r, r, o, r):
    """Speaker diarization errors."""

class EmotionDetectionError(T, r, a, n, s, c, r, i, p, t, i, o, n, E, r, r, o, r):
    """Emotion detection errors."""

class OutputGenerationError(V, t, t, i, r, o, E, r, r, o, r):
    """Output format generation errors."""
    def __init__((
        self,
        message: str,
        output_format: Optional[str] = None,
        output_path: Optional[str] = None,
        **kwargs
    )):
        """Initialize OutputGenerationError with output details."""

class SecurityError(V, t, t, i, r, o, E, r, r, o, r):
    """Security-related errors."""

class CacheError(V, t, t, i, r, o, E, r, r, o, r):
    """Caching system errors."""

class ResourceError(V, t, t, i, r, o, E, r, r, o, r):
    """Resource constraint and availability errors."""
    def __init__((
        self,
        message: str,
        resource_type: Optional[str] = None,
        current_usage: Optional[float] = None,
        limit: Optional[float] = None,
        **kwargs
    )):
        """Initialize ResourceError with resource details."""

def __init__((
        self, 
        message: str, 
        error_code: Optional[str] = None,
        correlation_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None
    )):
    """Initialize VttiroError with comprehensive error information."""

def to_dict((self)) -> Dict[str, Any]:
    """Convert error to dictionary for structured logging and serialization."""

def __str__((self)) -> str:
    """String representation with correlation ID for easy tracking."""

def __init__((
        self,
        message: str,
        service_name: Optional[str] = None,
        status_code: Optional[int] = None,
        response_body: Optional[str] = None,
        **kwargs
    )):
    """Initialize APIError with service-specific information."""

def __init__((
        self,
        message: str,
        retry_after: Optional[int] = None,
        current_usage: Optional[int] = None,
        limit: Optional[int] = None,
        **kwargs
    )):
    """Initialize RateLimitError with rate limiting details."""

def __init__((
        self,
        message: str,
        model_name: Optional[str] = None,
        model_version: Optional[str] = None,
        **kwargs
    )):
    """Initialize ModelError with model-specific information."""

def __init__((
        self,
        message: str,
        file_path: Optional[str] = None,
        processing_stage: Optional[str] = None,
        **kwargs
    )):
    """Initialize ProcessingError with processing details."""

def __init__((
        self,
        message: str,
        output_format: Optional[str] = None,
        output_path: Optional[str] = None,
        **kwargs
    )):
    """Initialize OutputGenerationError with output details."""

def __init__((
        self,
        message: str,
        resource_type: Optional[str] = None,
        current_usage: Optional[float] = None,
        limit: Optional[float] = None,
        **kwargs
    )):
    """Initialize ResourceError with resource details."""

def create_error((
    error_class: type,
    message: str,
    correlation_id: Optional[str] = None,
    **kwargs
)) -> VttiroError:
    """Create an error instance with consistent correlation ID handling."""

def from_error_code((error_code: str, message: str, **kwargs)) -> VttiroError:
    """Create error instance from error code."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/src/vttiro/vttiro.py
# Language: python

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
import logging

class Config:
    """Configuration settings for vttiro."""

def process_data((
    data: List[Any],
    config: Optional[Config] = None,
    *,
    debug: bool = False
)) -> Dict[str, Any]:
    """Process the input data according to configuration."""

def main(()) -> None:
    """Main entry point for vttiro."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/conftest.py
# Language: python

import pytest
import asyncio
import tempfile
import shutil
from pathlib import Path
from unittest.mock import MagicMock, AsyncMock
from typing import Dict, Any, Optional
from vttiro.core.config import VttiroConfig, TranscriptionResult

class MockAPIResponse:
    """Mock API response for testing network operations."""
    def __init__((self, json_data: Dict[str, Any], status_code: int = 200)):
    def json((self)):
    def raise_for_status((self)):

class PerformanceCollector:
    """Collect performance metrics during tests."""
    def __init__((self)):
    def record((self, name: str, value: float, unit: str = "seconds")):
        """Record a performance metric."""
    def get_average((self, name: str)) -> Optional[float]:
        """Get average value for a metric."""

def event_loop(()):
    """Create an instance of the default event loop for the test session."""

def temp_dir(()):
    """Create a temporary directory for test files."""

def mock_config(()):
    """Create a mock VttiroConfig for testing."""

def mock_transcription_result(()):
    """Create a mock TranscriptionResult for testing."""

def mock_video_metadata(()):
    """Create mock video metadata for testing."""

def mock_audio_chunks(()):
    """Create mock audio chunks for testing."""

def mock_video_processor(()):
    """Create a mock VideoProcessor for testing."""

def mock_transcription_engine(()):
    """Create a mock TranscriptionEngine for testing."""

def mock_transcription_ensemble(()):
    """Create a mock TranscriptionEnsemble for testing."""

def sample_audio_file((temp_dir)):
    """Create a sample audio file for testing."""

def sample_video_file((temp_dir)):
    """Create a sample video file for testing."""

def sample_webvtt_content(()):
    """Create sample WebVTT content for testing."""

def api_key_config(()):
    """Create configuration with mock API keys for testing."""

def mock_network_responses(()):
    """Create mock network responses for API testing."""

def setup_test_environment((monkeypatch, temp_dir)):
    """Set up test environment with isolated temporary directories."""

def __init__((self, json_data: Dict[str, Any], status_code: int = 200)):

def json((self)):

def raise_for_status((self)):

def mock_api_response_factory(()):
    """Factory for creating mock API responses."""

def __init__((self)):

def record((self, name: str, value: float, unit: str = "seconds")):
    """Record a performance metric."""

def get_average((self, name: str)) -> Optional[float]:
    """Get average value for a metric."""

def performance_collector(()):
    """Create a performance metrics collector."""

def generate_test_transcription_results((count: int = 5)) -> list:
    """Generate test transcription results."""

def generate_test_error_scenarios(()):
    """Generate common error scenarios for testing."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/factories.py
# Language: python

import factory
import factory.fuzzy
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List
from vttiro.core.config import VttiroConfig, TranscriptionResult

class VideoMetadataFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating video metadata objects."""

class Meta:

class TranscriptionResultFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating TranscriptionResult objects."""

class Meta:

class AudioChunkFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating AudioChunk objects."""

class Meta:

class VttiroConfigFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating VttiroConfig objects."""

class Meta:

class ErrorScenarioFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating error test scenarios."""

class Meta:

class APIResponseFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating mock API responses."""

class Meta:

class PerformanceMetricsFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating performance test metrics."""

class Meta:

class TestFileFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for creating test file information."""

class Meta:

class LargeFileFactory(T, e, s, t, F, i, l, e, F, a, c, t, o, r, y):
    """Factory for large test files."""

class SmallFileFactory(T, e, s, t, F, i, l, e, F, a, c, t, o, r, y):
    """Factory for small test files."""

class HighQualityTranscriptionFactory(T, r, a, n, s, c, r, i, p, t, i, o, n, R, e, s, u, l, t, F, a, c, t, o, r, y):
    """Factory for high-quality transcription results."""

class LowQualityTranscriptionFactory(T, r, a, n, s, c, r, i, p, t, i, o, n, R, e, s, u, l, t, F, a, c, t, o, r, y):
    """Factory for low-quality transcription results."""

class MultilingualTranscriptionFactory(T, r, a, n, s, c, r, i, p, t, i, o, n, R, e, s, u, l, t, F, a, c, t, o, r, y):
    """Factory for multilingual transcription scenarios."""

class BatchProcessingScenarioFactory(f, a, c, t, o, r, y, ., F, a, c, t, o, r, y):
    """Factory for batch processing test scenarios."""

class Meta:

def create_successful_transcription_batch((count: int = 5)) -> List[TranscriptionResult]:
    """Create a batch of successful transcription results."""

def create_mixed_quality_transcription_batch((count: int = 10)) -> List[TranscriptionResult]:
    """Create a batch with mixed quality transcription results."""

def create_error_cascade_scenario(()) -> List[Dict[str, Any]]:
    """Create a scenario with cascading errors for resilience testing."""

def create_performance_test_files(()) -> List[Dict[str, Any]]:
    """Create a variety of files for performance testing."""

def create_multilingual_test_batch(()) -> List[TranscriptionResult]:
    """Create multilingual transcription results for testing."""

def get_factory((name: str)) -> factory.Factory:
    """Get a factory by name."""

def create_test_data((factory_name: str, count: int = 1, **kwargs)) -> Any:
    """Create test data using a named factory."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/run_tests.py
# Language: python

import argparse
import subprocess
import sys
from pathlib import Path
from typing import List, Optional

class TestRunner:
    """Test runner for different test categories and configurations."""
    def __init__((self, root_dir: Optional[Path] = None)):
        """Initialize test runner."""
    def run_command((self, cmd: List[str], description: str = "")) -> int:
        """Run a command and return exit code."""
    def unit_tests((self, coverage: bool = True, parallel: bool = True)) -> int:
        """Run unit tests. ..."""
    def integration_tests((self)) -> int:
        """Run integration tests."""
    def performance_tests((self)) -> int:
        """Run performance and benchmark tests."""
    def property_tests((self, max_examples: int = 100)) -> int:
        """Run property-based tests with Hypothesis."""
    def error_handling_tests((self)) -> int:
        """Run error handling and resilience tests."""
    def slow_tests((self)) -> int:
        """Run slow tests (network, API, large file processing)."""
    def api_tests((self)) -> int:
        """Run tests requiring API keys."""
    def quick_tests((self)) -> int:
        """Run quick test suite (unit tests only, no slow tests)."""
    def full_test_suite((self)) -> int:
        """Run complete test suite with all categories."""
    def regression_tests((self)) -> int:
        """Run regression tests to ensure no performance degradation."""
    def security_tests((self)) -> int:
        """Run security-focused tests."""
    def type_check((self)) -> int:
        """Run type checking with mypy."""
    def lint_check((self)) -> int:
        """Run code quality checks with ruff."""
    def pre_commit_tests((self)) -> int:
        """Run pre-commit test suite (quick tests + linting + type check)."""
    def ci_tests((self)) -> int:
        """Run CI test suite optimized for continuous integration."""

def __init__((self, root_dir: Optional[Path] = None)):
    """Initialize test runner."""

def run_command((self, cmd: List[str], description: str = "")) -> int:
    """Run a command and return exit code."""

def unit_tests((self, coverage: bool = True, parallel: bool = True)) -> int:
    """Run unit tests. ..."""

def integration_tests((self)) -> int:
    """Run integration tests."""

def performance_tests((self)) -> int:
    """Run performance and benchmark tests."""

def property_tests((self, max_examples: int = 100)) -> int:
    """Run property-based tests with Hypothesis."""

def error_handling_tests((self)) -> int:
    """Run error handling and resilience tests."""

def slow_tests((self)) -> int:
    """Run slow tests (network, API, large file processing)."""

def api_tests((self)) -> int:
    """Run tests requiring API keys."""

def quick_tests((self)) -> int:
    """Run quick test suite (unit tests only, no slow tests)."""

def full_test_suite((self)) -> int:
    """Run complete test suite with all categories."""

def regression_tests((self)) -> int:
    """Run regression tests to ensure no performance degradation."""

def security_tests((self)) -> int:
    """Run security-focused tests."""

def type_check((self)) -> int:
    """Run type checking with mypy."""

def lint_check((self)) -> int:
    """Run code quality checks with ruff."""

def pre_commit_tests((self)) -> int:
    """Run pre-commit test suite (quick tests + linting + type check)."""

def ci_tests((self)) -> int:
    """Run CI test suite optimized for continuous integration."""

def main(()):
    """Main entry point for test runner."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_config.py
# Language: python

import pytest
import os
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.utils.exceptions import ConfigurationError, ValidationError

class TestVttiroConfig:
    """Test VttiroConfig class functionality."""
    def test_config_creation_with_defaults((self)):
        """Test configuration creation with default values."""
    def test_config_validation_success((self)):
        """Test successful configuration validation."""
    def test_config_validation_invalid_duration((self)):
        """Test configuration validation with invalid duration."""
    def test_config_validation_invalid_chunk_size((self)):
        """Test configuration validation with invalid chunk size."""
    def test_config_update_from_env((self, monkeypatch)):
        """Test configuration update from environment variables."""
    def test_config_update_from_env_invalid_values((self, monkeypatch)):
        """Test configuration update with invalid environment values."""
    def test_config_load_from_file((self, temp_dir)):
        """Test loading configuration from file."""
    def test_config_load_from_file_not_found((self)):
        """Test loading configuration from non-existent file."""
    def test_config_load_from_file_invalid_yaml((self, temp_dir)):
        """Test loading configuration from invalid YAML file."""
    def test_config_save_to_file((self, temp_dir)):
        """Test saving configuration to file."""
    def test_config_load_default_locations((self, temp_dir, monkeypatch)):
        """Test loading configuration from default locations."""
    def test_config_api_key_validation((self)):
        """Test API key validation."""
    def test_config_to_dict((self)):
        """Test configuration serialization to dictionary."""
    def test_config_from_dict((self)):
        """Test configuration creation from dictionary."""
    def test_config_merge((self)):
        """Test configuration merging."""

class TestTranscriptionResult:
    """Test TranscriptionResult data class."""
    def test_transcription_result_creation((self)):
        """Test TranscriptionResult creation with required fields."""
    def test_transcription_result_with_all_fields((self)):
        """Test TranscriptionResult creation with all fields."""
    def test_transcription_result_validation((self)):
        """Test TranscriptionResult validation."""
    def test_transcription_result_to_dict((self)):
        """Test TranscriptionResult serialization."""
    def test_transcription_result_from_dict((self)):
        """Test TranscriptionResult creation from dictionary."""
    def test_transcription_result_duration_property((self)):
        """Test TranscriptionResult duration property."""
    def test_transcription_result_word_count_property((self)):
        """Test TranscriptionResult word count property."""

class TestConfigurationIntegration:
    """Test configuration integration with other components."""
    def test_config_environment_precedence((self, monkeypatch, temp_dir)):
        """Test that environment variables take precedence over file config."""
    def test_config_with_multiple_api_keys((self)):
        """Test configuration with multiple API keys."""
    def test_config_security_key_masking((self)):
        """Test that API keys are properly masked in logs/serialization."""

def test_config_creation_with_defaults((self)):
    """Test configuration creation with default values."""

def test_config_validation_success((self)):
    """Test successful configuration validation."""

def test_config_validation_invalid_duration((self)):
    """Test configuration validation with invalid duration."""

def test_config_validation_invalid_chunk_size((self)):
    """Test configuration validation with invalid chunk size."""

def test_config_update_from_env((self, monkeypatch)):
    """Test configuration update from environment variables."""

def test_config_update_from_env_invalid_values((self, monkeypatch)):
    """Test configuration update with invalid environment values."""

def test_config_load_from_file((self, temp_dir)):
    """Test loading configuration from file."""

def test_config_load_from_file_not_found((self)):
    """Test loading configuration from non-existent file."""

def test_config_load_from_file_invalid_yaml((self, temp_dir)):
    """Test loading configuration from invalid YAML file."""

def test_config_save_to_file((self, temp_dir)):
    """Test saving configuration to file."""

def test_config_load_default_locations((self, temp_dir, monkeypatch)):
    """Test loading configuration from default locations."""

def test_config_api_key_validation((self)):
    """Test API key validation."""

def test_config_to_dict((self)):
    """Test configuration serialization to dictionary."""

def test_config_from_dict((self)):
    """Test configuration creation from dictionary."""

def test_config_merge((self)):
    """Test configuration merging."""

def test_transcription_result_creation((self)):
    """Test TranscriptionResult creation with required fields."""

def test_transcription_result_with_all_fields((self)):
    """Test TranscriptionResult creation with all fields."""

def test_transcription_result_validation((self)):
    """Test TranscriptionResult validation."""

def test_transcription_result_to_dict((self)):
    """Test TranscriptionResult serialization."""

def test_transcription_result_from_dict((self)):
    """Test TranscriptionResult creation from dictionary."""

def test_transcription_result_duration_property((self)):
    """Test TranscriptionResult duration property."""

def test_transcription_result_word_count_property((self)):
    """Test TranscriptionResult word count property."""

def test_config_environment_precedence((self, monkeypatch, temp_dir)):
    """Test that environment variables take precedence over file config."""

def test_config_with_multiple_api_keys((self)):
    """Test configuration with multiple API keys."""

def test_config_security_key_masking((self)):
    """Test that API keys are properly masked in logs/serialization."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_engine_model_selection.py
# Language: python

import pytest
from vttiro.models.base import (
    TranscriptionEngine,
    GeminiModel,
    AssemblyAIModel, 
    DeepgramModel,
    get_default_model,
    get_available_models,
    validate_engine_model_combination,
    get_model_enum_class,
    ENGINE_DEFAULT_MODELS,
    ENGINE_AVAILABLE_MODELS
)

class TestTranscriptionEngine:
    """Test TranscriptionEngine enum."""
    def test_engine_values((self)):
        """Test that all expected engines are available."""
    def test_engine_from_string((self)):
        """Test creating engines from string values."""
    def test_invalid_engine_raises_error((self)):
        """Test that invalid engine strings raise ValueError."""

class TestModelEnums:
    """Test individual model enums."""
    def test_gemini_models((self)):
        """Test GeminiModel enum values."""
    def test_assemblyai_models((self)):
        """Test AssemblyAIModel enum values."""
    def test_deepgram_models((self)):
        """Test DeepgramModel enum values."""

class TestDefaultModels:
    """Test default model functionality."""
    def test_get_default_model_for_all_engines((self)):
        """Test that each engine has a default model."""
    def test_default_models_exist_in_available((self)):
        """Test that default models are in the available models list."""
    def test_specific_default_values((self)):
        """Test specific expected default values."""

class TestAvailableModels:
    """Test available models functionality."""
    def test_get_available_models_returns_list((self)):
        """Test that get_available_models returns a list."""
    def test_available_models_are_strings((self)):
        """Test that all available models are strings."""
    def test_specific_available_models((self)):
        """Test specific models are available for each engine."""

class TestValidateEngineModel:
    """Test engine/model combination validation."""
    def test_valid_combinations((self)):
        """Test that valid engine/model combinations return True."""
    def test_invalid_engine((self)):
        """Test that invalid engines return False."""
    def test_invalid_model_for_valid_engine((self)):
        """Test that invalid models for valid engines return False."""
    def test_case_sensitivity((self)):
        """Test that validation is case sensitive."""

class TestModelEnumClass:
    """Test get_model_enum_class functionality."""
    def test_get_model_enum_class_for_all_engines((self)):
        """Test that each engine returns correct enum class."""
    def test_invalid_engine_raises_error((self)):
        """Test that invalid engine raises ValueError."""

class TestEngineModelMappings:
    """Test the underlying mappings are consistent."""
    def test_engine_default_models_consistency((self)):
        """Test that ENGINE_DEFAULT_MODELS has all engines."""
    def test_engine_available_models_consistency((self)):
        """Test that ENGINE_AVAILABLE_MODELS has all engines."""
    def test_default_models_in_available_models((self)):
        """Test that default models are in available models."""
    def test_available_models_match_enum_values((self)):
        """Test that available models match actual enum values."""

class TestEngineModelIntegration:
    """Integration tests for engine/model system."""
    def test_full_workflow_validation((self)):
        """Test a complete workflow of engine/model selection and validation."""
    def test_cross_engine_contamination((self)):
        """Test that models from one engine don't work with another."""

class TestParametrizedValidation:
    """Parametrized tests for comprehensive validation."""
    def test_all_valid_combinations((self, all_engine_model_combinations)):
        """Test all valid engine/model combinations."""

def test_engine_values((self)):
    """Test that all expected engines are available."""

def test_engine_from_string((self)):
    """Test creating engines from string values."""

def test_invalid_engine_raises_error((self)):
    """Test that invalid engine strings raise ValueError."""

def test_gemini_models((self)):
    """Test GeminiModel enum values."""

def test_assemblyai_models((self)):
    """Test AssemblyAIModel enum values."""

def test_deepgram_models((self)):
    """Test DeepgramModel enum values."""

def test_get_default_model_for_all_engines((self)):
    """Test that each engine has a default model."""

def test_default_models_exist_in_available((self)):
    """Test that default models are in the available models list."""

def test_specific_default_values((self)):
    """Test specific expected default values."""

def test_get_available_models_returns_list((self)):
    """Test that get_available_models returns a list."""

def test_available_models_are_strings((self)):
    """Test that all available models are strings."""

def test_specific_available_models((self)):
    """Test specific models are available for each engine."""

def test_valid_combinations((self)):
    """Test that valid engine/model combinations return True."""

def test_invalid_engine((self)):
    """Test that invalid engines return False."""

def test_invalid_model_for_valid_engine((self)):
    """Test that invalid models for valid engines return False."""

def test_case_sensitivity((self)):
    """Test that validation is case sensitive."""

def test_get_model_enum_class_for_all_engines((self)):
    """Test that each engine returns correct enum class."""

def test_invalid_engine_raises_error((self)):
    """Test that invalid engine raises ValueError."""

def test_engine_default_models_consistency((self)):
    """Test that ENGINE_DEFAULT_MODELS has all engines."""

def test_engine_available_models_consistency((self)):
    """Test that ENGINE_AVAILABLE_MODELS has all engines."""

def test_default_models_in_available_models((self)):
    """Test that default models are in available models."""

def test_available_models_match_enum_values((self)):
    """Test that available models match actual enum values."""

def test_full_workflow_validation((self)):
    """Test a complete workflow of engine/model selection and validation."""

def test_cross_engine_contamination((self)):
    """Test that models from one engine don't work with another."""

def all_engine_model_combinations(()):
    """Generate all valid engine/model combinations."""

def test_all_valid_combinations((self, all_engine_model_combinations)):
    """Test all valid engine/model combinations."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_enhanced_config.py
# Language: python

import os
import tempfile
import time
from pathlib import Path
from unittest.mock import patch, MagicMock
import pytest
import yaml
from src.vttiro.config.enhanced import (
    EnhancedVttiroConfig, SecureApiConfig, ProcessingConfig, 
    ValidationConfig, CachingConfig, MonitoringConfig, 
    SecretManager, ConfigValidationError, EnvironmentType
)

class TestSecretManager:
    """Test the SecretManager class."""
    def test_generate_key((self)):
        """Test key generation."""
    def test_encrypt_decrypt((self)):
        """Test encryption and decryption."""
    def test_encrypt_decrypt_with_custom_key((self)):
        """Test encryption/decryption with custom key."""
    def test_decrypt_invalid_data((self)):
        """Test decryption with invalid data."""

class TestSecureApiConfig:
    """Test the SecureApiConfig class."""
    def test_valid_config((self)):
        """Test valid API configuration."""
    def test_timeout_validation((self)):
        """Test timeout validation."""
    def test_retry_validation((self)):
        """Test retry validation."""
    def test_rate_limit_validation((self)):
        """Test rate limit validation."""

class TestProcessingConfig:
    """Test the ProcessingConfig class."""
    def test_valid_config((self)):
        """Test valid processing configuration."""
    def test_chunk_duration_validation((self)):
        """Test chunk duration validation."""
    def test_overlap_validation((self)):
        """Test overlap duration validation."""

class TestValidationConfig:
    """Test the ValidationConfig class."""
    def test_valid_config((self)):
        """Test valid validation configuration."""
    def test_file_size_validation((self)):
        """Test file size validation."""
    def test_domain_validation((self)):
        """Test domain validation."""

class TestCachingConfig:
    """Test the CachingConfig class."""
    def test_valid_config((self)):
        """Test valid caching configuration."""
    def test_cache_size_validation((self)):
        """Test cache size validation."""

class TestMonitoringConfig:
    """Test the MonitoringConfig class."""
    def test_valid_config((self)):
        """Test valid monitoring configuration."""
    def test_log_level_validation((self)):
        """Test log level validation."""
    def test_port_validation((self)):
        """Test port validation."""

class TestEnhancedVttiroConfig:
    """Test the main enhanced configuration class."""
    def test_default_config((self)):
        """Test default configuration creation."""
    def test_config_validation((self)):
        """Test configuration validation."""
    def test_environment_type_validation((self)):
        """Test environment type validation."""
    def test_save_and_load((self)):
        """Test saving and loading configuration."""
    def test_environment_specific_validation((self)):
        """Test environment-specific validation."""

class TestConfigurationIntegration:
    """Integration tests for the complete configuration system."""
    def test_full_configuration_lifecycle((self)):
        """Test complete configuration lifecycle with templates."""
    def test_secret_management_integration((self)):
        """Test secret management integration."""

def test_generate_key((self)):
    """Test key generation."""

def test_encrypt_decrypt((self)):
    """Test encryption and decryption."""

def test_encrypt_decrypt_with_custom_key((self)):
    """Test encryption/decryption with custom key."""

def test_decrypt_invalid_data((self)):
    """Test decryption with invalid data."""

def test_valid_config((self)):
    """Test valid API configuration."""

def test_timeout_validation((self)):
    """Test timeout validation."""

def test_retry_validation((self)):
    """Test retry validation."""

def test_rate_limit_validation((self)):
    """Test rate limit validation."""

def test_valid_config((self)):
    """Test valid processing configuration."""

def test_chunk_duration_validation((self)):
    """Test chunk duration validation."""

def test_overlap_validation((self)):
    """Test overlap duration validation."""

def test_valid_config((self)):
    """Test valid validation configuration."""

def test_file_size_validation((self)):
    """Test file size validation."""

def test_domain_validation((self)):
    """Test domain validation."""

def test_valid_config((self)):
    """Test valid caching configuration."""

def test_cache_size_validation((self)):
    """Test cache size validation."""

def test_valid_config((self)):
    """Test valid monitoring configuration."""

def test_log_level_validation((self)):
    """Test log level validation."""

def test_port_validation((self)):
    """Test port validation."""

def test_default_config((self)):
    """Test default configuration creation."""

def test_config_validation((self)):
    """Test configuration validation."""

def test_environment_type_validation((self)):
    """Test environment type validation."""

def test_save_and_load((self)):
    """Test saving and loading configuration."""

def test_environment_specific_validation((self)):
    """Test environment-specific validation."""

def test_full_configuration_lifecycle((self)):
    """Test complete configuration lifecycle with templates."""

def test_secret_management_integration((self)):
    """Test secret management integration."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_error_handling.py
# Language: python

import pytest
import asyncio
import uuid
from unittest.mock import AsyncMock, MagicMock
from datetime import datetime, timedelta
from vttiro.utils.exceptions import (
    VttiroError,
    ConfigurationError,
    ValidationError,
    TranscriptionError,
    APIError,
    NetworkError,
    AuthenticationError,
    RateLimitError,
    ServiceUnavailableError,
    ModelError,
    ProcessingError,
    create_error,
    from_error_code,
)

class TestExceptionHierarchy:
    """Test exception hierarchy and error creation."""
    def test_vttiro_error_creation((self)):
        """Test VttiroError base exception creation."""
    def test_error_serialization((self)):
        """Test error to_dict serialization."""
    def test_api_error_with_service_info((self)):
        """Test APIError with service-specific information."""
    def test_rate_limit_error_details((self)):
        """Test RateLimitError with rate limiting details."""
    def test_model_error_details((self)):
        """Test ModelError with model-specific information."""
    def test_processing_error_details((self)):
        """Test ProcessingError with processing details."""
    def test_create_error_function((self)):
        """Test error creation utility function."""
    def test_from_error_code_function((self)):
        """Test error creation from error code."""
    def test_invalid_error_code((self)):
        """Test error creation with invalid error code."""

def test_vttiro_error_creation((self)):
    """Test VttiroError base exception creation."""

def test_error_serialization((self)):
    """Test error to_dict serialization."""

def test_api_error_with_service_info((self)):
    """Test APIError with service-specific information."""

def test_rate_limit_error_details((self)):
    """Test RateLimitError with rate limiting details."""

def test_model_error_details((self)):
    """Test ModelError with model-specific information."""

def test_processing_error_details((self)):
    """Test ProcessingError with processing details."""

def test_create_error_function((self)):
    """Test error creation utility function."""

def test_from_error_code_function((self)):
    """Test error creation from error code."""

def test_invalid_error_code((self)):
    """Test error creation with invalid error code."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_package.py
# Language: python

import vttiro

def test_version(()):
    """Verify package exposes version."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_performance.py
# Language: python

import pytest
import time
import asyncio
import memory_profiler
from unittest.mock import AsyncMock, MagicMock, patch
from pathlib import Path
from vttiro.core.transcriber import Transcriber
from vttiro.core.config import VttiroConfig, TranscriptionResult

class TestTranscriberPerformance:
    """Performance tests for Transcriber class."""
    def test_config_loading_performance((self, temp_dir, performance_collector)):
        """Test configuration loading performance."""

def benchmark_transcriber((self, mock_config)):
    """Create transcriber for benchmarking."""

def test_transcribe_performance_small_file((self, benchmark_transcriber, performance_collector)):
    """Benchmark transcription of small audio file (30 seconds)."""

def test_transcribe_performance_medium_file((self, benchmark_transcriber, performance_collector)):
    """Benchmark transcription of medium audio file (5 minutes)."""

def test_batch_transcribe_performance((self, benchmark_transcriber, performance_collector)):
    """Benchmark batch transcription performance."""

def mock_transcribe((source, output_file, **kwargs)):

def test_memory_usage_transcription((self, benchmark_transcriber)):
    """Test memory usage during transcription."""

def test_config_loading_performance((self, temp_dir, performance_collector)):
    """Test configuration loading performance."""

def perf_helper(()):
    """Performance testing helper fixture."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_property_based.py
# Language: python

import pytest
from hypothesis import given, strategies as st, settings, assume, example
from hypothesis.stateful import RuleBasedStateMachine, Bundle, rule, initialize, invariant
import string
import tempfile
from pathlib import Path
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.utils.exceptions import VttiroError, ValidationError
from vttiro.core.transcriber import Transcriber
from vttiro.core.transcriber import Transcriber

class TestTranscriptionResultProperties:
    """Property-based tests for TranscriptionResult."""

class TestConfigurationProperties:
    """Property-based tests for configuration handling."""

class TestErrorHandlingProperties:
    """Property-based tests for error handling framework."""

class TestStatefulTranscriberBehavior(R, u, l, e, B, a, s, e, d, S, t, a, t, e, M, a, c, h, i, n, e):
    """Stateful testing for Transcriber behavior."""

class TestFileSystemProperties:
    """Property-based tests for file system operations."""

def transcription_text((draw)):
    """Generate realistic transcription text."""

def confidence_score((draw)):
    """Generate realistic confidence scores."""

def timestamp((draw)):
    """Generate realistic timestamp values."""

def language_code((draw)):
    """Generate realistic language codes."""

def transcription_result((draw)):
    """Generate TranscriptionResult instances."""

def test_transcription_result_invariants((self, result)):
    """Test invariants that should always hold for TranscriptionResult."""

def test_transcription_result_serialization_roundtrip((self, text, confidence, language, start_time)):
    """Test that serialization and deserialization preserve data."""

def test_transcription_results_sorting((self, results)):
    """Test that transcription results can be sorted by time."""

def test_transcription_result_edge_cases((self, text, confidence, language)):
    """Test edge cases for TranscriptionResult creation."""

def test_config_validation_properties((self, max_duration, chunk_duration, max_workers, memory_limit)):
    """Test configuration validation with various valid inputs."""

def test_api_key_handling_properties((self, api_key)):
    """Test API key handling with various key formats."""

def test_config_metadata_handling((self, metadata)):
    """Test configuration metadata handling with arbitrary data."""

def test_error_creation_properties((self, message, error_code)):
    """Test error creation with various messages and codes."""

def init_state((self)):
    """Initialize the state machine."""

def create_config((self)):
    """Create a configuration."""

def create_source((self, duration)):
    """Create a mock source with given duration."""

def attempt_transcription((self, config, source)):
    """Attempt to transcribe a source with given config."""

def transcription_count_non_negative((self)):
    """Transcription count should never be negative."""

def error_types_are_valid((self)):
    """All encountered errors should be known types."""

def test_filename_sanitization_properties((self, filename)):
    """Test filename sanitization with various inputs."""

def test_webvtt_generation_properties((self, text_segments)):
    """Test WebVTT generation with various text inputs."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_transcriber_integration.py
# Language: python

import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch
from pathlib import Path
from vttiro.core.transcriber import Transcriber
from vttiro.core.config import VttiroConfig
from vttiro.utils.exceptions import (
    ValidationError,
    ProcessingError,
    TranscriptionError,
    OutputGenerationError,
)

class TestTranscriberIntegration:
    """Test Transcriber class integration with error handling framework."""
    def test_transcriber_initialization_with_error_handling((self, mock_config)):
        """Test transcriber initialization includes error handling components."""
    def test_transcriber_initialization_failure((self)):
        """Test transcriber initialization failure handling."""
    def test_engine_initialization_with_all_failures((self, mock_config)):
        """Test engine initialization when all AI engines fail."""
    def test_correlation_id_propagation_in_batch((self, transcriber)):
        """Test correlation IDs are properly propagated in batch operations."""
    def test_circuit_breaker_integration((self, transcriber)):
        """Test circuit breaker integration with transcriber."""
    def test_retry_manager_integration((self, transcriber)):
        """Test retry manager integration with transcriber."""

class TestTranscriberErrorScenarios:
    """Test specific error scenarios and recovery patterns."""

def mock_config((self)):
    """Create mock configuration for testing."""

def transcriber((self, mock_config)):
    """Create Transcriber instance with mocked dependencies."""

def test_transcriber_initialization_with_error_handling((self, mock_config)):
    """Test transcriber initialization includes error handling components."""

def test_transcriber_initialization_failure((self)):
    """Test transcriber initialization failure handling."""

def test_engine_initialization_with_all_failures((self, mock_config)):
    """Test engine initialization when all AI engines fail."""

def test_transcribe_with_correlation_id((self, transcriber)):
    """Test transcribe method generates correlation IDs."""

def test_transcribe_processing_error((self, transcriber)):
    """Test transcribe method handles processing errors."""

def test_transcribe_output_generation_error((self, transcriber)):
    """Test transcribe method handles output generation errors."""

def test_batch_transcribe_validation_error((self, transcriber)):
    """Test batch transcribe validates input."""

def test_batch_transcribe_partial_failures((self, transcriber)):
    """Test batch transcribe handles partial failures gracefully."""

def mock_transcribe((source, output_file, **kwargs)):

def test_correlation_id_propagation_in_batch((self, transcriber)):
    """Test correlation IDs are properly propagated in batch operations."""

def test_circuit_breaker_integration((self, transcriber)):
    """Test circuit breaker integration with transcriber."""

def test_retry_manager_integration((self, transcriber)):
    """Test retry manager integration with transcriber."""

def test_segment_transcription_failure_graceful_degradation((self)):
    """Test graceful degradation when segment transcription fails."""

def test_api_rate_limit_handling((self)):
    """Test API rate limit error handling and retry logic."""

def test_network_timeout_recovery((self)):
    """Test network timeout recovery with circuit breaker."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/vttiro/tests/test_video_processing.py
# Language: python

import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch, mock_open
from pathlib import Path
from vttiro.processing.video import VideoProcessor, AudioChunk
from vttiro.core.config import VttiroConfig
from vttiro.utils.exceptions import ProcessingError, ValidationError

class TestVideoProcessor:
    """Test VideoProcessor class functionality."""
    def test_video_processor_initialization((self, video_processor, mock_config)):
        """Test VideoProcessor initialization."""
    def test_audio_segmentation_energy_based((self, video_processor)):
        """Test energy-based audio segmentation."""
    def test_audio_segmentation_fixed_duration((self, video_processor)):
        """Test fixed-duration audio segmentation."""
    def test_audio_metadata_extraction((self, video_processor, mock_ffmpeg)):
        """Test audio metadata extraction."""
    def test_youtube_metadata_extraction((self, video_processor, mock_youtube_dl)):
        """Test YouTube metadata extraction."""
    def test_local_file_metadata_extraction((self, video_processor, mock_ffmpeg, temp_dir)):
        """Test local file metadata extraction."""
    def test_error_handling_invalid_audio_format((self, video_processor)):
        """Test error handling for invalid audio format."""

class TestAudioChunk:
    """Test AudioChunk data class."""
    def test_audio_chunk_creation((self)):
        """Test AudioChunk creation with valid parameters."""
    def test_audio_chunk_validation((self)):
        """Test AudioChunk validation."""
    def test_audio_chunk_properties((self)):
        """Test AudioChunk computed properties."""

class TestVideoProcessingIntegration:
    """Integration tests for video processing pipeline."""

def video_processor((self, mock_config)):
    """Create VideoProcessor instance for testing."""

def mock_youtube_dl((self)):
    """Mock youtube-dl/yt-dlp functionality."""

def mock_ffmpeg((self)):
    """Mock FFmpeg functionality."""

def test_video_processor_initialization((self, video_processor, mock_config)):
    """Test VideoProcessor initialization."""

def test_process_youtube_url((self, video_processor, mock_youtube_dl, mock_ffmpeg)):
    """Test processing YouTube URL."""

def test_process_local_file((self, video_processor, mock_ffmpeg, temp_dir)):
    """Test processing local video file."""

def test_process_source_invalid_url((self, video_processor)):
    """Test processing invalid URL."""

def test_process_source_missing_file((self, video_processor)):
    """Test processing non-existent file."""

def test_audio_extraction_failure((self, video_processor, mock_youtube_dl)):
    """Test handling of audio extraction failure."""

def test_audio_segmentation_energy_based((self, video_processor)):
    """Test energy-based audio segmentation."""

def test_audio_segmentation_fixed_duration((self, video_processor)):
    """Test fixed-duration audio segmentation."""

def test_audio_metadata_extraction((self, video_processor, mock_ffmpeg)):
    """Test audio metadata extraction."""

def test_parallel_audio_processing((self, video_processor, mock_ffmpeg)):
    """Test parallel processing of audio segments."""

def mock_process_segment((segment)):
    """Mock segment processing."""

def test_youtube_metadata_extraction((self, video_processor, mock_youtube_dl)):
    """Test YouTube metadata extraction."""

def test_local_file_metadata_extraction((self, video_processor, mock_ffmpeg, temp_dir)):
    """Test local file metadata extraction."""

def test_cleanup_temp_files((self, video_processor, temp_dir)):
    """Test cleanup of temporary files."""

def test_error_handling_invalid_audio_format((self, video_processor)):
    """Test error handling for invalid audio format."""

def test_memory_efficient_processing((self, video_processor, mock_config)):
    """Test memory-efficient processing for large files."""

def test_audio_chunk_creation((self)):
    """Test AudioChunk creation with valid parameters."""

def test_audio_chunk_validation((self)):
    """Test AudioChunk validation."""

def test_audio_chunk_properties((self)):
    """Test AudioChunk computed properties."""

def test_full_video_processing_pipeline((self, mock_config, temp_dir)):
    """Test complete video processing pipeline."""

def test_large_file_processing_performance((self, mock_config)):
    """Test processing performance with large files."""


</documents>