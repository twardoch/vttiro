========================
CODE SNIPPETS
========================
TITLE: Install GenAI Processors Library
DESCRIPTION: Installs the GenAI Processors library using pip. This command is typically run in a Python environment like Google Colab to set up the necessary dependencies for using the library.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install genai-processors
```

----------------------------------------

TITLE: Connect to GenAI Live API and Process Audio Output in Python
DESCRIPTION: This example illustrates connecting to the GenAI Live API using `LiveProcessor` for real-time text and audio interaction. It configures input/output modalities, defines a `collect_audio` function to aggregate audio parts, and demonstrates processing and displaying the audio output.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
from genai_processors.core import live_model
from google.genai import types as genai_types
from IPython.display import Audio, display
import numpy as np

LIVE_MODEL_NAME = "gemini-2.0-flash-live-001"

live_processor = live_model.LiveProcessor(
    api_key=API_KEY,
    model_name=LIVE_MODEL_NAME,
    realtime_config=genai_types.LiveConnectConfig(
        # Basic configuration for real-time text and audio interaction
        output_audio_transcription={},  # Enable transcription of audio output
        realtime_input_config=genai_types.RealtimeInputConfig(
            turn_coverage=(  # Model sees all real-time input in a turn
                "TURN_INCLUDES_ALL_INPUT"
            )
        ),
        response_modalities=["AUDIO"],  # Request audio output
    ),
)


@processor.processor_function
async def collect_audio(
    content: AsyncIterable[content_api.ProcessorPart],
) -> AsyncIterable[content_api.ProcessorPart]:
  """Yields a single Part containing all the audio from `content`."""
  audio_bytes = b""
  async for part in content:
    if content_api.is_audio(part.mimetype):
      audio_bytes += part.bytes
    elif content_api.is_text(part.mimetype):
      print(part)
  # This is yielded when the input stream is closed.
  yield content_api.ProcessorPart(
      audio_bytes,
      mimetype="audio/l16;rate=24000",
  )


# We only add text here, but this can contain audio, images, etc. This would
# typically come from a camera, microphone, or other input source.
input_stream = streams.stream_content(
    [
        content_api.ProcessorPart(
            "How are you today?", substream_name="realtime"
        )
    ],
    # This is needed for this example only: we wait here to give enough time
    # for the model to generate audio before we close the stream.
    with_delay_sec=7,
)
print("\nLive Processor Output:")
p = live_processor + collect_audio
async for part in p(input_stream):
  audio_track = Audio(
      data=np.frombuffer(part.bytes, dtype=np.int16),
      rate=24000,
      autoplay=True,
  )
  display(audio_track)
```

----------------------------------------

TITLE: Install GenAI Processors Python Package
DESCRIPTION: This snippet demonstrates how to install the `genai_processors` library using pip, which is necessary to run the examples and utilize the content API.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/content_api_intro.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install genai_processors
```

----------------------------------------

TITLE: Chain Multiple Processor Sources and Processors
DESCRIPTION: This Python example demonstrates chaining multiple processor sources and regular processors using the `+` operator. It shows how `TerminalInput` can initiate a stream, which is then combined with audio input and fed into a live model, illustrating the sequential processing of `ProcessorPart` objects from various origins.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_30

LANGUAGE: python
CODE:
```
# Imagine audio_io.AudioIn and live_model.LiveModel are other processors
# that produce or process data.
p = TerminalInput('>') + audio_io.AudioIn(...) + live_model.LiveModel(...)
# Here, TerminalInput starts the stream, which is then combined with
# audio input, and finally fed into a live model.
# endless_stream() is an empty stream that never ends.
# It is often used when a source initiates the stream.
async for part in p(streams.endless_stream()):
    # Process the combined output
    pass
```

----------------------------------------

TITLE: Install Libraries and Initialize GenAI Processors
DESCRIPTION: This Python snippet installs the `genai_processors` library, imports necessary modules for content processing, streams, and Jinja templating, and defines a helper function `render_part` for displaying `ProcessorPart` objects, handling status messages and general content.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/research_example.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install genai_processors

from genai_processors import content_api
from genai_processors import processor
from genai_processors import streams
from genai_processors.core import jinja_template
from genai_processors.examples import research
from google.colab import userdata
from IPython.display import Markdown, display

ProcessorPart = processor.ProcessorPart

def render_part(part: ProcessorPart) -> None:
  if part.substream_name == "status":
    display(Markdown(f"--- \n *Status*: {part.text}"))
  else:
    try:
      display(Markdown(part.text))
    except Exception:
      display(Markdown(f" {part.text} "))
```

----------------------------------------

TITLE: Parallel Execution of PartProcessors with `//` Operator
DESCRIPTION: Demonstrates how to define `PartProcessor` functions (`append_star`, `append_hash`) and combine them for parallel execution using the `//` operator. It shows how input parts are processed concurrently and their outputs concatenated, respecting the order of processors in the expression. Includes an example of streaming content and printing the results.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
@processor.part_processor_function
async def append_star(
    part: content_api.ProcessorPart,
) -> AsyncIterable[content_api.ProcessorPart]:
  """Appends a star to the text."""
  if content_api.is_text(part.mimetype):
    yield content_api.ProcessorPart(part.text + "*")


@processor.part_processor_function
async def append_hash(
    part: content_api.ProcessorPart,
) -> AsyncIterable[content_api.ProcessorPart]:
  """Appends a hash to the text."""
  if content_api.is_text(part.mimetype):
    yield content_api.ProcessorPart(part.text + "#")


parallel_processors = append_star // append_hash // processor.PASSTHROUGH_ALWAYS

input_parts_parallel = streams.stream_content([
    "Item_1",
    "Item_2",
    content_api.ProcessorPart(b"", mimetype="audio/l16;rate=24000"),
])

print("\nParallel Part Processors Output:")
async for part in parallel_processors.to_processor()(input_parts_parallel):
  print(part)
```

----------------------------------------

TITLE: Apply Processor Asynchronously with `async for`
DESCRIPTION: Demonstrates the recommended asynchronous application of a `genai-processor` instance by iterating over it directly with `async for`. It uses `streams.stream_content` to create an input stream from a list of strings.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
from genai_processors import streams

input_parts = ["Hello", "World"]
input_stream = streams.stream_content(input_parts)

print("\nAsynchronous Output:")
async for part in simple_text_processor(input_stream):
  print(part.text)
```

----------------------------------------

TITLE: Applying a PartProcessor to a Stream using to_processor()
DESCRIPTION: Demonstrates how to convert a `PartProcessor` (like `duplicate_part`) into a full `Processor` using `to_processor()` to apply it to an `AsyncIterable` stream of `ProcessorPart` objects. It then iterates and prints the text content of the processed parts.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
p = duplicate_part.to_processor()

print("\nPart Processor Output:")
async for part in p(input_parts_duplicate):
  print(part.text)
```

----------------------------------------

TITLE: Instantiate Processor Class for Asynchronous Application
DESCRIPTION: Illustrates the necessity of instantiating a `Processor` class before applying it asynchronously. This snippet shows how to replace direct class usage with an instantiated object for proper execution.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
p = SimpleTextProcessor("[EoS]")
async for part in p(stream):
  ...
```

----------------------------------------

TITLE: Chain Processors with `status` Substream Handling
DESCRIPTION: Illustrates chaining processors when one of them generates `status` substream parts. This example shows that parts in `debug` and `status` substreams are returned immediately to the caller and are not processed by subsequent processors in the chain, demonstrating special handling for these types.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
@processor.processor_function
async def simple_text_processor_with_status(
    content: AsyncIterable[content_api.ProcessorPart],
) -> AsyncIterable[content_api.ProcessorPart]:
  """Replaces dots with '[EoS]'."""
  async for part in content:
    if content_api.is_text(part.mimetype):
      yield content_api.ProcessorPart(part.text.replace(".", "[EoS]"))
      yield processor.status(f"Simple processor done on {part.text}")
    else:
      yield part


chained_processor = simple_text_processor_with_status + another_text_processor
input_streams = streams.stream_content(["First.", "Second."])

print("\nChained Processor Output:")
async for part in chained_processor(input_streams):
  print(part)
```

----------------------------------------

TITLE: Conditional Part Processing with `PartSwitch`
DESCRIPTION: Demonstrates the use of `switch.PartSwitch` to apply different `PartProcessors` based on a condition, such as `substream_name`. It shows how to define multiple cases and a default processor, ensuring the output stream maintains the order of the input stream while processing parts concurrently.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_23

LANGUAGE: python
CODE:
```
from genai_processors import switch

input_stream = streams.stream_content([
    content_api.ProcessorPart("a1", substream_name="a"),
    content_api.ProcessorPart("b1", substream_name="b"),
    content_api.ProcessorPart("a2", substream_name="a"),
    content_api.ProcessorPart("b2", substream_name="b"),
    content_api.ProcessorPart("b3", substream_name="b"),
])

m = (
    switch.PartSwitch(content_api.get_substream_name)
    .case("a", append_star)
    .case("b", append_hash)
    .default(processor.passthrough())
)

print("\nPartSwitch Output:")
p = m.to_processor()
async for part in p(input_stream):
  print(part)
```

----------------------------------------

TITLE: Apply Processor Synchronously with `apply_sync`
DESCRIPTION: Shows how to apply a processor synchronously using the `processor.apply_sync` method. It includes `nest_asyncio.apply()` to enable running async loops in environments like Colab, ensuring compatibility for synchronous execution.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
import nest_asyncio

nest_asyncio.apply()  # Needed to run async loops in Colab

processed_parts_sync = processor.apply_sync(simple_text_processor, input_parts)

print("Synchronous Output:")
for part in processed_parts_sync:
  print(part.text)
```

----------------------------------------

TITLE: Integrate Google GenAI Models as Processors (Python)
DESCRIPTION: Shows how to initialize and use Google's Generative AI models as processors within the GenAI Processors library. It demonstrates setting up a `GenaiModel` with an API key and model name, and chaining it with other processors to create a processing pipeline for prompts.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
from genai_processors.core import genai_model
from google.genai import types as genai_types

# Initialize the GenAI model processor
# Replace 'gemini-2.0-flash' with your desired model name
genai_processor = genai_model.GenaiModel(
    api_key=API_KEY,
    model_name="gemini-2.0-flash",
    generate_content_config=genai_types.GenerateContentConfig(temperature=0.7),
)

# Chain the GenAI processor with a processor to lowercase all inputs.
genai_pipeline = another_text_processor + genai_processor

input_prompt_genai = [
    "Explain the Concept of LARGE LANGUAGE MODELS",
    "in two sentences",
]
input_stream_genai = streams.stream_content(input_prompt_genai)

print("\nGenAI Pipeline Output:")
async for part in genai_pipeline(input_stream_genai):
  print(part.text)
```

----------------------------------------

TITLE: Split and Merge AsyncIterables for Parallel Processing Pipelines
DESCRIPTION: This example demonstrates how to build complex processing graphs by splitting a single `AsyncIterable` into multiple identical streams using `streams.split` and then merging them back with `streams.merge`. It showcases a scenario where different processing branches (e.g., `append_a`, `append_b`) operate concurrently on the same initial data before their results are combined.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_27

LANGUAGE: python
CODE:
```
import asyncio
from genai_processors import content_api, processor, streams


@processor.processor_function
async def append_a(
    content: AsyncIterable[content_api.ProcessorPart],
) -> AsyncIterable[content_api.ProcessorPart]:
  async for part in content:
    yield content_api.ProcessorPart(part.text + "A")


@processor.processor_function
async def append_b(
    content: AsyncIterable[content_api.ProcessorPart],
) -> AsyncIterable[content_api.ProcessorPart]:
  async for part in content:
    yield content_api.ProcessorPart(part.text + "B")


initial_stream = streams.stream_content(
    ["Start", "Finish"],
    # We add a delay after yielding each item. This lets the "Start" items be
    # yielded first.
    with_delay_sec=0.001,
)

# Split the stream into two
stream1, stream2 = streams.split(initial_stream, n=2);

# Process each stream independently
processed_stream1 = append_a(stream1)
processed_stream2 = append_b(stream2)

# Merge the processed streams
merged_stream = streams.merge([processed_stream1, processed_stream2])

print("\nSplit and Merge Example Output:")
async for part in merged_stream:
  print(part.text)
```

----------------------------------------

TITLE: Accessing and Combining Content Types with `content_api`
DESCRIPTION: Illustrates how to create `ProcessorPart` objects for different content types (e.g., image, text) and access their content using `content_api` utilities. It shows how to create an image, convert it to bytes, and then use `content_api.as_text` to extract and combine text from a list of mixed parts.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_24

LANGUAGE: python
CODE:
```
import io
from PIL import Image

# Create a simple black image
img = Image.new("RGB", (60, 30), color="black")
img_byte_arr = io.BytesIO()
img.save(img_byte_arr, format="PNG")
img_bytes = img_byte_arr.getvalue()

image_part = content_api.ProcessorPart(img_bytes, mimetype="image/png")
text_part = content_api.ProcessorPart("Some text")

# Accessing content
print("\nContent API Examples:")
print(f"Text part text: {text_part.text}")
print(f"Image part mimetype: {image_part.mimetype}")

# Using content_api.as_text to extract text from a list of parts
all_parts = [text_part, image_part, content_api.ProcessorPart(" more text")]
print(f"Combined text from parts: {content_api.as_text(all_parts)}")
```

----------------------------------------

TITLE: Create PartProcessor with Custom Match Function Decorator in Python
DESCRIPTION: This example demonstrates how to define a `PartProcessor` using the `@processor.part_processor_function` decorator, incorporating a custom `match_fn`. The `duplicate_part` function specifically duplicates text parts, showcasing selective processing based on the `match_text` function.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
def match_text(part: content_api.ProcessorPart) -> bool:
  return content_api.is_text(part.mimetype)


@processor.part_processor_function(match_fn=match_text)
async def duplicate_part(
    part: content_api.ProcessorPart,
) -> AsyncIterable[content_api.ProcessorPart]:
  """Duplicates the input part."""
  yield part
  yield part


input_parts_duplicate = streams.stream_content(["A", "B"])
```

----------------------------------------

TITLE: Chain Processors using the `+` Operator
DESCRIPTION: Demonstrates how to chain multiple `genai-processors` together using the `+` operator. It defines a new processor `another_text_processor` that lowercases text and then chains it with `simple_text_processor` to create a combined processing pipeline.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
@processor.processor_function
async def another_text_processor(
    content: AsyncIterable[content_api.ProcessorPart],
) -> AsyncIterable[content_api.ProcessorPart]:
  """Lowercases everything."""
  async for part in content:
    if content_api.is_text(part.mimetype):
      yield content_api.ProcessorPart(part.text.lower())
    else:
      yield part


chained_processor = simple_text_processor + another_text_processor
input_streams = streams.stream_content(["First. Second."])

print("\nChained Processor Output:")
async for part in chained_processor(input_streams):
  print(part.text)
```

----------------------------------------

TITLE: Chaining Filtered PartProcessors for Specific Types
DESCRIPTION: Shows a typical usage pattern for combining `PartProcessors` that pre-process input based on content type. It uses `processor.create_filter` to apply specific processors (e.g., `image_processor`, `audio_processor`) only to matching content types, then combines them with `//` and `PASSTHROUGH_FALLBACK` for comprehensive handling.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
p1 = processor.create_filter(content_api.is_image) + image_processor
p2 = processor.create_filter(content_api.is_audio) + audio_processor
total_processor = p1 // p2 // processor.PASSTHROUGH_FALLBACK
```

----------------------------------------

TITLE: Install GenAI Processors Library with Pip
DESCRIPTION: This Python snippet installs the `genai-processors` library using pip, a crucial first step for developing with GenAI model processors. It's designed for environments like Colab or Jupyter notebooks, ensuring all necessary dependencies are available.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/create_your_own_processor.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install genai-processors
```

----------------------------------------

TITLE: Install GenAI Processors Python Library
DESCRIPTION: This snippet shows the command to install the `genai-processors` library using pip. This library is fundamental for interacting with Google's generative AI models and building real-time agents, and it should be executed in a Python environment like Google Colab.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/live_processor_intro.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install genai-processors
```

----------------------------------------

TITLE: Correct Usage: Converting PartProcessor to Processor for AsyncIterable Input
DESCRIPTION: Demonstrates the correct way to handle `AsyncIterable` inputs with a `PartProcessor`. By first converting the `PartProcessor` to a full `Processor` using `to_processor()`, it can then correctly accept and process an `AsyncIterable` stream of `ProcessorPart` objects.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
# p is a now a processor.
p = part_processor.to_processor()

async def my_processor(
    content: AsyncIterable[ProcessorPart]
) -> AsyncIterable[ProcessorPart]:
  # This is ok, `p` accepts AsyncIterables as input.
  async for part in p(content):
    ...
```

----------------------------------------

TITLE: Installation Command
DESCRIPTION: Installs the genai-processors library using pip. Requires Python 3.10+.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/README.md#_snippet_2

LANGUAGE: Bash
CODE:
```
pip install genai-processors
```

----------------------------------------

TITLE: Compare GenAI Processor vs. PartProcessor Performance (Python)
DESCRIPTION: This example demonstrates the performance difference between `Processor` and `PartProcessor` for an uppercase transformation. It uses `processor_function` and `part_processor_function` decorators, and measures execution time with `asyncio` and `timeit`.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/create_your_own_processor.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
from typing import AsyncIterable
from genai_processors import content_api
from genai_processors import processor
from genai_processors import streams
import nest_asyncio

nest_asyncio.apply()  # Needed to run async loops in Colab


@processor.processor_function
async def upper_case_processor(
    content: AsyncIterable[content_api.ProcessorPart],
) -> AsyncIterable[content_api.ProcessorPartTypes]:
  async for part in content:
    if content_api.is_text(part.mimetype):
      yield part.text.upper()
    else:
      yield part
    # Sleep a bit to simulate more compute intensive task
    await asyncio.sleep(0.001)


@processor.part_processor_function
async def upper_case_part_processor(
    part: content_api.ProcessorPart,
) -> AsyncIterable[content_api.ProcessorPartTypes]:
  # The code below is the same block as the `async for` block in the function
  # above.
  if content_api.is_text(part.mimetype):
    yield part.text.upper()
  else:
    yield part
  # Sleep a bit to simulate more compute intensive task
  await asyncio.sleep(0.001)


async def load_test(processor: processor.Processor):
  input_stream = streams.stream_content(["hello"] * 1000)
  async for _ in processor(input_stream):
    pass


print("time with Processor:")
%timeit asyncio.run(load_test(upper_case_processor))
print("time with PartProcessor:")
%timeit asyncio.run(load_test(upper_case_part_processor.to_processor()))
```

----------------------------------------

TITLE: Create ProcessorContent from Strings, Parts, and Lists (Python)
DESCRIPTION: Demonstrates the flexibility of `ProcessorContent` creation. It shows how to initialize `ProcessorContent` directly from individual strings and `ProcessorPart` objects, from a list of `ProcessorPart` instances, and by copying an existing `ProcessorContent`. The example also illustrates iterating over `ProcessorContent` and accessing its components.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/content_api_intro.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
# From individual strings/parts
content1 = ProcessorContent(
    "This is the first part.",
    ProcessorPart(generate_gdm_logo_bytes(), mimetype="image/png", role="user"),
    "And a final textual comment.",
)

print("Content 1:")
for part in content1:  # You can iterate directly over ProcessorContent
  print(
      f"  - {part.mimetype}:"
      f" {part.text if is_text(part.mimetype) else '[binary data]'}"
  )
print(f"Length of Content 1: {len(content1)}")

# From a list of ProcessorPart objects
parts_list = [
    ProcessorPart("Query about cats.", role="user"),
    ProcessorPart("Cats are fascinating creatures!", role="model"),
]
content2 = ProcessorContent(parts_list)

print("\nContent 2:")
for (
    mime,
    part_obj,
) in content2.items():  # .items() yields (mimetype, ProcessorPart)
  print(f"  - Role: {part_obj.role}, Mimetype: {mime}, Text: {part_obj.text}")

# From another ProcessorContent object (creates a new collection)
content3 = ProcessorContent(content1)
print(f"\nContent 3 (copy of Content 1):")
print(f"Is Content 1 same object as Content 3? {content1 is content3}")
print(f"Is Content 1 equal to Content 3? {content1 == content3}")
```

----------------------------------------

TITLE: Install GenAI Processors Python Library
DESCRIPTION: This command installs the `genai-processors` library using pip, the standard Python package installer. It ensures all necessary dependencies are downloaded and configured, making the library ready for use in Python projects.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/README.pypi.md#_snippet_0

LANGUAGE: sh
CODE:
```
pip install genai-processors
```

----------------------------------------

TITLE: Route Input Parts with `switch.Switch` Processor (Python)
DESCRIPTION: Illustrates using `switch.Switch` to route input parts to specific processors based on conditions. This enables exclusive processing, where parts are directed to a single matching processor. It supports `case` statements for conditional routing and a `default` option for unmatched parts.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
from genai_processors import switch

input_stream = streams.stream_content([
    content_api.ProcessorPart("a1", substream_name="a"),
    content_api.ProcessorPart("b1", substream_name="b"),
    content_api.ProcessorPart("a2", substream_name="a"),
    content_api.ProcessorPart("b2", substream_name="b"),
    content_api.ProcessorPart("b3", substream_name="b"),
])

m = (
    switch.Switch(content_api.get_substream_name)
    .case("a", another_text_processor)
    .case("b", simple_text_processor)
    .default(processor.passthrough())
)

print("\nSwitch Processor Output:")
async for part in m(input_stream):
  print(part)
```

----------------------------------------

TITLE: Chain GenAI Processor with Text Processor for Pipeline in Python
DESCRIPTION: This snippet demonstrates chaining a GenAI processor with another text processor to form a pipeline. It shows how to stream input prompts, process them, and print the output, including debug information like Time To First Token (TTFT).

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
genai_pipeline = (
    another_text_processor
    # Add a tag "GenAI Model" to which processor the TTFT applies to
    + debug.TTFTSingleStream("GenAI Model", genai_processor)
)

input_prompt_genai = [
    "Explain the Concept of LARGE LANGUAGE MODELS",
    "in two sentences",
]
input_stream_genai = streams.stream_content(input_prompt_genai)

print("\nGenAI Pipeline Output:")
async for part in genai_pipeline(input_stream_genai):
  print(part.text)
```

----------------------------------------

TITLE: Commentator State Machine: Core Transitions and Logic
DESCRIPTION: Describes the various state transitions handled by the `CommentatorStateMachine`'s `update` method, illustrating how the `LiveCommentator` system dynamically responds to internal and external events to manage commentary flow.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/examples/live/README.md#_snippet_3

LANGUAGE: APIDOC
CODE:
```
CommentatorStateMachine.update() - State Transition Logic:

1. Starting Commentary (OFF -> TALKING)
   - Trigger: Event detection (people in front of camera).
   - Action: Send "start commentating" request to model.
   - Model triggers `start_commentating` function.
   - `LiveCommentator` receives function call.
   - State Change: OFF -> TALKING.
   - Post-transition Action: Request commentary generation (REQUEST_FROM_COMMENTATOR) from model.

2. User Interrupts (TALKING -> USER_IS_TALKING -> REQUESTING_RESPONSE)
   - Trigger: Gemini Live API's VAD detects user speaking.
   - Action: `LiveCommentator` receives `interrupted` signal.
   - State Change: TALKING -> USER_IS_TALKING.
   - Action: Stop current audio output.
   - Delay: Wait for user utterance to finish.
   - State Change: USER_IS_TALKING -> REQUESTING_RESPONSE.
   - Post-transition Action: Request response to user from model.

3. Event Interrupts (TALKING -> REQUESTING_INTERRUPTION -> INTERRUPTED_FROM_DETECTION -> TALKING)
   - Trigger: `EventDetection` processor detects significant event.
   - Action: `LiveCommentator` receives `interrupt_request` signal.
   - State Change: TALKING -> REQUESTING_INTERRUPTION.
   - Action: Request event comment from model (REQUEST_INTERRUPT). Model's response confirms interruption.
   - State Change: REQUESTING_INTERRUPTION -> INTERRUPTED_FROM_DETECTION (upon model confirmation).
   - Action: `LiveCommentator` yields `interrupted` response, stops previous audio, starts playing new audio.
   - State Change: INTERRUPTED_FROM_DETECTION -> TALKING.

4. Scheduled Comments (TALKING -> REQUESTING_COMMENT -> TALKING)
   - Trigger: `LiveCommentator` tracks current commentary duration.
   - Action: New comment scheduled based on estimated TTFT to play just after current one. State remains TALKING.
   - Trigger: Scheduled time arrives.
   - State Change: TALKING -> REQUESTING_COMMENT.
   - Action: Request next comment from model (REQUEST_FROM_COMMENTATOR).
   - Trigger: First audio part of new comment received.
   - State Change: REQUESTING_COMMENT -> TALKING.

5. Waiting for User (TALKING -> WAITING_FOR_USER)
   - Trigger: `LiveCommentator` receives `wait_for_user` async tool call from model.
   - State Change: TALKING -> WAITING_FOR_USER.
   - Action: Current audio plays to end; timer starts.
   - Conditional Logic:
     - If user doesn't interrupt within `MAX_SILENCE_WAIT_FOR_USER_SEC`: New comment scheduled.
     - If user interrupts: State transitions to USER_IS_TALKING (logic from step 2 applies).
     - If event detected: State transitions as in step 3.

6. Stopping Commentary (* -> OFF)
   - Trigger: `EventDetection` processor detects no one in front of camera.
   - Action: Send "stop commentating" request to model.
   - Action: `LiveCommentator` receives tool call cancellation from model.
   - State Change: Any state (*) -> OFF.
   - Post-transition Action: Cancel any scheduled comments.
```

----------------------------------------

TITLE: Define a Processor Source with Terminal Input
DESCRIPTION: This Python snippet defines an asynchronous generator function `TerminalInput` decorated with `@processor.source`. It continuously reads input from the terminal, yielding each line as a `ProcessorPart` until 'q' is entered, demonstrating how to create a custom data source for a processing pipeline.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_29

LANGUAGE: python
CODE:
```
@processor.source()
async def TerminalInput(
    prompt: str,
) -> AsyncIterable[content_api.ProcessorPartTypes]:
  while True:
    input_text = await asyncio.to_thread(input, prompt)
    if input_text == 'q':
      break
    yield input_text


async for part in TerminalInput('>'):
  print(part)
```

----------------------------------------

TITLE: Create ProcessorPart from Raw Bytes in Python
DESCRIPTION: Illustrates the creation of a `ProcessorPart` instance from raw byte data, emphasizing the requirement to explicitly specify the `mimetype`. The example also shows how to verify the presence of bytes and attempt to convert the content into a PIL Image.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/content_api_intro.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
gdm_png_bytes = generate_gdm_logo_bytes()

image_bytes_part = ProcessorPart(gdm_png_bytes, mimetype="image/png")

print(f"ProcessorPart (from bytes): {image_bytes_part}")
print(f"MIME type: {image_bytes_part.mimetype}")
print(f"Has bytes: {image_bytes_part.bytes is not None}")

# You can access it as a PIL Image too
try:
  pil_img = image_bytes_part.pil_image
  display(pil_img)
except Exception as e:
  print(f"Error converting to PIL Image: {e}")
```

----------------------------------------

TITLE: Create Looping Stream Pipelines with Merge and Asyncio Queues
DESCRIPTION: This advanced pattern illustrates how to create dynamic stream loops by merging an initial stream with content dequeued from an `asyncio.Queue`. This allows for re-injecting processed data or new parts back into the stream, enabling complex real-time agent behaviors where processor output can influence subsequent inputs. The `stop_on_first` parameter is crucial for controlling loop termination.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_28

LANGUAGE: python
CODE:
```
input_stream = streams.stream_content(
    [content_api.ProcessorPart("Hello"), content_api.ProcessorPart("World")],
    # Adds a 0.1 second delay after streaming each part. This is needed in this
    # example to insert the content of the input_queue into the stream before it
    # is closed.
    with_delay_sec=0.1,
)
input_queue = asyncio.Queue()
stream_loop = streams.merge(
    [input_stream, streams.dequeue(input_queue)],
    stop_on_first=True,
)


async def inject_new_part():
  async for part in append_a(stream_loop):
    print(part.text)
    # Wait for 0.09 seconds to inject a new part before the next part is
    # streamed.
    await asyncio.sleep(0.09)
    # Inject a "new_part" Part in the stream_loop.
    await input_queue.put(content_api.ProcessorPart("new_part"))


# This will output: HelloA, new_partA, WorldA
asyncio.run(inject_new_part())
```

----------------------------------------

TITLE: Run GenAI Processors in Parallel with `parallel_concat` (Python)
DESCRIPTION: Demonstrates concurrent execution of GenAI processors using `processor.parallel_concat()`. This function takes a sequence of processors and concatenates their outputs, following the order of the processor list. It's suitable for combining outputs from multiple processors sequentially.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
input_stream = streams.stream_content(["First.", "Second."])

p = [another_text_processor, simple_text_processor_with_status]

p = processor.parallel_concat(p)

print("\nParallel Processor Output:")
async for part in p(input_stream):
  print(part)
```

----------------------------------------

TITLE: Combine GenAI Processors for Chained Operations
DESCRIPTION: This example demonstrates how to chain multiple `genai_processors` together using the `+` operator. It defines a custom `UpperGenAI` processor that first adds a preamble, then sends the content to a Google GenAI model, and finally converts the model's output to uppercase. The `call` method orchestrates this sequential processing, showcasing how different processing steps can be combined into a single, cohesive workflow.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/create_your_own_processor.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
from genai_processors.core import genai_model
from genai_processors.core import preamble
from google.colab import userdata
from google.genai import types as genai_types


class UpperGenAI(processor.Processor):

  def __init__(self):
    self._preamble = preamble.Preamble(content=['what is the definition of: '])
    self._model = genai_model.GenaiModel(
        # Use your API KEY here
        api_key=userdata.get('GOOGLE_API_KEY'),
        model_name='gemini-2.0-flash',
        generate_content_config=genai_types.GenerateContentConfig(
            temperature=0.7
        ),
    )
    self._post_processing = upper_case_processor

  async def call(
      self, content: AsyncIterable[content_api.ProcessorPart]
  ) -> AsyncIterable[content_api.ProcessorPartTypes]:
    p = self._preamble + self._model + self._post_processing
    async for part in p(content):
      yield part


input_stream = streams.stream_content(['processor'])
async for part in UpperGenAI()(input_stream):
  print(part.text)
```

----------------------------------------

TITLE: Use a Processor Source as an Input Stream
DESCRIPTION: This Python snippet illustrates how a processor source, such as `TerminalInput`, can directly serve as an input stream for another processor like `live_model.LiveModel`. It highlights the flexibility of sources acting as `AsyncIterable` objects, allowing them to feed data directly into subsequent processing stages.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_31

LANGUAGE: python
CODE:
```
# Here, TerminalInput generates parts, and live_model processes them.
async for part in live_model.LiveModel(...)(TerminalInput('>')):
    # Process parts from the live model
    pass
```

----------------------------------------

TITLE: Define Match Function Signature for PartProcessor in Python
DESCRIPTION: This snippet provides the signature for a `match` function, which is used with `PartProcessor` to determine if a `ProcessorPart` is relevant. It explains that returning `False` optimizes scheduling by preventing irrelevant parts from being processed.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
def match(part: content_api.ProcessorPart) -> bool:
  """Returns False if `part` is irrelevant for the processor, True otherwise."""
  ...
```

----------------------------------------

TITLE: Implement Real-time Text-to-Audio Agent with LiveProcessor
DESCRIPTION: This comprehensive Python example illustrates the creation of a real-time conversational agent using `genai-processors` and the Gemini Live API. It configures a `LiveProcessor` to handle audio output, defines an asynchronous function to collect and yield audio parts, and manages user text input to enable interactive audio responses from the model.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/live_processor_intro.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import asyncio
from typing import AsyncIterable
from genai_processors import content_api
from genai_processors import processor
from genai_processors import streams
from genai_processors.core import live_model
from google.genai import types as genai_types
from IPython.display import Audio, display
import numpy as np

LIVE_MODEL_NAME = "gemini-2.0-flash-live-001"

live_processor = live_model.LiveProcessor(
    api_key=API_KEY,
    model_name=LIVE_MODEL_NAME,
    realtime_config=genai_types.LiveConnectConfig(
        realtime_input_config=genai_types.RealtimeInputConfig(
            turn_coverage=(  # Model sees all real-time input in a turn
                "TURN_INCLUDES_ALL_INPUT"
            )
        ),
        response_modalities=["AUDIO"],  # Request audio output
    ),
)


@processor.processor_function
async def collect_audio(
    content: AsyncIterable[content_api.ProcessorPart],
) -> AsyncIterable[content_api.ProcessorPart]:
  """Yields a single Part containing all the audio from `content`."""
  audio_bytes = b""
  async for part in content:
    if content_api.is_audio(part.mimetype):
      audio_bytes += part.bytes
    else:
      yield part
    if part.get_metadata("generation_complete"):
      # this is returned when the input stream is closed
      yield content_api.ProcessorPart(
          audio_bytes,
          mimetype="audio/l16;rate=24000",
          metadata={"turn_complete": True},
      )
      audio_bytes = b""


async def text_input(
    live_queue: asyncio.Queue[content_api.ProcessorPartTypes | None],
):
  """Gets a single user input and adds it to the `live_queue`."""
  # We wrap the `input` function into an asyncio thread to avoid blocking the
  # asyncio event loop as `input` is a blocking function.
  await asyncio.sleep(0.5)
  text_in = await asyncio.to_thread(input, "User (type q to stop) > ")
  if text_in == "q":
    await live_queue.put(None)
  else:
    await live_queue.put(
        content_api.ProcessorPart(
            text_in,
            mimetype="text/plain",
        )
    )


# Prepare the input stream
live_queue = asyncio.Queue()
greetings_stream = streams.stream_content(["Hi there!"])
# This is a common idiom to merge a queue inside the input stream. Any part
# added to `live_queue` will be inserted into the input_stream.
input_stream = streams.merge(
    [greetings_stream, streams.dequeue(live_queue)],
)

# Prepare the live processor - the live processor will receive "Hi there" first.
# It will produce an audio output, after what we will schedule a task to take
# the user input. The collect_audio processor will collect all audio parts from
# the live processor and yield them as a single part that we can render in this
```

----------------------------------------

TITLE: Using `PASSTHROUGH_FALLBACK` for Parallel Processors
DESCRIPTION: Illustrates the `PASSTHROUGH_FALLBACK` mode for parallel `PartProcessor` chains. This mode ensures that if no individual processor in the `//` group returns output for a given input part, the input part is returned as-is, preventing loss of unhandled parts.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
parallel_processors = (
  append_star // append_hash // processor.PASSTHROUGH_FALLBACK
)
```

----------------------------------------

TITLE: Install genai-processors and Import Modules
DESCRIPTION: Installs the genai-processors library and imports necessary components for building AI pipelines, including core processors, Drive integration, GenAI models, and Colab utilities.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/drive_processors.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
# Copyright 2025 DeepMind Technologies Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

!pip install genai-processors

from genai_processors import processor
from genai_processors.core import drive
from genai_processors.core import genai_model
from genai_processors.core import preamble
from google.colab import auth as colab_auth
from google.colab import userdata
from IPython.display import Markdown, clear_output, display
```

----------------------------------------

TITLE: Creating a PartProcessor Filter for Text Content
DESCRIPTION: Illustrates how to create a `PartProcessor` that acts as a filter. Using `processor.create_filter()` with a callable like `content_api.is_text`, this snippet shows how to process an input stream and only output parts that satisfy the filter condition (e.g., only text parts).

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
# Creates a PartProcessor that only outputs the text parts. All other parts
# are dropped.
p = processor.create_filter(content_api.is_text)
```

----------------------------------------

TITLE: Chain TopicGenerator and TopicResearcher for Topic Generation
DESCRIPTION: This Python example shows how to chain `TopicGenerator` and `TopicResearcher` to create a processing pipeline. It initializes both processors, connects them, streams user input, and collects `Topic` dataclass objects, while rendering status updates.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/research_example.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
topics = []
p_researcher = research.TopicResearcher(api_key=GOOGLE_API_KEY)

pipeline = p_generator + p_researcher

input_stream = streams.stream_content([ProcessorPart(USER_PROMPT)])
async for content_part in pipeline(input_stream):
  if content_part.mimetype == 'application/json; type=Topic':
    topics.append(content_part.get_dataclass(research.interfaces.Topic))
  elif content_part.substream_name == 'status':
    render_part(content_part)

print(f'Pipeline produced {len(topics)} `Topic` `ProcessorParts`:\n\n')

for t in topics:
  print(t)
```

----------------------------------------

TITLE: Incorrect Usage: Passing AsyncIterable to a PartProcessor
DESCRIPTION: Highlights a common error where a `PartProcessor` is incorrectly used directly with an `AsyncIterable` input. `PartProcessors` expect individual `ProcessorPart` objects, not an iterable stream, leading to a type mismatch or runtime error.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
# p is a  PartProcessor defined somewhere else in the code.
p = part_processor

async def my_processor(
    content: AsyncIterable[ProcessorPart]
) -> AsyncIterable[ProcessorPart]:
  # This is an error as `p` expects a ProcessorPart and not an AsyncIterable.
  async for part in p(content):
    ...
```

----------------------------------------

TITLE: Google Slides Processor Pipeline Setup
DESCRIPTION: Configures a pipeline to process Google Slides content using a GenAI model. It initializes the Slides processor, sets up a preamble with user instructions, and defines the GenAI model to be used.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/drive_processors.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
PRESENTATION_ID = "YOUR-PRESENTATION-ID"  # @param {type:"string"}
USER_PROMPT = "Describe the presentation in detail."  # @param {type:"string"}

p_slides = drive.Slides(creds=CREDS)

p_preamble = preamble.Preamble(
    content=f"""You are an expert in reviewing Google Slides presentations.

You have been provided with slides, and must use them to answer the user's question.

User question: {USER_PROMPT}"""
)

p_genai = genai_model.GenaiModel(
    model_name='gemini-2.5-flash', api_key=GOOGLE_API_KEY
)

pipeline = p_slides + p_preamble + p_genai

req = drive.SlidesRequest(presentation_id=PRESENTATION_ID)
req_part = processor.ProcessorPart.from_dataclass(dataclass=req)

input_stream = processor.stream_content([req_part])

streaming_text = ''
async for content_part in pipeline(input_stream):
  streaming_text += content_part.text
  clear_output(wait=True)
  display(Markdown(streaming_text))
```

----------------------------------------

TITLE: Google Docs Processor Pipeline Setup
DESCRIPTION: Configures a pipeline to process Google Docs content using a GenAI model. It initializes the Docs processor, sets up a preamble with user instructions, and defines the GenAI model to be used.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/drive_processors.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
DOC_ID = "YOUR-DOC-ID"  # @param {type:"string"}
USER_PROMPT = "Describe the document in detail."  # @param {type:"string"}

p_docs = drive.Docs(creds=CREDS)

p_preamble = preamble.Preamble(
    content=f"""You are an expert in reviewing Google Docs.

You have been provided with a document, and must use it to answer the user's question.

User question: {USER_PROMPT}"""
)

p_genai = genai_model.GenaiModel(
    model_name='gemini-2.5-flash', api_key=GOOGLE_API_KEY
)

pipeline = p_docs + p_preamble + p_genai

req = drive.DocsRequest(doc_id=DOC_ID)
req_part = processor.ProcessorPart.from_dataclass(dataclass=req)

input_stream = processor.stream_content([req_part])

streaming_text = ''
async for content_part in pipeline(input_stream):
  streaming_text += content_part.text
  clear_output(wait=True)
  display(Markdown(streaming_text))
```

----------------------------------------

TITLE: Retrieve Google API Key from Colab Userdata
DESCRIPTION: Retrieves the 'GOOGLE_API_KEY' from Colab's user data secrets. This is the recommended way to securely access API keys in a Colab environment for authentication with GenAI models, avoiding hardcoding sensitive information.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
from google.colab import userdata

API_KEY = userdata.get('GOOGLE_API_KEY')
```

----------------------------------------

TITLE: Define Parameterized Text Processor Class
DESCRIPTION: Implements a `Processor` by inheriting from `processor.Processor` and overriding the `call` method. This class allows for parameterization (`eos_string`) and demonstrates using a `Preamble` to add a prefix to the content stream, replacing periods with a custom string. This approach is recommended for processors requiring persistent state or custom initialization.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/processor_intro.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from genai_processors.core import preamble


class SimpleTextProcessor(processor.Processor):

  def __init__(self, eos_string: str):
    self._eos = eos_string
    # Preamble adds a prefix to a content stream.
    self._preamble = preamble.Preamble("Starting. ")

  async def call(
      self,
      content: AsyncIterable[content_api.ProcessorPart],
  ) -> AsyncIterable[content_api.ProcessorPart]:
    """Replaces dots with '[EoS]'."""
    async for part in self._preamble(content):
      if content_api.is_text(part.mimetype):
        yield content_api.ProcessorPart(part.text.replace(".", self._eos))
      else:
        yield part
```

----------------------------------------

TITLE: Create ProcessorPart from Text Data
DESCRIPTION: This example demonstrates how to instantiate a `ProcessorPart` object directly from a string. It shows how the MIME type is automatically inferred and how to access the text content of the part.

SOURCE: https://github.com/google-gemini/genai-processors/blob/main/notebooks/content_api_intro.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
text_data = "Hello, GenAI World! This is a text part."
text_part = ProcessorPart(text_data)

print(f"Original data: {text_data}")
print(f"ProcessorPart: {text_part}")
print(
    f"MIME type: {text_part.mimetype}"
)  # Automatically inferred as 'text/plain'
print(f"Text content: '{text_part.text}'")
```