---
issue_id: 202
title: OpenAI Whisper API Transcription Engine Implementation
type: enhancement
priority: medium
created: 2025-01-21
status: planned
assignee: system
---

# OpenAI Transcription Engine Implementation Plan

## Overview

This plan outlines the detailed implementation of an OpenAI-powered transcription engine for vttiro, analogous to the existing Gemini engine. The implementation will leverage OpenAI's Audio API with both Whisper-1 and the newer GPT-4o transcription models to provide high-quality audio transcription with native timing information.

## Technical Requirements Analysis

### Core Objectives
1. **Native API Integration**: Direct integration with OpenAI's Audio API (`/v1/audio/transcriptions`)
2. **Model Flexibility**: Support both Whisper-1 and GPT-4o transcription models (`gpt-4o-transcribe`, `gpt-4o-mini-transcribe`)
3. **WebVTT-First Approach**: Request WebVTT format directly from OpenAI when supported, eliminating artificial timestamp estimation
4. **Format Compatibility**: Handle multiple response formats (`json`, `text`, `srt`, `verbose_json`, `vtt`)
5. **Seamless Integration**: Follow vttiro's architecture patterns for consistent behavior with existing engines

### Key Technical Challenges
1. **Model Capability Differences**: Whisper-1 supports full parameter set, while GPT-4o models have limited parameters
2. **Format Support Variations**: GPT-4o models only support `json`/`text`, while Whisper-1 supports `vtt`/`srt`
3. **File Size Limitations**: 25MB limit requires chunking strategy for large files
4. **Streaming vs Batch**: GPT-4o models support streaming, Whisper-1 does not
5. **Cost Optimization**: Balance between accuracy and cost across different models

## Architecture Design

### 1. Core Components Structure

```
src/vttiro/models/openai.py
├── OpenAITranscriber (main engine class)
├── OpenAIModel (enum for model variants)
├── OpenAIResponseProcessor (handles different response formats)
└── OpenAIFileManager (handles file upload and chunking)
```

### 2. Model Hierarchy Integration

**Base Classes Extension:**
- Extend `TranscriptionEngine` from `src/vttiro/core/transcription.py`
- Add `OpenAIModel` enum to `src/vttiro/models/base.py`
- Update `TranscriptionEngine` enum to include `OPENAI = "openai"`
- Add model capabilities to `MODEL_CAPABILITIES` dictionary

**Model Variants:**
```python
class OpenAIModel(str, Enum):
    WHISPER_1 = "whisper-1"
    GPT_4O_TRANSCRIBE = "gpt-4o-transcribe"
    GPT_4O_MINI_TRANSCRIBE = "gpt-4o-mini-transcribe"
```

### 3. Configuration Integration

**API Key Management:**
- Environment variable: `OPENAI_API_KEY`
- Config field: `config.transcription.openai_api_key`
- Validation during engine initialization

**Model-Specific Settings:**
```python
openai_config = {
    "whisper-1": {
        "supports_vtt": True,
        "supports_verbose_json": True,
        "supports_streaming": False,
        "max_params": "all"
    },
    "gpt-4o-transcribe": {
        "supports_vtt": False,
        "supports_verbose_json": False, 
        "supports_streaming": True,
        "max_params": "limited"
    }
}
```

## Detailed Implementation Plan

### Phase 1: Core Engine Implementation (4-6 hours)

#### 1.1 Create OpenAI Model Definitions
**File:** `src/vttiro/models/base.py`

```python
class OpenAIModel(str, Enum):
    """OpenAI transcription model variants."""
    WHISPER_1 = "whisper-1"
    GPT_4O_TRANSCRIBE = "gpt-4o-transcribe" 
    GPT_4O_MINI_TRANSCRIBE = "gpt-4o-mini-transcribe"

# Add to existing enums
class TranscriptionEngine(str, Enum):
    GEMINI = "gemini"
    ASSEMBLYAI = "assemblyai"
    DEEPGRAM = "deepgram"
    OPENAI = "openai"  # NEW

# Update mappings
ENGINE_DEFAULT_MODELS[TranscriptionEngine.OPENAI] = OpenAIModel.GPT_4O_TRANSCRIBE.value
ENGINE_AVAILABLE_MODELS[TranscriptionEngine.OPENAI] = [model.value for model in OpenAIModel]
```

#### 1.2 Add Model Capabilities
**File:** `src/vttiro/models/base.py`

```python
MODEL_CAPABILITIES.update({
    "whisper-1": ModelCapability(
        max_duration_seconds=1500,  # 25MB file limit roughly 25 minutes
        cost_per_minute=0.006,      # $0.006 per minute
        speed_factor=1.0,           # Standard processing speed
        accuracy_score=0.89,        # Good baseline accuracy
        language_support=set(WHISPER_LANGUAGES),  # 98 languages
        supports_diarization=False, # Whisper doesn't do native diarization
        supports_timestamps=True,
        supports_confidence=True,
        recommended_for={"general", "multilingual", "batch"},
        limitations=["25MB file limit", "No native speaker diarization", "No streaming"],
        warning_threshold_minutes=20.0
    ),
    
    "gpt-4o-transcribe": ModelCapability(
        max_duration_seconds=1500,  # Similar file limit
        cost_per_minute=0.012,      # Estimated higher cost
        speed_factor=1.5,           # Faster due to GPT-4o architecture
        accuracy_score=0.94,        # Higher accuracy expected
        language_support={"en", "es", "fr", "de", "it", "pt", "ja", "ko", "zh", "ar", "ru", "hi"},
        supports_diarization=True,  # GPT-4o can do contextual speaker identification
        supports_timestamps=True,
        supports_confidence=True,
        supports_emotions=True,     # GPT-4o can detect emotions
        recommended_for={"high-quality", "english", "streaming", "real-time"},
        limitations=["Limited parameter support", "Higher cost", "Fewer languages than Whisper"],
        warning_threshold_minutes=10.0  # Warn earlier due to cost
    ),
    
    "gpt-4o-mini-transcribe": ModelCapability(
        max_duration_seconds=1500,
        cost_per_minute=0.003,      # Lower cost option
        speed_factor=2.0,           # Faster processing
        accuracy_score=0.90,        # Good accuracy, slightly lower than full GPT-4o
        language_support={"en", "es", "fr", "de", "it", "pt", "ja", "ko", "zh"},
        supports_diarization=True,
        supports_timestamps=True,
        supports_confidence=True,
        recommended_for={"cost-effective", "fast", "general"},
        limitations=["Limited parameter support", "Fewer languages"],
        warning_threshold_minutes=15.0
    )
})
```

#### 1.3 Main Engine Implementation 
**File:** `src/vttiro/models/openai.py`

```python
#!/usr/bin/env python3
# this_file: src/vttiro/models/openai.py
"""OpenAI Audio API transcription engine with Whisper and GPT-4o support."""

import asyncio
from pathlib import Path
from typing import Optional, Dict, Any, List
import time
import io

try:
    from loguru import logger
except ImportError:
    import logging as logger

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI not available. Install with: uv add openai")
    OpenAI = None

from vttiro.core.transcription import TranscriptionEngine
from vttiro.core.config import VttiroConfig, TranscriptionResult
from vttiro.models.base import OpenAIModel
from vttiro.core.prompts import WebVTTPromptGenerator, PromptTemplate


class OpenAITranscriber(TranscriptionEngine):
    """OpenAI Audio API transcription engine with Whisper-1 and GPT-4o support."""
    
    def __init__(self, config: VttiroConfig, model: OpenAIModel = OpenAIModel.GPT_4O_TRANSCRIBE):
        super().__init__(config)
        self.model_variant = model
        
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI not available. Install with: uv add openai")
        
        # Configure OpenAI API
        api_key = config.transcription.openai_api_key
        if not api_key:
            raise ValueError("OpenAI API key not configured. Set OPENAI_API_KEY environment variable.")
            
        self.client = OpenAI(api_key=api_key)
        
        # Model-specific configuration
        self.model_config = self._get_model_config(model)
        
        # Language support mapping
        self._supported_languages = self._get_supported_languages(model)
        
        # Initialize WebVTT prompt generator for GPT-4o models
        if model in [OpenAIModel.GPT_4O_TRANSCRIBE, OpenAIModel.GPT_4O_MINI_TRANSCRIBE]:
            self.prompt_generator = WebVTTPromptGenerator(
                include_examples=True,
                include_diarization=True,
                include_emotions=True,
                template=PromptTemplate.SPEAKER_DIARIZATION
            )
        else:
            self.prompt_generator = None
    
    @property
    def name(self) -> str:
        return f"openai/{self.model_variant.value}"
    
    def _get_model_config(self, model: OpenAIModel) -> Dict[str, Any]:
        """Get model-specific configuration."""
        configs = {
            OpenAIModel.WHISPER_1: {
                "supports_vtt": True,
                "supports_verbose_json": True,
                "supports_streaming": False,
                "supports_prompting": True,
                "supports_all_params": True,
                "default_response_format": "verbose_json"
            },
            OpenAIModel.GPT_4O_TRANSCRIBE: {
                "supports_vtt": False,
                "supports_verbose_json": False,
                "supports_streaming": True,
                "supports_prompting": True,
                "supports_all_params": False,
                "default_response_format": "json"
            },
            OpenAIModel.GPT_4O_MINI_TRANSCRIBE: {
                "supports_vtt": False,
                "supports_verbose_json": False,
                "supports_streaming": True,
                "supports_prompting": True,
                "supports_all_params": False,
                "default_response_format": "json"
            }
        }
        return configs[model]
    
    async def transcribe(
        self, 
        audio_path: Path, 
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> TranscriptionResult:
        """Transcribe audio using OpenAI Audio API."""
        start_time = time.time()
        
        logger.info(f"Transcribing with OpenAI {self.model_variant.value}: {audio_path}")
        
        try:
            # Validate file size and prepare for upload
            if not audio_path.exists():
                raise FileNotFoundError(f"Audio file not found: {audio_path}")
            
            file_size = audio_path.stat().st_size
            if file_size > 25 * 1024 * 1024:  # 25MB limit
                raise ValueError(f"File too large: {file_size / (1024*1024):.1f}MB. OpenAI limit is 25MB.")
            
            # Prepare transcription parameters
            transcription_params = self._prepare_transcription_params(language, context)
            
            # Perform transcription
            response = await self._transcribe_with_openai(audio_path, transcription_params)
            
            # Process response based on model and format
            result = self._process_openai_response(response, transcription_params, start_time)
            
            logger.info(f"OpenAI transcription completed in {result.processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"OpenAI transcription failed for {audio_path}: {e}")
            raise
    
    def _prepare_transcription_params(
        self, 
        language: Optional[str], 
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Prepare parameters for OpenAI transcription."""
        params = {
            "model": self.model_variant.value
        }
        
        # Response format selection
        if self.model_config["supports_vtt"] and self.config.output.format == "webvtt":
            params["response_format"] = "vtt"
        elif self.model_config["supports_verbose_json"]:
            params["response_format"] = "verbose_json"
        else:
            params["response_format"] = self.model_config["default_response_format"]
        
        # Language parameter
        if language and language != "auto" and language in self._supported_languages:
            params["language"] = language
        
        # Model-specific parameters
        if self.model_config["supports_all_params"]:
            # Whisper-1 supports full parameter set
            if context:
                params["timestamp_granularities"] = ["word", "segment"]
            
        # Prompting for GPT-4o models (using prompt parameter)
        if (self.model_config["supports_prompting"] and 
            self.model_variant in [OpenAIModel.GPT_4O_TRANSCRIBE, OpenAIModel.GPT_4O_MINI_TRANSCRIBE]):
            
            # Generate WebVTT-format prompt for GPT-4o models
            if self.prompt_generator:
                prompt = self.prompt_generator.generate_webvtt_prompt(language, context)
                params["prompt"] = prompt[:224]  # OpenAI prompt limit
        
        # Whisper-1 specific prompting (different approach)
        elif (self.model_variant == OpenAIModel.WHISPER_1 and context):
            # For Whisper-1, use prompt for context and proper noun guidance
            context_prompt = self._generate_whisper_prompt(context)
            if context_prompt:
                params["prompt"] = context_prompt[:224]  # 224 token limit
        
        return params
    
    async def _transcribe_with_openai(
        self, 
        audio_path: Path, 
        params: Dict[str, Any]
    ) -> Any:
        """Perform transcription using OpenAI Audio API."""
        try:
            with open(audio_path, "rb") as audio_file:
                # Use asyncio.to_thread for I/O bound operation
                response = await asyncio.to_thread(
                    self.client.audio.transcriptions.create,
                    file=audio_file,
                    **params
                )
            return response
        except Exception as e:
            logger.error(f"OpenAI API call failed: {e}")
            raise
    
    def _process_openai_response(
        self, 
        response: Any, 
        params: Dict[str, Any], 
        start_time: float
    ) -> TranscriptionResult:
        """Process OpenAI API response into TranscriptionResult."""
        processing_time = time.time() - start_time
        response_format = params.get("response_format", "json")
        
        # Handle different response formats
        if response_format == "vtt":
            # Direct WebVTT response from Whisper-1
            webvtt_content = response
            transcribed_text, word_timestamps = self._parse_webvtt_response(webvtt_content)
            metadata = {
                "format": "webvtt",
                "webvtt_content": webvtt_content,
                "native_timing": True
            }
            
        elif response_format == "verbose_json":
            # Detailed JSON response with segments and words
            transcribed_text = response.text
            word_timestamps = self._extract_word_timestamps(response)
            metadata = {
                "format": "verbose_json",
                "segments": getattr(response, 'segments', []),
                "native_timing": True
            }
            
        else:
            # Basic JSON/text response
            if hasattr(response, 'text'):
                transcribed_text = response.text
            else:
                transcribed_text = str(response)
            
            word_timestamps = []
            metadata = {
                "format": response_format,
                "native_timing": False
            }
        
        # Common metadata
        metadata.update({
            "engine": "openai",
            "model": self.model_variant.value,
            "response_format": response_format,
            "processing_time": processing_time
        })
        
        return TranscriptionResult(
            text=transcribed_text,
            confidence=0.90,  # OpenAI doesn't provide confidence scores
            word_timestamps=word_timestamps,
            processing_time=processing_time,
            model_name=self.model_variant.value,
            language=params.get("language", "auto"),
            metadata=metadata
        )
```

### Phase 2: Response Processing and Format Handling (2-3 hours)

#### 2.1 WebVTT Response Parser
```python
def _parse_webvtt_response(self, webvtt_content: str) -> tuple[str, List[Dict[str, Any]]]:
    """Parse WebVTT content from Whisper-1 vtt format response."""
    # Similar to Gemini's parser but adapted for OpenAI WebVTT format
    # Implementation details from existing Gemini parser
    pass

def _extract_word_timestamps(self, response: Any) -> List[Dict[str, Any]]:
    """Extract word-level timestamps from verbose_json response."""
    word_timestamps = []
    
    if hasattr(response, 'segments'):
        for segment in response.segments:
            if hasattr(segment, 'words'):
                for word_data in segment.words:
                    word_timestamps.append({
                        "word": word_data.word,
                        "start": word_data.start,
                        "end": word_data.end,
                        "confidence": getattr(word_data, 'confidence', 0.90)
                    })
    
    return word_timestamps
```

#### 2.2 Whisper Prompt Generation
```python
def _generate_whisper_prompt(self, context: Dict[str, Any]) -> Optional[str]:
    """Generate context-aware prompt for Whisper-1 model."""
    prompt_parts = []
    
    # Add video title for proper noun recognition
    if context.get('video_title'):
        title_words = context['video_title'].split()[:10]  # Limit words
        prompt_parts.extend(title_words)
    
    # Add known terminology from description
    if context.get('video_description'):
        desc = context['video_description'][:100]  # Limit length
        # Extract proper nouns and technical terms
        # Simple approach: capitalize words, acronyms
        import re
        proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', desc)
        acronyms = re.findall(r'\b[A-Z]{2,}\b', desc)
        prompt_parts.extend(proper_nouns[:5])
        prompt_parts.extend(acronyms[:3])
    
    if prompt_parts:
        return ', '.join(prompt_parts[:20])  # Stay within token limits
    
    return None
```

### Phase 3: Integration and Configuration (1-2 hours)

#### 3.1 Update Base Configuration
**File:** `src/vttiro/core/config.py`

```python
@dataclass
class TranscriptionConfig:
    # ... existing fields ...
    openai_api_key: Optional[str] = None
```

#### 3.2 Update Engine Factory
**File:** `src/vttiro/core/transcriber.py` or main factory

```python
def create_transcription_engine(engine: str, model: str, config: VttiroConfig) -> TranscriptionEngine:
    """Factory function for creating transcription engines."""
    if engine == "openai":
        from vttiro.models.openai import OpenAITranscriber, OpenAIModel
        model_enum = OpenAIModel(model)
        return OpenAITranscriber(config, model_enum)
    elif engine == "gemini":
        # ... existing implementation
        pass
    # ... other engines
```

### Phase 4: Streaming Support (Optional - 2-3 hours)

#### 4.1 Streaming Implementation for GPT-4o Models
```python
async def transcribe_streaming(
    self,
    audio_path: Path,
    language: Optional[str] = None,
    context: Optional[Dict[str, Any]] = None
) -> AsyncGenerator[TranscriptionResult, None]:
    """Stream transcription results for GPT-4o models."""
    if not self.model_config["supports_streaming"]:
        # Fall back to regular transcription
        result = await self.transcribe(audio_path, language, context)
        yield result
        return
    
    # Implement streaming logic using OpenAI's streaming API
    params = self._prepare_transcription_params(language, context)
    params["stream"] = True
    
    with open(audio_path, "rb") as audio_file:
        stream = await asyncio.to_thread(
            self.client.audio.transcriptions.create,
            file=audio_file,
            **params
        )
        
        async for chunk in stream:
            # Process streaming chunk
            yield self._process_streaming_chunk(chunk)
```

### Phase 5: Testing and Validation (2-3 hours)

#### 5.1 Unit Tests
**File:** `tests/test_openai_engine.py`

```python
import pytest
from unittest.mock import Mock, patch
from vttiro.models.openai import OpenAITranscriber, OpenAIModel
from vttiro.core.config import VttiroConfig

class TestOpenAITranscriber:
    def test_initialization(self):
        """Test OpenAI transcriber initialization."""
        pass
    
    def test_model_config_whisper1(self):
        """Test Whisper-1 specific configuration."""
        pass
    
    def test_model_config_gpt4o(self):
        """Test GPT-4o specific configuration."""
        pass
    
    @patch('openai.OpenAI')
    async def test_transcribe_whisper1_vtt(self, mock_openai):
        """Test Whisper-1 with VTT output."""
        pass
    
    @patch('openai.OpenAI')  
    async def test_transcribe_gpt4o_json(self, mock_openai):
        """Test GPT-4o with JSON output."""
        pass
```

#### 5.2 Integration Tests
```python
async def test_openai_engine_integration():
    """Test OpenAI engine with real API (requires API key)."""
    # Skip if no API key available
    pass

def test_cost_estimation():
    """Test cost estimation accuracy."""
    pass
```

## File Structure Summary

```
src/vttiro/models/
├── __init__.py          # Updated imports
├── base.py              # Add OpenAI models and capabilities  
├── openai.py            # NEW - Main OpenAI engine implementation
├── gemini.py            # Existing
├── assemblyai.py        # Existing  
└── deepgram.py          # Existing

tests/
├── test_openai_engine.py    # NEW - OpenAI engine tests
└── ...existing test files

issues/
└── 202.txt              # This implementation plan
```

## Cost and Performance Considerations

### Model Selection Strategy
1. **Whisper-1**: Best for multilingual content, batch processing, cost-effective
2. **GPT-4o-transcribe**: Best for high-accuracy English content, real-time needs
3. **GPT-4o-mini-transcribe**: Best balance of cost/performance for general use

### Cost Analysis (per hour of audio)
- Whisper-1: ~$0.36/hour
- GPT-4o-transcribe: ~$0.72/hour (estimated)
- GPT-4o-mini-transcribe: ~$0.18/hour (estimated)

### Performance Optimization
1. **File Format**: Prefer MP3/M4A over WAV for smaller file sizes
2. **Chunking Strategy**: Split large files at silence boundaries
3. **Caching**: Cache API responses for development/testing
4. **Retry Logic**: Implement exponential backoff for rate limiting

## Risk Mitigation

### API Limitations
- **File Size**: Implement chunking for >25MB files
- **Rate Limiting**: Add request throttling and retry logic
- **Model Availability**: Graceful fallback between models

### Quality Assurance  
- **Response Validation**: Verify WebVTT format compliance
- **Timestamp Validation**: Ensure sequential, non-overlapping timestamps
- **Fallback Processing**: Handle malformed responses gracefully

### Security Considerations
- **API Key Management**: Secure storage and validation
- **File Handling**: Temporary file cleanup
- **Error Messages**: Avoid leaking sensitive information in logs

## Future Enhancements

### Phase 2 Features (Post-MVP)
1. **Real-time Streaming**: Full implementation for live transcription
2. **Batch Processing**: Optimize for multiple file processing
3. **Custom Fine-tuning**: Support for custom Whisper models
4. **Advanced Diarization**: Combine with pyannote for better speaker separation

### Integration Opportunities
1. **Ensemble Mode**: Combine OpenAI with other engines for validation
2. **Cost Optimization**: Dynamic model selection based on budget constraints
3. **Quality Metrics**: Compare accuracy across engines for content types

## Implementation Timeline

**Total Estimated Time: 12-18 hours**

- Phase 1 (Core Engine): 4-6 hours
- Phase 2 (Response Processing): 2-3 hours  
- Phase 3 (Integration): 1-2 hours
- Phase 4 (Streaming): 2-3 hours (optional)
- Phase 5 (Testing): 2-3 hours
- Documentation and Polish: 1 hour

## Success Criteria

### Functional Requirements
- [ ] Successfully transcribe audio using all three OpenAI models
- [ ] Generate properly formatted WebVTT output with valid timestamps
- [ ] Handle file size limitations gracefully with clear error messages
- [ ] Integrate seamlessly with existing vttiro CLI and configuration
- [ ] Provide accurate cost estimation for different models

### Quality Requirements  
- [ ] Transcription accuracy comparable to existing engines
- [ ] Processing time within acceptable limits (< 2x real-time for most content)
- [ ] Robust error handling and logging
- [ ] Comprehensive test coverage (>80%)

### Performance Requirements
- [ ] Handle files up to 25MB without chunking
- [ ] Support concurrent transcription requests
- [ ] Memory efficient processing (no memory leaks)
- [ ] Graceful degradation when API limits reached

This implementation plan provides a comprehensive roadmap for adding OpenAI transcription capabilities to vttiro while maintaining consistency with the existing architecture and addressing the WebVTT timing issues identified in PLAN.md.